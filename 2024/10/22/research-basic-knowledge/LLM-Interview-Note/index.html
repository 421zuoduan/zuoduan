<!DOCTYPE html>
<html lang="zh-cn,en,default">
<head>
  <meta charset="UTF-8">




<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

  <meta name="author" content="崔若晨 Ruochen Cui">


  <meta name="subtitle" content="崔若晨">


  <meta name="description" content="电子科技大学四年级本科生

A fourth year undergraduate student at UESTC
">


  <meta name="keywords" content="Ruochen Cui,崔若晨,Artificial Intelligence">


<title>LLM_Interview_Note 阅读笔记 | Ruochen Cui</title>



<link rel="icon" href="/web_ico.ico">


<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"
/>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">


<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/css/search.css">



<script src="/lib/jquery.min.js"></script>


<script src="/lib/iconify-icon.min.js"></script>


<script src="https://cdn.tailwindcss.com?plugins=typography"></script>
<script>
  tailwind.config = {
    darkMode: "class",
  };
</script>

<script>
  (function () {
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const setting = localStorage.getItem("hexo-color-scheme") || "auto";
    if (setting === "dark" || (prefersDark && setting !== "light"))
      document.documentElement.classList.toggle("dark", true);
    let isDark = document.documentElement.classList.contains("dark");
  })();

  $(document).ready(function () {
    // init icon
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const isDark = document.documentElement.classList.contains("dark");
    $("#theme-icon").attr("icon", isDark ? "ic:round-dark-mode" : "ic:round-light-mode");

    function toggleGiscusTheme() {
      const isDark = document.documentElement.classList.contains("dark");
      const giscusFrame = document.querySelector("iframe.giscus-frame");
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage(
          {
            giscus: {
              setConfig: {
                theme: isDark ? "dark" : "light",
              },
            },
          },
          "https://giscus.app"
        );
      }
    }


    // toggle dark mode
    function toggleDark() {
      let isDark = document.documentElement.classList.contains("dark");
      const setting = localStorage.getItem("hexo-color-scheme") || "auto";
      isDark = !isDark;
      document.documentElement.classList.toggle("dark", isDark);
      $("#theme-icon").attr("icon", isDark ? "ic:round-dark-mode" : "ic:round-light-mode");
      if (prefersDark === isDark) {
        localStorage.setItem("hexo-color-scheme", "auto");
      } else {
        localStorage.setItem("hexo-color-scheme", isDark ? "dark" : "light");
      }
      toggleGiscusTheme();
    }
    $("#toggle-dark").click(toggleDark);

    // listen dark mode change
    window
      .matchMedia("(prefers-color-scheme: dark)")
      .addEventListener("change", (e) => {
        const setting = localStorage.getItem("hexo-color-scheme") || "auto";
        if (setting === "auto") {
          document.documentElement.classList.toggle("dark", e.matches);
          $("#theme-icon").attr(
            "icon",
            e.matches ? "ic:round-dark-mode" : "ic:round-light-mode"
          );
          toggleGiscusTheme();
        }
      });
  });
</script>




<meta name="generator" content="Hexo 7.1.1"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
<body 
  class="
    bg-[var(--c-0)]
    text-[var(--c-80)]
  ">
  <!-- The navigation bar -->
<header class="
    flex flex-row items-center
    w-full
    pr-4
    z-10
    border-b-[1px]
    border-b-[var(--c-border)]
    dark:bg-[var(--c-0)]
    dark:border-b-[var(--c-0)]
    gap-2
    h-[var(--h-header)]
    text-[var(--c-80)]
">
  <!-- Left part -->
  <div class="overflow-hidden h-full flex flex-row items-center">
    <!-- Site Title on the top left -->
    <a href="/" class="
            whitespace-nowrap
            text-2xl
            text-[var(--c-theme)]
            hover:text-[var(--c-theme)]
            pl-4
            font-black
            bg-gradient-to-r from-cyan-500
            to-blue-500 bg-clip-text text-transparent
          ">
      Ruochen Cui
    </a>
  </div>
  <!-- Div for pushing items to both sides -->
  <div class="flex-1"></div>
  <!-- Right part -->
  <div class="flex flex-row items-center z-20 h-full">
    <!-- Page links -->
    <div class="hidden sm:flex flex-row h-full">
      
      
      
      
      
      
      <a href="/./archives" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:inbox-fill" width="22">
        </iconify-icon>
        
        
        <p>Posts</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./publications" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:science-fill" width="22">
        </iconify-icon>
        
        
        <p>Publications</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./about" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:user-info-fill" width="22">
        </iconify-icon>
        
        
        <p>About</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./categories" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:classify-2-fill" width="22">
        </iconify-icon>
        
        
        <p>Categories</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./tags" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:tag-fill" width="22">
        </iconify-icon>
        
        
        <p>Tags</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./index" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:home-2-fill" width="22">
        </iconify-icon>
        
        
      </a>
      
    </div>
    <!-- Icons on the right -->
    <div class="flex flex-row items-center">

      <!-- TODO: Add search icon here -->

      <!-- Dark/light toggle icon -->
      <a class="flex group p-1" title="toggle theme" id="toggle-dark">
        <iconify-icon class="transition-transform
                    group-hover:rotate-[45deg]
                    group-hover:scale-125
                    group-hover:text-[var(--c-theme)]" width="24" id="theme-icon">
        </iconify-icon>
      </a>
      <!-- Icon for dropout menu on small screens -->
      <div class="flex p-1 mx-1 sm:hidden">
        <a class="w-5 h-5" aria-hidden="true" id="open-menu">
          <iconify-icon width="24" icon="mingcute:menu-fill" class="transition-transform hover:scale-125 hover:rotate-[5deg]">
          </iconify-icon>
        </a>
        <a class="w-5 h-5 hidden" aria-hidden="true" id="close-menu">
          <iconify-icon width="24" icon="mingcute:close-circle-fill" class="transition-transform hover:scale-125 hover:rotate-[80deg]">
          </iconify-icon>
        </a>
      </div>
    </div>
  </div>
</header>

<!-- Dropdown menu on small screens -->
<div id="menu-panel" class="
        h-0
        overflow-hidden
        sm:hidden
        w-full
        z-10
        rounded
    ">
  <div id="menu-content" class="
        flex
        flex-row
        justify-center
        items-center
        font-bold
        text-xl
        border-b-[1px]
        relative
        z-20
        border-[var(--c-sep)]
        px-2
        py-2
        -translate-y-full
        transition-transform
        duration-200
        ">
    
    
    
    <a href="/./archives" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:inbox-fill" width="22">
      </iconify-icon>
      <p>
        posts
      </p>
    </a>
    
    
    
    
    <a href="/./publications" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:science-fill" width="22">
      </iconify-icon>
      <p>
        publications
      </p>
    </a>
    
    
    
    
    <a href="/./about" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:user-info-fill" width="22">
      </iconify-icon>
      <p>
        about
      </p>
    </a>
    
    
    
    
    <a href="/./categories" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:classify-2-fill" width="22">
      </iconify-icon>
      <p>
        categories
      </p>
    </a>
    
    
    
    
    <a href="/./tags" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:tag-fill" width="22">
      </iconify-icon>
      <p>
        tags
      </p>
    </a>
    
    
    
    
    <a href="/./index" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:home-2-fill" width="22">
      </iconify-icon>
      <p>
        home
      </p>
    </a>
    
    
  </div>
</div>
  <main>
    <!-- css -->

<link rel="stylesheet" href="/lib/fancybox/fancybox.min.css">

  
<link rel="stylesheet" href="/lib/tocbot/tocbot.min.css">

    <!-- toc -->
    
  <!-- tocbot -->
<nav class="post-toc toc text-sm w-40 relative top-32 right-4 opacity-70 hidden lg:block" style="position: fixed !important;"></nav>


<section class="px-6 max-w-prose mx-auto md:px-0">
  <!-- Post header before content -->
  <header class="py-4">
    <div class="flex flex-col gap-2 pt-4 md:pt-6">
      <!-- Title -->
      <div id="article-title" class="leading-snug">
        <p class="text-3xl font-bold text-[var(--c-100)] mb-4">LLM_Interview_Note 阅读笔记</p>
      </div>
      <!-- Meta data -->
      <div>
        <section class="
          flex flex-col gap-x-2 gap-y-1 text-sm text-[var(--c-100)]">
          <div class="flex flex-wrap items-center gap-x-2 gap-y-1">
            <!-- Dates -->
            <div class="flex items-center gap-1">
              <iconify-icon width="18" icon="mingcute:add-circle-fill" ></iconify-icon>
              Created: <time class="w-max">2024-10-22</time>
            </div>
            <div class="flex items-center gap-1">
              <iconify-icon width="18" icon="mingcute:refresh-3-fill" ></iconify-icon>
              Edited: <time class="w-max">2024-11-21</time>
            </div>
          </div>
          <div class="flex flex-wrap items-center gap-x-3 gap-y-3">
            <!-- Author -->
            
              <span class="flex items-center gap-1 group">
                <iconify-icon width="18" icon="mingcute:user-edit-fill" ></iconify-icon>
                <p>myself</p>
              </span>
            

            <!-- Word count -->
            <span class="flex items-center gap-1">
              <iconify-icon width="18" icon="mingcute:book-2-fill" ></iconify-icon>
              <span>2.6k words, 9 min</span>
            </span>
            <!-- Categories -->
            
              <!-- <span class="text-gray-400">·</span> -->
              <span class="flex flex-row items-center gap-1 group hover:underline">
                <iconify-icon class="transition-all group-hover:scale-125 mr-0"
                  width="18"
                  icon="mingcute:classify-2-fill">
                </iconify-icon>
                <a class="article-category-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a>
              </span>
            
          </div>
        </section>
      </div>
      <!-- tags -->
      <div>
        
<div class="flex flex-wrap gap-1">
  
    
      <a href="/tags/llm/" 
        class="
          tag
          text-sm
          rounded-full
          px-[5px]
          border-[1px]
          border-[var(--c-theme)]
          text-[var(--c-theme)]
          bg-[var(--c-0)]
          dark:bg-[var(--c-0)]
          dark:drop-shadow-none
          hover:bg-[var(--c-theme)]
          hover:text-[var(--c-0)]
          dark:hover:text-[var(--c-theme-2)]
        ">
        llm
      </a>
    
      <a href="/tags/note/" 
        class="
          tag
          text-sm
          rounded-full
          px-[5px]
          border-[1px]
          border-[var(--c-theme)]
          text-[var(--c-theme)]
          bg-[var(--c-0)]
          dark:bg-[var(--c-0)]
          dark:drop-shadow-none
          hover:bg-[var(--c-theme)]
          hover:text-[var(--c-0)]
          dark:hover:text-[var(--c-theme-2)]
        ">
        note
      </a>
    
  
</div>
      </div>
    </div>
  </header>
  <!-- content -->
  <article class="post-content prose m-auto dark:prose-invert">
    <p>本篇笔记基于 <a target="_blank" rel="noopener" href="https://github.com/wdndev/llm_interview_note/tree/main">wdndev/llm_interview_note: 主要记录大语言大模型（LLMs） 算法（应用）工程师相关的知识及面试题</a> 制作, 抽取了一些自认为面试时会被问到的内容, 用于补足个人薄弱的专业基础, 也许找个实习?</p>
<h1 id="大语言模型基础"><a href="#大语言模型基础" class="headerlink" title="大语言模型基础"></a>大语言模型基础</h1><h2 id="LLM-概念"><a href="#LLM-概念" class="headerlink" title="LLM 概念"></a>LLM 概念</h2><h3 id="当前主流的开源模型体系"><a href="#当前主流的开源模型体系" class="headerlink" title="当前主流的开源模型体系"></a>当前主流的开源模型体系</h3><ol>
<li>GPT: OpenAI 发布</li>
<li>BERT: Google 发布</li>
<li>XLNet: CMU 和 Google Brain 发布</li>
<li>RoBERTa: Facebook 发布</li>
<li>T5: Google 发布</li>
</ol>
<h3 id="Prefix-LM-和-Causal-LM"><a href="#Prefix-LM-和-Causal-LM" class="headerlink" title="Prefix LM 和 Causal LM"></a>Prefix LM 和 Causal LM</h3><ul>
<li>Prefix LM 前缀语言模型: 采用 Encoder-Decoder 的结构, Encoder 使用 Auto Encoding (AE, 自编码) 模式, Decoder 使用Auto Regressive (AR, 自回归) 模式. 待生成的 token 可以看到 Encoder 侧所有 token 和 Decoder 侧已生成的 token</li>
<li>Causal LM 因果语言模型: Decoder-Only 结构, 使用 Auto Regressive 模式, 根据历史 token 来预测下一个 token, 在 Attention Mask 这里操作</li>
</ul>
<p><img src="https://github.com/wdndev/llm_interview_note/raw/main/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.llm%E6%A6%82%E5%BF%B5/image/image_ZPQiHay1ZD.png" alt="img"></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/588325646/answer/3383505083">为什么现在的LLM都是Decoder only的架构？ - 知乎</a></p>
<blockquote>
<p>要明白 prefix LM, 要先知道 NLP 里的 Encoder-Decoder 结构:</p>
<ul>
<li>Encoder 将输入转化为固定长度的向量, Decoder 将固定长度的向量转化为不定长度的文本输出</li>
<li>Encoder 的输出得到的是上下文向量, 这是解码器的初始输入, Decoder 的处理过程与 casual LM 是类似的, 只有 mask 不同</li>
<li>所以严格来说, 上面两张图都是 Decoder 部分的 mask 表示, Prefix LM 左上角九个格子代表经过 Encoder, 在 Encoder 中互相看到过</li>
<li>嗯, 现在才懂, 我是笨蛋</li>
</ul>
</blockquote>
<h3 id="LLM-的训练目标"><a href="#LLM-的训练目标" class="headerlink" title="LLM 的训练目标"></a>LLM 的训练目标</h3><p>通常训练目标是最大似然估计 Maximum Likelihood Estimation (MLE), 即最大化模型生成训练数据中观察到的文本序列的概率</p>
<h3 id="涌现能力"><a href="#涌现能力" class="headerlink" title="涌现能力"></a>涌现能力</h3><p>涌现能力: 模型在训练过程中能够生成出令人惊喜、创造性和新颖的内容或行为</p>
<p>涌现能力的产生原因:</p>
<ol>
<li><p>任务的评价指标不够平滑. 当评价指标很严格时, 比如给四个 emoji 让猜电影名字, 在参数量很大时才能答对; 当评价指标比较宽松 (或者说离散的更 “连续”), 比如多选题, 任务效果就会有持续稳定的提升</p>
</li>
<li><p>复杂任务vs子任务: 子任务的平滑增长, 会带来复杂的整体任务的涌现</p>
</li>
<li><p>Grokking (顿悟) 来解释涌现: 预训练数据是巨大的, 但是与相关问题的训练数据其实很少, 所以当训练数据/模型规模足够大时, 就能看到 Grokking 现象</p>
<blockquote>
<p>Grokking 现象: Grokking 现象是描述训练数据量较少的 ML 任务的，但是任务最小训练数据量必须达到一定的量，才会出现 Grokking 现象。这里有两点，一个是本身训练数据量少，另外是最小数据量要达到临界值。只有同时满足上面两个条件，才有可能出现Grokking现象。</p>
</blockquote>
</li>
</ol>
<h3 id="为何现在的大模型大部分是Decoder-only结构"><a href="#为何现在的大模型大部分是Decoder-only结构" class="headerlink" title="为何现在的大模型大部分是Decoder only结构"></a>为何现在的大模型大部分是Decoder only结构</h3><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/588325646/answer/3383505083">为什么现在的LLM都是Decoder only的架构？ - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/588325646/answer/3383505083">为什么现在的LLM都是Decoder only的架构？ - 知乎</a></p>
<ol>
<li><strong>Encoder 的低秩问题</strong>: Attention 矩阵由一个 $n\times d$ 矩阵和一个 $d\times n$ 矩阵相乘后再加 softmax ($n&gt; d$) 得到, 这种形式的 Attention 矩阵因为低秩问题带来表达能力的下降. Decoder-Only 架构中, Attention 矩阵是一个下三角矩阵, 三角阵的行列式等于对角线元素之积, softmax使得对角线均为整数, 所以行列式为正, 即 Decoder-Only 架构的 Attention 矩阵满秩, 满秩由更强的表达能力</li>
<li><strong>预训练任务难度问题</strong>: 每个位置能接触的信息比其他架构少, 要预测下一个 token 的难度更大, 当模型足够大, 数据足够多时,  Decoder-Only 模型学习通用表征的上限更高</li>
<li><strong>Zero-Shot 性能问题</strong>: prompt 相当于隐式微调, 在Decoder-Only 中可以作用到 Decoder 的每一层, 这使得 Decoder-Only 在 in-context learning 上更有优势. 结果是, Decoder-Only 模型在没有数据作 tuning 时的 Zero-Shot 表现最好</li>
<li><strong>效率问题/训练时间问题</strong>: Decoder-Only 支持复用 KV-Cache, 对多轮对话更友好</li>
<li><strong>causal attention 具有隐式的位置编码功能</strong></li>
<li><strong>依赖轨迹问题</strong>: 跟随 OpenAI 做出了很多工作 </li>
</ol>
<h3 id="大模型架构介绍"><a href="#大模型架构介绍" class="headerlink" title="大模型架构介绍"></a>大模型架构介绍</h3><ul>
<li>以BERT为代表的<strong>encoder-only</strong></li>
<li>以T5和BART为代表的<strong>encoder-decoder</strong></li>
<li>以GPT为代表的<strong>decoder-only</strong>，</li>
<li>以UNILM9为代表的PrefixLM(相比于GPT只改了attention mask，前缀部分是双向，后面要生成的部分是单向的causal mask%)</li>
</ul>
<h3 id="LLM-复读机问题"><a href="#LLM-复读机问题" class="headerlink" title="LLM 复读机问题"></a>LLM 复读机问题</h3><p>LLM 复读机问题 (LLMs Parroting Problem): 大模型生成文本时过度依赖输入文本的复制, 缺乏创造性和独特性. 回答问题时, 可能会简单地复制输入文本的一部分或全部内容</p>
<p>出现原因?</p>
<ol>
<li>数据偏差: 一部分文本出现频率较高</li>
<li>训练目标的限制: 大模型训练通常基于自监督学习的方法训练, 这样的训练目标可能使模型更倾向于生成与输入相似的文本</li>
<li>缺乏多样性的训练数据</li>
<li>模型结构和参数设置: 比如生成策略, 温度系数</li>
</ol>
<p>如何缓解?</p>
<ol>
<li>多样性训练数据</li>
<li>引入噪声: 生成文本时, 引入一些随机性或噪声, 如采样不同的词或短语</li>
<li>温度系数调整: 较高的温度值会增加随机性</li>
<li>Beam搜索调整: Beam搜索在生成过程中维护了一个候选序列的集合, 调整Beam大小和搜索宽度, 可以控制生成文本的多样性和创造性</li>
<li>后处理和过滤: 去除重复的句子或短语, 提高生成文本的质量和多样性, 可以使用文本相似度计算方法</li>
<li>人工干预和控制</li>
</ol>
<h3 id="如何让大模型处理更长的文本"><a href="#如何让大模型处理更长的文本" class="headerlink" title="如何让大模型处理更长的文本"></a>如何让大模型处理更长的文本</h3><ol>
<li>分块处理: 长文本分割成短片段, 逐个输入处理</li>
<li>层次建模: 页分段, 段分句, 句分词</li>
<li>部分生成: 给 prompt, 让模型生成后续内容</li>
<li>注意力机制</li>
<li>模型结构优化</li>
</ol>
<h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><h3 id="中文分词难点"><a href="#中文分词难点" class="headerlink" title="中文分词难点"></a>中文分词难点</h3><p>中文词语组合繁多, 分词容易产生歧义. 集中在分词标准, 切分歧义 (下雨天留客天留人不留) 和未登录词 (新词). 未登录词的影响最大</p>
<h3 id="中文分词算法"><a href="#中文分词算法" class="headerlink" title="中文分词算法"></a>中文分词算法</h3><p>主要分为基于词典的规则匹配方法和基于统计的机器学习方法</p>
<ol>
<li><p>基于词典的分词算法: 本质是字符串匹配</p>
</li>
<li><p>基于统计的分词算法: 本质是序列标注问题. 将语句中的字, 按照他们在词中的位置进行标注. 标注有 B (词开始的一个字), E (词最后一个字), M (词中间的字, 可能多个), S (一个字表示的词)</p>
<p> 比如, HMM 隐马尔科夫模型, CRF 条件随机场, 深度学习</p>
</li>
</ol>
<h2 id="jieba-分词用法及原理"><a href="#jieba-分词用法及原理" class="headerlink" title="jieba 分词用法及原理"></a>jieba 分词用法及原理</h2><p>jieba 是一个准确度和速度都不错的开源项目</p>
<p>四种模式</p>
<ol>
<li>精确模式: 将文本精确切割</li>
<li>全模式: 如果一句话有多重切割方式, 每一种都切割一遍. 切割后有冗余</li>
<li>搜索引擎模式: 精确模式基础上, 对长词语再次切分</li>
<li>paddle 模式: 使用序列标注网络模型实现分词</li>
</ol>
<blockquote>
<p>懒了, 不看了</p>
</blockquote>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="激活函数作用"><a href="#激活函数作用" class="headerlink" title="激活函数作用"></a>激活函数作用</h3><p>给模型引入非线性能力</p>
<ul>
<li><code>Sigmoid</code> 和 <code>tanh</code>: 将输出限制在 (0, 1) 和 (-1, 1) 之间. 这二者适合做概率值的处理, 如 LSTM 中的各种门. 二者不适合做深层网络的训练, 否则会出现梯度的小事</li>
<li><code>ReLU</code> 适合深层网络的训练, 无最大值限制</li>
</ul>
<h3 id="梯度爆炸和梯度消失"><a href="#梯度爆炸和梯度消失" class="headerlink" title="梯度爆炸和梯度消失"></a>梯度爆炸和梯度消失</h3><p>解决</p>
<ol>
<li>截断梯度</li>
<li>残差链接</li>
<li>normalization</li>
</ol>
<h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p><img src="https://github.com/wdndev/llm_interview_note/raw/main/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/image/image_6zR9l2rasJ.png" alt="img"></p>
<p>公式</p>
<script type="math/tex; mode=display">
\sigma(z) = \frac{1}{1+e^{-z}}</script><p>导数公式</p>
<script type="math/tex; mode=display">
\sigma'(z) = \sigma(z)(1-\sigma(z))</script><p>优缺点:</p>
<ul>
<li><p>优点:</p>
<ol>
<li>平滑, 易于求导</li>
<li>取值范围是 (0, 1), 可用于求概率或分类问题</li>
</ol>
</li>
<li><p>缺点:</p>
<ol>
<li>计算量大, 包含幂运算</li>
<li>sigmoid 导数的取值范围是 [0, 0.25], 多次相乘后容易梯度消失</li>
<li>sigmoid 的输出均值不是 0, 随着网络加深会改变数据的原始分布</li>
</ol>
</li>
</ul>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p><img src="https://github.com/wdndev/llm_interview_note/raw/main/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/image/image_UBkA2B4TXM.png" alt="img"></p>
<p>公式</p>
<script type="math/tex; mode=display">
\tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}} = \frac{2}{1+e^{-2z}} - 1</script><p>导数公式</p>
<script type="math/tex; mode=display">
\tanh(x)' = 1-(\tanh(x))^2</script><blockquote>
<p>tanh 函数可以由 sigmoid 函数经过平移和拉伸得到, 可以认为是基于 sigmoid 函数的一种改进</p>
</blockquote>
<ul>
<li>tanh 的输出范围是 (-1, 1), 解决了输出均值非 0 的问题</li>
<li>tanh 的导数取值范围是 (0, 1), 存在梯度消失问题, 但问题会比 sigmoid 好一些</li>
<li>幂运算依然存在, 计算量比较大</li>
</ul>
<h3 id="ReLU-系列"><a href="#ReLU-系列" class="headerlink" title="ReLU 系列"></a>ReLU 系列</h3><h4 id="ReLU-函数"><a href="#ReLU-函数" class="headerlink" title="ReLU 函数"></a>ReLU 函数</h4><p><img src="https://github.com/wdndev/llm_interview_note/raw/main/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/image/image_QNh8iUWMvD.png" alt="img"></p>
<p>优缺点</p>
<ul>
<li>优点:<ol>
<li>z&gt;0 时, ReLU 激活函数的导数恒为常数 1, 避免了 sigmoid 和 tanh 的梯度消失问题</li>
<li>计算复杂度低, 没有幂运算</li>
<li>实验验证发现, 模型收敛的速度更快</li>
<li>z&lt;0 时, ReLU 激活函数的导数恒为常数 0, 保留了数据的关键信息, 将 z&lt;0 的部分置为 0, 产生稀疏矩阵, 使模型具有鲁棒性</li>
</ol>
</li>
<li>缺点:<ol>
<li>z&lt;0 时, ReLU 激活函数的导数恒为常数 0, 可能导致部分神经元不对输入数据做响应, 一部分参数不被更新</li>
<li>可能导致梯度爆炸, 解决方法是梯度截断</li>
<li>ReLU 的输出不是 0 均值</li>
</ol>
</li>
</ul>
<h4 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h4><p><img src="https://github.com/wdndev/llm_interview_note/raw/main/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/image/image_Q2b0vDHio-.png" alt="img" style="zoom: 33%;" /></p>
<p>z&lt;0 部分, 采用一个很小的斜率 0.01. 效果不稳定, 用得不多</p>
<h3 id="GeLU"><a href="#GeLU" class="headerlink" title="GeLU"></a>GeLU</h3><p>这个比较好玩, 我们想一下 <code>ReLU</code> 的设计思路, 输入值大于 0 时是恒等映射, 输入值小于 0 时是置零映射.</p>
<p>参考 <code>ReLU</code> 激活函数, 设计另一个包含恒等映射和置零映射的激活函数, 新的函数应当满足: 输入为一个较大的正值时, 希望为恒等映射; 输入为一个较小的负值时, 希望为置零映射. 只有这两种映射</p>
<p>设 $\phi(x)$ 为输入 $x$ 后输出 $f(x)$ 的可能性, 函数应当满足:</p>
<script type="math/tex; mode=display">
f(x) = x\cdot \phi(x) + 0\cdot (1-\phi (x)) = x\cdot \phi(x)</script><p>也即:</p>
<script type="math/tex; mode=display">
\begin{aligned} 

f(x) & =x \cdot p(X<x)+0 \cdot(1-p(X<x)) \\& =x \cdot p(X<x)

\end{aligned}</script><p><img src="https://github.com/wdndev/llm_interview_note/raw/main/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/image/image_xXCb_c_3eT.png" alt="img"></p>
<p>优点</p>
<ol>
<li>负值区域不为 0, 解决了 Dead ReLU 问题</li>
<li>GeLU 函数处处连续, 光滑可导</li>
</ol>
<h3 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h3><p><img src="https://github.com/wdndev/llm_interview_note/raw/main/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/image/image_b8KrMUlxex.png" alt="img" style="zoom:33%;" /></p>
<p>公式 (这里 $\sigma$ 为 Sigmoid 函数)</p>
<script type="math/tex; mode=display">
f(x) = x\cdot \sigma(x)</script><p>导数公式</p>
<script type="math/tex; mode=display">
f'(x) = f(x) + \sigma(x)\cdot (1-f(x))</script><p>特点: </p>
<ol>
<li>没有上边界, 不会出现梯度消失问题, 和 ReLU 一样</li>
<li>有下边界, 可以产生更强的正则化效果</li>
<li>非单调</li>
<li>处处连续且可导, 容易训练</li>
</ol>
<h3 id="GLU"><a href="#GLU" class="headerlink" title="GLU"></a>GLU</h3><p>PaLM 和 LLaMA 中使用 SwiGLU 替代了 FFN</p>
<script type="math/tex; mode=display">
\operatorname{GLU}(x)=x \otimes \sigma(g(x))</script><p>这里 $g(x)$ 指的是向量 $x$ 经过一层 MLP 或卷积, $\otimes$ 表示两个向量逐元素相乘, $\sigma$ 表示 Sigmoid 函数</p>
<p>简单讲就是通过 Sigmoid 在趋近于 0 时截断函数, 接近于 1 时允许通过, 故称门控激活函数</p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>语言模型的经典定义是对 token 序列的概率分布</p>
<h3 id="自回归语言模型"><a href="#自回归语言模型" class="headerlink" title="自回归语言模型"></a>自回归语言模型</h3><p>序列 $x<em>{1:L}$ 的联合分布 $p(x</em>{1:L})$ 的常见写法是概率的链式法则</p>
<script type="math/tex; mode=display">
p(x_{1:L}) = p(x_1)p(x_2|x_1)p(x_3|x_1, x_2)\cdots p(x_L|x_{1:L-1}) = \prod_{i=1}^L p(x_i|x_{1:i-1})</script><p>token 的分布有 $x<em>i\sim p(x_i|x</em>{1:i-1})^{\frac{1}{T}}$, 这里 $T\geq 0$ 是温度系数, 用于控制能从语言模型中得到多少随机性</p>
<blockquote>
<p>“退火”这个术语来源于冶金学，其中热的金属会逐渐冷却以改变其物理性质。在这里，它类比的是对概率分布进行调整的过程。<strong>“退火”分布是通过将原始概率分布的每个元素都取幂 1/T ，然后重新标准化得到的新分布</strong>。当 T≠1 时，这个过程会改变原始概率分布，因此从”退火”分布中采样得到的结果可能与对每一步的条件分布应用 T 并进行迭代采样的结果不同。</p>
</blockquote>
<h3 id="大模型相关历史"><a href="#大模型相关历史" class="headerlink" title="大模型相关历史"></a>大模型相关历史</h3><h4 id="信息理论-英语的熵-n-gram-模型"><a href="#信息理论-英语的熵-n-gram-模型" class="headerlink" title="信息理论, 英语的熵, n-gram 模型"></a>信息理论, 英语的熵, n-gram 模型</h4><blockquote>
<p>信息论, 香农 TAT</p>
</blockquote>
<p><strong>熵与交叉熵</strong></p>
<p>熵的信息意义: 将样本 $p$ 编码乘比特串所需要的预期比特数的度量. 熵越小, 表明序列的结构性越强, 编码的长度越短</p>
<p>交叉熵的信息意义: 使用由模型 $q$ 给出的压缩方案, 需要多少比特来编码样本 $p$. 交叉熵的上界是熵</p>
<p><strong>N-gram 模型</strong></p>
<p>一个 N-gram 模型中, 关于 $x_i$ 的预测只依赖最后 $n-1$ 个字符. 但是存在问题, 如果 $n$ 太小, 无法捕获长距离依赖, 如果 $n$ 太大, 统计上无法得到概率的有效估计. 这导致语言模型只能限制在语音识别和机器翻译等任务中</p>
<p>N-gram 模型在计算上极其高效, 但是统计上效率低下, 在短上下文长度中与另一个模型联合使用是很有用的</p>
<p><strong>神经语言模型</strong></p>
<p>LSTM, RNN, Transformer, $n$ 一直在变大</p>
<h2 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h2><h2 id="句法分析"><a href="#句法分析" class="headerlink" title="句法分析"></a>句法分析</h2><h2 id="词向量-Embedding"><a href="#词向量-Embedding" class="headerlink" title="词向量 (Embedding)"></a>词向量 (Embedding)</h2><h1 id="大语言模型架构"><a href="#大语言模型架构" class="headerlink" title="大语言模型架构"></a>大语言模型架构</h1><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><h3 id="对-Attention-机制的理解"><a href="#对-Attention-机制的理解" class="headerlink" title="对 Attention 机制的理解?"></a>对 Attention 机制的理解?</h3><p>核心思想是显式的对重要部分动态加权, 关注序列中的重要部分, 忽略不重要的部分.</p>
<h3 id="Self-Attention-的计算步骤"><a href="#Self-Attention-的计算步骤" class="headerlink" title="Self Attention 的计算步骤"></a>Self Attention 的计算步骤</h3><ol>
<li>对输入序列的每个位置, 计算其与其他位置间的相似度得分</li>
<li>对得分进行缩放处理, 防止梯度爆炸</li>
<li>将得分用 softmax 函数转换为注意力权重, 计算每个位置的加权和</li>
<li>用注意力权重对输入序列的所有位置进行加权求和, 得到每个位置的自注意输出</li>
</ol>
<script type="math/tex; mode=display">
\text{Attention} (Q,K, V) = \text{softmax} = \left(\frac{QK^T}{\sqrt{d_k}}  \right)V</script><h3 id="Self-Attention-计算中-如何对-padding-位做-mask"><a href="#Self-Attention-计算中-如何对-padding-位做-mask" class="headerlink" title="Self Attention 计算中, 如何对 padding 位做 mask"></a>Self Attention 计算中, 如何对 padding 位做 mask</h3><p>QK在点积后, 先经过 mask 再进行 softmax, 因此要屏蔽的部分要在 mask 后输出为负无穷</p>
<h3 id="Self-Attention-和全连接层的区别"><a href="#Self-Attention-和全连接层的区别" class="headerlink" title="Self Attention 和全连接层的区别"></a>Self Attention 和全连接层的区别</h3><p>单一的全连接层其实就是 V, Q 和 K 共同产生 Attention Score, 这个分数其实就是给 V 一个权重</p>
<blockquote>
<p>来一个比较形象的比喻吧。如果一个神经网络的任务是从一堆白色小球中找到一个略微发灰的，那么全连接就是在里面随便乱抓然后凭记忆和感觉找，而attention则是左手拿一个白色小球，右手从袋子里一个一个抓出来，两两对比颜色，你左手抓的那个白色小球就是Query。</p>
</blockquote>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="多头注意力机制中每个头为什么要降维"><a href="#多头注意力机制中每个头为什么要降维" class="headerlink" title="多头注意力机制中每个头为什么要降维"></a>多头注意力机制中每个头为什么要降维</h3><p>对每个头降维是为了<strong>增加模型的表达能力和效率</strong></p>
<p>每个 head 是独立的注意力机制, 可以学习不同类型的特征和关系. 多个 head 可以并行地学习多种不同的特征表示, 从而增强了模型的表示能力</p>
<p>使用多个注意力头时, 注意力机制的计算复杂度会增加, 这使得 Transformer 在处理大规模输入时很耗时</p>
<p>为了缓解计算复杂度问题, 每个 head 上要降维, 输入向量通过线性变换被映射到一个较低的维度空间. (也就是生成QKV的三个映射)</p>
<h3 id="Transformer-在哪里权重共享-为什么可以共享"><a href="#Transformer-在哪里权重共享-为什么可以共享" class="headerlink" title="Transformer 在哪里权重共享? 为什么可以共享?"></a>Transformer 在哪里权重共享? 为什么可以共享?</h3><p>Transformer 的权重共享指在堆叠的不同 Transformer 层中, 相同结构用相同的参数</p>
<ol>
<li>嵌入层和输出层, 即 word2vec 和 vec2word 的过程</li>
<li>位置编码的权重, 不同位置的输入在不同层间有相同表示</li>
<li>编码器和解码器的词嵌入权重共享</li>
<li>解码器自注意力中的权重共享 (QKV 权重共享)</li>
</ol>
<h3 id="Transformer-的点积模型做缩放的原因"><a href="#Transformer-的点积模型做缩放的原因" class="headerlink" title="Transformer 的点积模型做缩放的原因"></a>Transformer 的点积模型做缩放的原因</h3><p>控制注意力权重的尺度, 避免在计算过程中出现梯度爆炸的问题. 在 softmax 后, attention 的分布接近一个 one-hot 分布, 这带来梯度消失问题, 导致训练效果差</p>
<p>解决方案:</p>
<ol>
<li>Q 和 K 内积后除以 $\sqrt{d}$, 使 QK 的方差变为 1. 常规的 Transformer 如 BERT 里都是这样做的</li>
<li>不除以 $\sqrt{d}$, 在初始化 Q 和 K 的全连接层时, 其初始化方差要多除以一个 $d$. T5 采用了这样的方法</li>
</ol>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><h3 id="BERT-用字粒度和词粒度的优缺点有哪些"><a href="#BERT-用字粒度和词粒度的优缺点有哪些" class="headerlink" title="BERT 用字粒度和词粒度的优缺点有哪些"></a>BERT 用字粒度和词粒度的优缺点有哪些</h3><p>字粒度和词粒度都是用来文本表示的方法</p>
<ul>
<li>字粒度<ul>
<li>优点: 处理未登录词更方便. 字粒度可以处理任意字符串, 不需要像词粒度那样遇到未登录词就忽略或特殊标记. 字粒度对少见词和低频词可以学习更丰富的字符级别表示, 能捕捉细粒度信息</li>
<li>缺点: 计算复杂度高, 需要更多的训练数据. 输入序列变长, 计算复杂度和内存消耗更多; 对少见词和低频词需要更多训练数据来学习有效的字符级别表示, 否则会过拟合</li>
</ul>
</li>
<li>词粒度<ul>
<li>优点: 计算效率高, 学习更稳定的词级别表示. 减少输入序列长度, 降低模型计算复杂度和内存消耗, 对高频词和常见词有更好的表示能力</li>
<li>缺点: 处理未登录词较难. 只能忽略或采用特殊处理. 多音字等形态复杂的词汇难以捕捉细粒度信息</li>
</ul>
</li>
</ul>
<h3 id="BERT-使用的是-Transformer-里的-Encoder-还是-Decoder"><a href="#BERT-使用的是-Transformer-里的-Encoder-还是-Decoder" class="headerlink" title="BERT 使用的是 Transformer 里的 Encoder 还是 Decoder"></a>BERT 使用的是 Transformer 里的 Encoder 还是 Decoder</h3><p>BERT 仅使用了 Transformer 里的 Encoder 部分, 对其进行了一些修改和自定义的预训练任务</p>
<h3 id="BERT-的-Encoder-与-Decoder-掩码有什么区别"><a href="#BERT-的-Encoder-与-Decoder-掩码有什么区别" class="headerlink" title="BERT 的 Encoder 与 Decoder 掩码有什么区别?"></a>BERT 的 Encoder 与 Decoder 掩码有什么区别?</h3><p>Encoder 使用自注意力掩码和填充掩码 (padding mask, 处理句子变长问题), 此外 Decoder 还要使用 Encoder-Decoder 注意力掩码来避免未来位置信息的泄露, </p>
<blockquote>
<p>查资料时看到一个讲 mask 挺好的<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/572815558">博客</a><br>Transformer 的掩码主要分为三种</p>
<ol>
<li>序列掩码 Sequence Mask. BERT 使用 MLM 双向掩码训练, 此时要隐藏输入序列的某些部分</li>
<li>前瞻掩码 Look-ahead Mask, 也称因果掩码或未来掩码. 自回归模型防止前面的 token 看到后面的 token, 即上三角掩码</li>
<li>填充掩码, Padding Mask. 处理句子变长问题</li>
</ol>
</blockquote>
<h3 id="BERT-为什么要-mask-15-比例的词"><a href="#BERT-为什么要-mask-15-比例的词" class="headerlink" title="BERT 为什么要 mask 15% 比例的词?"></a>BERT 为什么要 mask 15% 比例的词?</h3><p>经验尝试</p>
<h3 id="为什么-BERT-第一句前会加一个-CLS-标志"><a href="#为什么-BERT-第一句前会加一个-CLS-标志" class="headerlink" title="为什么 BERT 第一句前会加一个 [CLS] 标志?"></a>为什么 BERT 第一句前会加一个 [CLS] 标志?</h3><p>BERT 最后一层 [CLS] 对应向量可以作为整句话的语义表示, 从而用于下游的分类任务. 这个无语义信息的符号能更公平地融合文本中各个词的语义信息</p>
<h3 id="BERT-非线性的来源在哪里"><a href="#BERT-非线性的来源在哪里" class="headerlink" title="BERT 非线性的来源在哪里?"></a>BERT 非线性的来源在哪里?</h3><p>FeedForward Network 里的 GeLU 激活函数 和 Self-Attention 里的 softmax</p>
<h3 id="BERT-训练时使用的学习率-warm-up-策略是怎样的-为何要这样做"><a href="#BERT-训练时使用的学习率-warm-up-策略是怎样的-为何要这样做" class="headerlink" title="BERT 训练时使用的学习率 warm-up 策略是怎样的? 为何要这样做?"></a>BERT 训练时使用的学习率 warm-up 策略是怎样的? 为何要这样做?</h3><p>warm-up 策略: 学习率在训练初期从小增大, 提高训练的稳定性, 加快模型收敛. 方法来自 ResNet, 设置如 StepLP, MultiStepLR, ExponentialLR 等函数</p>
<p>具体做法是, 在训练开始的若干个 step(也即一小部分训练数据的迭代次数) 内, 将学习率从一个较小的初始值增加到预定的最大学习率. 学习率线性变化.</p>
<p>学习率 warm-up 解决了 BERT 训练初期的两个问题</p>
<ol>
<li>训练不稳定: 训练初期, 由于模型参数随机初始化和模型的复杂性, 模型处于不稳定状态, 难以收敛, 学习率较小模型训练较为稳定</li>
<li>避免过拟合: BERT 模型需要较长的训练时间来获得高质量的表示, 早期使用较大的学习率可能导致模型在训练初期过度你和训练数据, 降低模型泛化性能</li>
</ol>
<h3 id="BERT-应用中-如何解决长文本问题"><a href="#BERT-应用中-如何解决长文本问题" class="headerlink" title="BERT 应用中, 如何解决长文本问题?"></a>BERT 应用中, 如何解决长文本问题?</h3><ol>
<li>截断与填充: BERT 输入为固定长度</li>
<li>Sliding Window: 长文本分成多个短文本</li>
<li>Hierarchical Model: 分层模型来处理长文本, 底层模型用于处理短文本, 不同片段的表示进行汇总或融合得到整个长文本表示</li>
<li>Longformer, BigBird 等模型: 采用不同注意力机制以处理超长序列</li>
<li>Document-Level Model: 更大的模型</li>
</ol>
<h2 id="MHA-amp-MQA-amp-MGA"><a href="#MHA-amp-MQA-amp-MGA" class="headerlink" title="MHA &amp; MQA &amp; MGA"></a>MHA &amp; MQA &amp; MGA</h2><h3 id="MHA"><a href="#MHA" class="headerlink" title="MHA"></a>MHA</h3><p>多个头本质是多个线性变换层, <strong>MHA 允许模型共同关注来自不同位置的不同表示子空间的信息</strong>, 如果只有一个头, 平均值会削弱这个信息</p>
<p>以代码实现为例, 头数表示为 $H$, 输入通道数为 $C$, 每个头的通道数为 $C’$, batch 数为 $B$, $L$ 代表序列长度. 下面的过程忽略了维度转置过程</p>
<script type="math/tex; mode=display">
\begin{aligned}
Q\times K^T &= (B, L, C) \times (B, C, L)\\

&= (B, H, L, C') \times (B, H, C', L)\\

&= (B, H, L, L)

\end{aligned}</script><p>上面是多头注意力机制得到的 $QK^T$, 这里 $H$ 代表不同的子空间, 所以每个头可以关注输入的不同部分, 从而提升模型的表达能力</p>
<h3 id="MQA"><a href="#MQA" class="headerlink" title="MQA"></a>MQA</h3><p>MQA (Multi-Query Attention) 让所有头共享同一份 K 和 V, 只有每个头的 Q 不同, 从而减少参数量</p>
<p>上面的方法提升了推理速度, 但损失了精度. 后续有论文将多个 MQA 的 checkpoint 融合, 具体实现是对 key 和 value 的头进行平均池化, 再用少量数据 finetuning</p>
<h3 id="GQA"><a href="#GQA" class="headerlink" title="GQA"></a>GQA</h3><p>GQA (Grouped-Query Attention) 是 MQA 的改进, 将 Q 分组, 每组共享一对 K 和 V. 具体实现与 MQA 相似</p>
<h2 id="Flash-Attention"><a href="#Flash-Attention" class="headerlink" title="Flash Attention"></a>Flash Attention</h2><p>Flash Attention 主要用于加速和节省内存</p>
<ol>
<li>计算 softmax 时不需要全量数据, 可以分段计算</li>
<li>反向传播时不存储 attention matrix, 只存储 softmax 归一化系数</li>
</ol>
<h3 id="Flash-Attention-的动机"><a href="#Flash-Attention-的动机" class="headerlink" title="Flash Attention 的动机"></a>Flash Attention 的动机</h3><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/611236756/answer/3132304304">知乎文章</a></p>
<p>过去很多 Efficient Transformer 工作关注减少 FLOPS, 但是计算速度并没有响应地减少, 归根结底是 MAC(Memory Access Cost 存储访问开销) 的问题. 不同硬件模块间的带宽和存储空间有明显差异. 以 A100 为例, SRAM 约 20MB, 但是吞吐量高达 19TB/s; 而 HBM(也即显存) 大小在 40~80GB, 但带宽只有 1.5TB/s. 我们平时常用 HBM, HBM 读写操作很频繁, 但 SRAM 使用率不高. </p>
<p>Flash Attention 希望将计算模块分解, 拆成很多小的计算任务, 从而利用 SRAM</p>
<h3 id="Softmax-Tiling"><a href="#Softmax-Tiling" class="headerlink" title="Softmax Tiling"></a>Softmax Tiling</h3><p>回顾下 softmax 的计算公式, 对于维度为 $B$ 的向量 $x\in \mathbb{R}^B$</p>
<script type="math/tex; mode=display">
\mathrm{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^B e^{x_j}}</script><p>softmax 的指数容易很大, 在计算中容易溢出, 既有深度学习框架中大多使用 softmax 的稳定版本如下, 该版本与原始 softmax 等价</p>
<script type="math/tex; mode=display">
\begin{aligned}

m(x) &= \max ([x_1, x_2, \dots, x_B])\\
f(x) &= [e^{x_1-m(x)}, \dots, e^{x_B-m(x)}]\\
l(x) &= \sum_i f(x)_i\\
\mathrm{softmax}(x) &= \frac{f(x)}{l(x)}

\end{aligned}</script><p>直觉上看, softmax 难以分块计算是因为分母指数求和项依赖于输入的每一个值</p>
<p>下面介绍 softmax 的分块计算方法:</p>
<ol>
<li>考虑一个大小为 2B 的向量 $x\in \mathbb{R}^{2B}$, 将其分为两部分 $x_1$ 和 $x_2$</li>
<li>按照上面稳定版 softmax 的计算方法, 得到 $x_1$ 的softmax 结果</li>
<li>保存 $m(x<em>1)$ 和 $l(x_1)$, 再额外保存两个全局标量 $m</em>{max}$ 和 $l_{all}$, 此时全局标量与局部标量的值是相同的</li>
<li>计算 $x<em>2$ 的 softmax 结果, 然后更新 $m</em>{max}$ 和 $l<em>{all}$ 的值, 新值记为 $m</em>{max}^{new}$ 和 $l_{all}^{new}$ </li>
<li>$m<em>{max}^{new}$ 显然易得, 经计算另一个新值为 $l</em>{all}^{new} = e^{m<em>{max}-m</em>{max}^{new}}l<em>{all} + e^{m</em>{x<em>2}-m</em>{max}^{new}}l(x_2)$<blockquote>
<p>下面是 $l<em>{all}^{new}$ 的推导过程<br>我们计算得到 $l(x_2)$, 这是局部的值, 从此处更新到全局需要用到 $m</em>{max}^{new}$<br>能计算得到 $l(x_2) = \sum_i e^{(x_2)_i} - m(x_2)$, 导致该值为局部值而非全局值的原因是指数项被减数为局部值, 所以要将该 max 值替换为全局值. 所以有以下变换</p>
<script type="math/tex; mode=display">
 \begin{aligned}
 l^{new}(x_2) &= l(x_2)\cdot e^{m_{max}-m_{max}^{new}}\\ 
 &= \sum_i e^{(x_2)_i} - m(x_2) \cdot e^{m(x_2)-m_{max}^{new}}\\
 &= \sum_i e^{(x_2)_i - m_{max}^{new}}
 \end{aligned}</script><p>此时 $l(x<em>2)$ 更新为全局值. 简言之, 要把某个 $l$ 更新为全局值时, 只要将其乘以一项 $e^{m-m</em>{max}^{new}}$, 这里 $m$ 为当前 $l$ 对应的最大值, $m_{max}^{new}$ 为当前最大值</p>
</blockquote>
</li>
<li>基于以上方法, 也可以直接更新 softmax 值, $\mathrm{softmax}^{(new)}(x<em>2) = \frac{\mathrm{softmax}(x_2)\cdot l(x_2)\cdot e^{m(x_2)-m</em>{max}^{new}}}{l_{all}^{new}}$<pre><code>&gt; 类似过程, 不赘述 (其实是看累了\doge) 
</code></pre></li>
</ol>
<p>上述是一个增量计算的过程. 先计算一个分块的局部 softmax 值, 存储起来, 处理完下一个分块, 然后根据此时新的全局最大值和全局指数求和项来更新旧的 softmax 值. 再处理下一个分块, 然后更新</p>
<script type="math/tex; mode=display">
m(x):=\max {i} ~ x{i} \ f(x):=\left[\begin{array}{llll}e^{x_{1}-m(x)} & \ldots & e^{x_{B}-m(x)}\end{array}\right] \ \ell(x):=\sum_{i} f(x)_{i} \ \operatorname{softmax}(x):=\frac{f(x)}{\ell(x)}</script><h3 id="Flash-Attention-的算法流程"><a href="#Flash-Attention-的算法流程" class="headerlink" title="Flash Attention 的算法流程"></a>Flash Attention 的算法流程</h3><p>需要做到</p>
<ol>
<li>在不访问整个输入的情况下计算 softmax; 将输入分块, 在输入快上多次传递, 以增量方式执行 softmax 缩减</li>
<li>后向传播中不能存储中间注意力矩阵, 通过存储归一化因子来减少 HBM 内存的消耗</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/20241121112134.png" alt="Flash Attention 算法流程图"></p>
<h2 id="Transformer-1"><a href="#Transformer-1" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="Transformer-与-RNN"><a href="#Transformer-与-RNN" class="headerlink" title="Transformer 与 RNN"></a>Transformer 与 RNN</h3><p>其实是 Attention + Point-Wise MLP 与 RNN 的区别. Attention 对输入做一个加权和, 加权和进入 point-wise MLP, 得到每个输入点的输出, 这里 Attention 相当于聚合了整个序列的信息; RNN 是把上一时刻的信息输出传入下一个做输入. 二者都实现了语义空间的转换, 都在关注如何有效使用序列的信息</p>
<p><strong>为何使用多头注意力机制</strong></p>
<p>多头保证 Transformer 注意不同子空间的信息, 可以捕捉更丰富的特征信息, 类比 CNN 中同时使用多个滤波器的作用. </p>
<p><strong>为什么 Q 和 K 使用不同的权重矩阵生成, 为何不能用同一个值自身点乘</strong></p>
<ol>
<li>QKV 不同可以保证在不同空间进行投影, 增强表达能力, 提高泛化能力</li>
<li>softmax 函数的性质决定, Attention 做的是一个 soft 版本的 argmax 操作, 得到的向量接近一个 one-hot 向量; 如果 QK 相同, 那么模型大概率会得到一个类似单位矩阵的 attention 矩阵, 这样自注意力就退化成一个 point-wise 线性映射</li>
</ol>
<p><strong>Attention 为何选择点乘而不是加法? 计算复杂度和效果上有什么区别</strong></p>
<ol>
<li>点乘操作可以通过矩阵乘法的方式优化, 可以并行化加快计算过程; 加法操作还需要激活函数等非线性计算, 相同计算复杂度下开销较高. 复杂度对比: 点乘和加法的注意力复杂度都是 $O(d)$, $d$ 为向量维度</li>
<li>数学上, 点乘是衡量两个向量相似度的自然方法, 这种相似性直接影响了注意力权重的大小, 直接且高效</li>
</ol>
<p><strong>为什么 softmax 前要对 attention 进行 scaled (为什么除以 $\sqrt{d_k}$)</strong></p>
<ol>
<li>softmax 内计算量级较大时, 输出近似 one-hot 编码形式, 导致梯度消失</li>
<li>为什么是 $\sqrt{d_k}$? Q K 各分量独立同分布, 均值为 0, 方差为1, 则 QK 点积均值为 0, 方差为 $d_k$. 统计上, 要让 QK 点击方差控制在 1, 需要将其除以 $\sqrt{d_k}$, 让 softmax 更平滑</li>
</ol>
<blockquote>
<p>考虑两个随机变量 $u$ 和 $v$, 长度均为 $d$, 它们的元素都来自均值为 0, 方差为 1 的独立分布. 对于向量的元素 $u_i$ 和 $v_i$ 来说, 点积为 $u_iv_i$. 我们来计算这一项的期望值和方差</p>
<p>期望值有 $E[u_iv_i] = E[u_i]E[v_i] = 0$</p>
<p>方差有</p>
<script type="math/tex; mode=display">Var(u_iv_i) = E[(u_iv_i)^2] - (E[u_iv_i])^2\\ E[u_i^2] = E[v_i^2] = 1 \\ Var(u_iv_i) = 1</script><p>因此向量 $u$ 和 $v$ 的点积是这些 $u<em>iv_i$ 项的总和 $u\cdot v = \sum^d</em>{i=1}u_iv_i$. 均值期望为 $E[u\cdot v] = d\cdot 0 = 0$; 方差期望是 $Var(u\cdot v) = d\cdot 1 = d$., 因此, Q 和 K 点积的方差与 $d_k$ 成正比</p>
<p>再回顾下基础知识, 一个方差为 $\sigma^2$ 的随机变量 $X$ 乘以一个常数 $a$ 后, 其方差变为 $a^2\sigma^2$. 因此, 为了让 Q 和 K 点积的方差控制在 1, 需要将其除以 $\sqrt{d_k}$</p>
</blockquote>
<p><strong>计算 attention score 时如何对 padding 作 mask 操作</strong></p>
<p>padding 位置的 attention score 为负无穷 (一般为 -1000), 使得 softmax 后的值接近 0, 从而不参与加权和计算</p>
<p><strong>为何多头注意力要对每个 head 降维</strong></p>
<p>对每个头降维是为了增加模型的表达能力和效率</p>
<p>每个 head 是独立的注意力机制, 可以学习不同类型的特征和关系. 多个 head 可以并行地学习多种不同的特征表示, 从而增强了模型的表示能力</p>
<p>使用多个注意力头时, 注意力机制的计算复杂度会增加, 这使得 Transformer 在处理大规模输入时很耗时</p>
<p>为了缓解计算复杂度问题, 每个 head 上要降维, 输入向量通过线性变换被映射到一个较低的维度空间. (也就是生成QKV的三个映射)</p>
<p><strong>Transformer 位置编码的优点和缺点</strong></p>
<p>self attention 是位置无关的, 无论句子顺序如何, 计算出的 token 的 hidding embedding 是一样的. transformer 用固定的 positional encoding 来表示 token 在句子中的绝对位置信息</p>
<p><strong>其他位置编码技术, 优缺点?</strong></p>
<p>相对位置编码 (RPE)</p>
<ol>
<li>计算 attention score 和 weighted value 时各加入一个可训练的表示相对位置的参数</li>
<li>生成多头注意力时, 将相对位置编码加入到 Q 和 K 的点积中, 使得绝对位置转换为相对</li>
<li></li>
</ol>

  </article>

  <!-- prev and next -->
  <div class="flex justify-between mt-4 pt-4
    border-t border-[var(--c-sep)] text-sm
    gap-2 text-[var(--c-50)]
  ">
    <div>
      
        <a href="/2024/10/28/photograph/photograph/"
          class="
            transition-all
            flex justify-center
            hover:-translate-x-1
            hover:text-[var(--c-80)]
          ">
          <iconify-icon width="20" icon="mingcute:left-fill" data-inline="false">
          </iconify-icon>
          手机拍照留念
        </a>
      
    </div>
    <div>
      
        <a href="/2024/10/15/env-setup/hexo-blog/"
          class="
            flex 
            justify-center
            hover:translate-x-1 
            transition-transform
            hover:text-[var(--c-100)]
          "
        >
          Hexo 博客代码结构介绍
          <iconify-icon width="20" icon="mingcute:right-fill" data-inline="false"></iconify-icon>
        </a>
      
    </div>
  </div>

  <!-- comment -->
  <div class="article-comments mt-12">
    
  <script src="https://giscus.app/client.js"
  data-repo="421zuoduan/blog-giscus-discussion"
  data-repo-id="R_kgDONKEKag"
  data-category="Announcements"
  data-category-id="DIC_kwDONKEKas4Cj9R8"
  data-mapping="pathname"
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="1"
  data-input-position="top"
  data-theme="preferred_color_scheme"
  data-lang="zh-CN"
  data-loading="lazy"
  crossorigin="anonymous"
  async>
</script>
<script>
  window.onload = function () {
    console.log("giscus loaded");
    const isDark = document.documentElement.classList.contains("dark");
    const giscusFrame = document.querySelector("iframe.giscus-frame");
    giscusFrame.contentWindow.postMessage(
      {
        giscus: {
          setConfig: {
            theme: isDark ? "dark" : "light",
          },
        },
      },
      "https://giscus.app"
    );
  };
</script>


  </div>
</section>
<!-- js inspect -->

<script src="/lib/clipboard.min.js"></script>


<script async src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
  $(document).ready(() => {
    const maraidConfig = {
      theme: "default",
      logLevel: 3,
      flowchart: { curve: "linear" },
      gantt: { axisFormat: "%m/%d/%Y" },
      sequence: { actorMargin: 50 },
    };
    mermaid.initialize(maraidConfig);
  });
</script>



<script src="/lib/fancybox/fancybox.umd.min.js"></script>

<script>
  $(document).ready(() => {
    $('.post-content').each(function(i){
      $(this).find('img').each(function(){
        if ($(this).parent().hasClass('fancybox') || $(this).parent().is('a')) return;
        var alt = this.alt;
        var title = this.title;
        if (alt) $(this).after('<span class="fancybox-alt">' + alt + '</span>');
        if (title) $(this).after('<span class="fancybox-title">' + title + '</span>');
        $(this).wrap('<a class="fancybox-img" href="' + this.src + '" data-fancybox=\"gallery\" data-caption="' + alt + '"></a>')
      });
      $(this).find('.fancybox').each(function(){
        $(this).attr('rel', 'article' + i);
      });
    });

    Fancybox.bind('[data-fancybox="gallery"]', {
        // options
    })
  })
</script>

<!-- tocbot begin -->

<script src="/lib/tocbot/tocbot.min.js"></script>

<script>
  $(document).ready(() => {
      tocbot.init({
        // Where to render the table of contents.
        tocSelector: '.post-toc',
        // Where to grab the headings to build the table of contents.
        contentSelector: '.post-content',
        // Which headings to grab inside of the contentSelector element.
        headingSelector: 'h1, h2, h3',
        // For headings inside relative or absolute positioned containers within content.
        hasInnerContainers: true,
    });
  })
</script>
<!-- tocbot end -->

  </main>
  <footer class="flex flex-col mt-18 mb-12 items-center
  text-[var(--c-50)] text-sm">
  <div class="flex flex-row items-center my-12">
    
    
        
        
            
            
        
        <a class="
            hover:text-[var(--c-theme)]
            hover:bg-[var(--c-20)]
            rounded-lg
            p-2
            my-1
            flex flex-row items-center
            group" title="Github" target="_blank" rel="noopener" href="https://www.github.com/421zuoduan">
            <iconify-icon width="28" icon="mingcute:github-fill"></iconify-icon>
        </a>
    
        
        
            
            
        
        <a class="
            hover:text-[var(--c-theme)]
            hover:bg-[var(--c-20)]
            rounded-lg
            p-2
            my-1
            flex flex-row items-center
            group" title="ZhiHu" target="_blank" rel="noopener" href="https://www.zhihu.com/people/ren-jian-lan-xue">
            <iconify-icon width="28" icon="ri:zhihu-line"></iconify-icon>
        </a>
    

  </div>
  <!-- busuanzi -->
  <div class="mb-6">
    
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!-- Busuanzi Analytics -->
<div class="flex flex-col items-center mb-2">
  <div class="flex flex-row items-center">
    <iconify-icon width="16" icon="ic:round-person" width="18"></iconify-icon>
    <span class="mr-1">访客 Visitors: </span>
    <span id="busuanzi_value_site_uv"></span>
  </div>
  <div class="flex flex-row items-center">
    <iconify-icon width="16" icon="carbon:view-filled" width="18"></iconify-icon>
    <span class="mx-1">浏览量 Page Views:</span>
    <span id="busuanzi_value_site_pv"></span>
  </div>
</div>
<!-- End Busuanzi Analytics -->


  </div>
  <!-- copyright -->
  <div class="flex flex-row items-center gap-2">
    <a class="hover:underline"
      target="_blank"
      href="https://creativecommons.org/licenses/by-nc-sa/4.0/"
    >
      CC BY-NC-SA 4.0
    </a>
    <span>© 2022-2024</span>
    <a class="hover:underline"
    href="https://github.com/chen-yingfa" 
    target="_blank" 
    rel="noopener noreferrer">陈英发</a>
  </div>
  <!-- powered by -->
  <div class="flex items-center gap-1">
    <span>Powered by</span>
    <a class="hover:underline" 
    href="https://hexo.io/" target="_blank" rel="noopener noreferrer">Hexo</a>
    <span>&</span>
    <a href="https://github.com/chen-yingfa/hexo-theme-fengye" 
    class="hover:underline"
    target="_blank"
      rel="noopener noreferrer"
      >
      枫叶 Fengye
    </a>
  </div>

</footer>

  <div class="
    back-to-top
    fixed right-6
    z-1024
    -bottom-20
    rounded-lg
    font-bold
    py-1 px-2
    text-[var(--c-80)]
    bg-[var(--c-20)]
    cursor-pointer
    text-center
    drop-shadow-md
  ">
    <span class="flex justify-center items-center text-sm">
      <span id="scrollpercent"><span>0</span> %</span>
      <iconify-icon width="18" icon="mingcute:arrow-to-up-fill" id="go-top"></iconify-icon>
    </span>
  </div>
  
<script src="/js/main.js"></script>


  <div class="fixed top-0 bottom-0 left-0 right-0 pointer-events-none print:hidden" id="maple"></div>
</body>

</html>
