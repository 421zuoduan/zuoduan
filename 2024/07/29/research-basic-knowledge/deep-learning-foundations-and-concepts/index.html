<!DOCTYPE html>
<html lang="zh-cn,en,default">
<head>
  <meta charset="UTF-8">




<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

  <meta name="author" content="崔若晨 Ruochen Cui">


  <meta name="subtitle" content="崔若晨">


  <meta name="description" content="电子科技大学四年级本科生

A fourth year undergraduate student at UESTC
">


  <meta name="keywords" content="Ruochen Cui,崔若晨,Artificial Intelligence">


<title>《Deep Learning Foundations and Concepts》学习笔记 | Ruochen Cui</title>



<link rel="icon" href="/web_ico.ico">


<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"
/>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">


<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/css/search.css">



<script src="/lib/jquery.min.js"></script>


<script src="/lib/iconify-icon.min.js"></script>


<script src="https://cdn.tailwindcss.com?plugins=typography"></script>
<script>
  tailwind.config = {
    darkMode: "class",
  };
</script>

<script>
  (function () {
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const setting = localStorage.getItem("hexo-color-scheme") || "auto";
    if (setting === "dark" || (prefersDark && setting !== "light"))
      document.documentElement.classList.toggle("dark", true);
    let isDark = document.documentElement.classList.contains("dark");
  })();

  $(document).ready(function () {
    // init icon
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const isDark = document.documentElement.classList.contains("dark");
    $("#theme-icon").attr("icon", isDark ? "ic:round-dark-mode" : "ic:round-light-mode");

    function toggleGiscusTheme() {
      const isDark = document.documentElement.classList.contains("dark");
      const giscusFrame = document.querySelector("iframe.giscus-frame");
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage(
          {
            giscus: {
              setConfig: {
                theme: isDark ? "dark" : "light",
              },
            },
          },
          "https://giscus.app"
        );
      }
    }


    // toggle dark mode
    function toggleDark() {
      let isDark = document.documentElement.classList.contains("dark");
      const setting = localStorage.getItem("hexo-color-scheme") || "auto";
      isDark = !isDark;
      document.documentElement.classList.toggle("dark", isDark);
      $("#theme-icon").attr("icon", isDark ? "ic:round-dark-mode" : "ic:round-light-mode");
      if (prefersDark === isDark) {
        localStorage.setItem("hexo-color-scheme", "auto");
      } else {
        localStorage.setItem("hexo-color-scheme", isDark ? "dark" : "light");
      }
      toggleGiscusTheme();
    }
    $("#toggle-dark").click(toggleDark);

    // listen dark mode change
    window
      .matchMedia("(prefers-color-scheme: dark)")
      .addEventListener("change", (e) => {
        const setting = localStorage.getItem("hexo-color-scheme") || "auto";
        if (setting === "auto") {
          document.documentElement.classList.toggle("dark", e.matches);
          $("#theme-icon").attr(
            "icon",
            e.matches ? "ic:round-dark-mode" : "ic:round-light-mode"
          );
          toggleGiscusTheme();
        }
      });
  });
</script>




<meta name="generator" content="Hexo 7.1.1"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
<body 
  class="
    bg-[var(--c-0)]
    text-[var(--c-80)]
  ">
  <!-- The navigation bar -->
<header class="
    flex flex-row items-center
    w-full
    pr-4
    z-10
    border-b-[1px]
    border-b-[var(--c-border)]
    dark:bg-[var(--c-0)]
    dark:border-b-[var(--c-0)]
    gap-2
    h-[var(--h-header)]
    text-[var(--c-80)]
">
  <!-- Left part -->
  <div class="overflow-hidden h-full flex flex-row items-center">
    <!-- Site Title on the top left -->
    <a href="/" class="
            whitespace-nowrap
            text-2xl
            text-[var(--c-theme)]
            hover:text-[var(--c-theme)]
            pl-4
            font-black
            bg-gradient-to-r from-cyan-500
            to-blue-500 bg-clip-text text-transparent
          ">
      Ruochen Cui
    </a>
  </div>
  <!-- Div for pushing items to both sides -->
  <div class="flex-1"></div>
  <!-- Right part -->
  <div class="flex flex-row items-center z-20 h-full">
    <!-- Page links -->
    <div class="hidden sm:flex flex-row h-full">
      
      
      
      
      
      
      <a href="/./archives" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:inbox-fill" width="22">
        </iconify-icon>
        
        
        <p>Posts</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./publications" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:science-fill" width="22">
        </iconify-icon>
        
        
        <p>Publications</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./about" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:user-info-fill" width="22">
        </iconify-icon>
        
        
        <p>About</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./categories" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:classify-2-fill" width="22">
        </iconify-icon>
        
        
        <p>Categories</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./tags" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:tag-fill" width="22">
        </iconify-icon>
        
        
        <p>Tags</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./index" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:home-2-fill" width="22">
        </iconify-icon>
        
        
      </a>
      
    </div>
    <!-- Icons on the right -->
    <div class="flex flex-row items-center">

      <!-- TODO: Add search icon here -->

      <!-- Dark/light toggle icon -->
      <a class="flex group p-1" title="toggle theme" id="toggle-dark">
        <iconify-icon class="transition-transform
                    group-hover:rotate-[45deg]
                    group-hover:scale-125
                    group-hover:text-[var(--c-theme)]" width="24" id="theme-icon">
        </iconify-icon>
      </a>
      <!-- Icon for dropout menu on small screens -->
      <div class="flex p-1 mx-1 sm:hidden">
        <a class="w-5 h-5" aria-hidden="true" id="open-menu">
          <iconify-icon width="24" icon="mingcute:menu-fill" class="transition-transform hover:scale-125 hover:rotate-[5deg]">
          </iconify-icon>
        </a>
        <a class="w-5 h-5 hidden" aria-hidden="true" id="close-menu">
          <iconify-icon width="24" icon="mingcute:close-circle-fill" class="transition-transform hover:scale-125 hover:rotate-[80deg]">
          </iconify-icon>
        </a>
      </div>
    </div>
  </div>
</header>

<!-- Dropdown menu on small screens -->
<div id="menu-panel" class="
        h-0
        overflow-hidden
        sm:hidden
        w-full
        z-10
        rounded
    ">
  <div id="menu-content" class="
        flex
        flex-row
        justify-center
        items-center
        font-bold
        text-xl
        border-b-[1px]
        relative
        z-20
        border-[var(--c-sep)]
        px-2
        py-2
        -translate-y-full
        transition-transform
        duration-200
        ">
    
    
    
    <a href="/./archives" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:inbox-fill" width="22">
      </iconify-icon>
      <p>
        posts
      </p>
    </a>
    
    
    
    
    <a href="/./publications" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:science-fill" width="22">
      </iconify-icon>
      <p>
        publications
      </p>
    </a>
    
    
    
    
    <a href="/./about" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:user-info-fill" width="22">
      </iconify-icon>
      <p>
        about
      </p>
    </a>
    
    
    
    
    <a href="/./categories" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:classify-2-fill" width="22">
      </iconify-icon>
      <p>
        categories
      </p>
    </a>
    
    
    
    
    <a href="/./tags" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:tag-fill" width="22">
      </iconify-icon>
      <p>
        tags
      </p>
    </a>
    
    
    
    
    <a href="/./index" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:home-2-fill" width="22">
      </iconify-icon>
      <p>
        home
      </p>
    </a>
    
    
  </div>
</div>
  <main>
    <!-- css -->

<link rel="stylesheet" href="/lib/fancybox/fancybox.min.css">

  
<link rel="stylesheet" href="/lib/tocbot/tocbot.min.css">

    <!-- toc -->
    
  <!-- tocbot -->
<nav class="post-toc toc text-sm w-40 relative top-32 right-4 opacity-70 hidden lg:block" style="position: fixed !important;"></nav>


<section class="px-6 max-w-prose mx-auto md:px-0">
  <!-- Post header before content -->
  <header class="py-4">
    <div class="flex flex-col gap-2 pt-4 md:pt-6">
      <!-- Title -->
      <div id="article-title" class="leading-snug">
        <p class="text-3xl font-bold text-[var(--c-100)] mb-4">《Deep Learning Foundations and Concepts》学习笔记</p>
      </div>
      <!-- Meta data -->
      <div>
        <section class="
          flex flex-col gap-x-2 gap-y-1 text-sm text-[var(--c-100)]">
          <div class="flex flex-wrap items-center gap-x-2 gap-y-1">
            <!-- Dates -->
            <div class="flex items-center gap-1">
              <iconify-icon width="18" icon="mingcute:add-circle-fill" ></iconify-icon>
              Created: <time class="w-max">2024-07-29</time>
            </div>
            <div class="flex items-center gap-1">
              <iconify-icon width="18" icon="mingcute:refresh-3-fill" ></iconify-icon>
              Edited: <time class="w-max">2025-03-10</time>
            </div>
          </div>
          <div class="flex flex-wrap items-center gap-x-3 gap-y-3">
            <!-- Author -->
            
              <span class="flex items-center gap-1 group">
                <iconify-icon width="18" icon="mingcute:user-edit-fill" ></iconify-icon>
                <p>myself</p>
              </span>
            

            <!-- Word count -->
            <span class="flex items-center gap-1">
              <iconify-icon width="18" icon="mingcute:book-2-fill" ></iconify-icon>
              <span>14.5k words, 63 min</span>
            </span>
            <!-- Categories -->
            
              <!-- <span class="text-gray-400">·</span> -->
              <span class="flex flex-row items-center gap-1 group hover:underline">
                <iconify-icon class="transition-all group-hover:scale-125 mr-0"
                  width="18"
                  icon="mingcute:classify-2-fill">
                </iconify-icon>
                <a class="article-category-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a>
              </span>
            
          </div>
        </section>
      </div>
      <!-- tags -->
      <div>
        
<div class="flex flex-wrap gap-1">
  
    
      <a href="/tags/math/" 
        class="
          tag
          text-sm
          rounded-full
          px-[5px]
          border-[1px]
          border-[var(--c-theme)]
          text-[var(--c-theme)]
          bg-[var(--c-0)]
          dark:bg-[var(--c-0)]
          dark:drop-shadow-none
          hover:bg-[var(--c-theme)]
          hover:text-[var(--c-0)]
          dark:hover:text-[var(--c-theme-2)]
        ">
        math
      </a>
    
      <a href="/tags/note/" 
        class="
          tag
          text-sm
          rounded-full
          px-[5px]
          border-[1px]
          border-[var(--c-theme)]
          text-[var(--c-theme)]
          bg-[var(--c-0)]
          dark:bg-[var(--c-0)]
          dark:drop-shadow-none
          hover:bg-[var(--c-theme)]
          hover:text-[var(--c-0)]
          dark:hover:text-[var(--c-theme-2)]
        ">
        note
      </a>
    
  
</div>
      </div>
    </div>
  </header>
  <!-- content -->
  <article class="post-content prose m-auto dark:prose-invert">
    <h1 id="Perface"><a href="#Perface" class="headerlink" title="Perface"></a>Perface</h1><p>机器学习的三个核心数学理论:</p>
<ol>
<li>Probability Theory 概率论</li>
<li>Linear Algebra 线性代数</li>
<li>Multivariate Calculs 多元计算</li>
</ol>
<p>本书对概率论提供了详实的介绍, 在附录中提供了线性代数的一些有用结论, 变分法和拉格朗日乘子法的结论</p>
<p>教材电子版, 书中图片的 PDF 和 JPEG 格式文件和课后习题可在<a target="_blank" rel="noopener" href="https://www.bishopbook.com">网站</a>找到</p>
<p>arxiv 上的文章 $\text{arXiv: YYMM.XXXXX}$, $\text{YY}$ 和 $\text{XX}$ 代表第一次上传时的年月, 更新后的版本表示为 $\text{arXiv: YYMM.XXXXXvN}$</p>
<h2 id="Mathematical-notation"><a href="#Mathematical-notation" class="headerlink" title="Mathematical notation"></a>Mathematical notation</h2><p><strong>线性代数</strong></p>
<ul>
<li>列向量 vector $\mathbf{x}$, lower case bold roman letters</li>
<li>矩阵 matrices $\mathbf{M}$, uppercase bold roman letters</li>
<li>转置 transpose $\mathbf{x}^{\mathrm{T}}$</li>
<li>有 $M$ 个元素的列向量 $(w_1, \dots, w_M)$</li>
<li>单位矩阵 identity/unit matrix $\mathbf{I}<em>M$, 单位矩阵中的元素表示为 $\delta</em>{ij}$</li>
<li>值为 1 的列向量 $\mathbf{1}$</li>
<li>矩阵 concat, $\mathbf{a}\oplus \mathbf{b} = (a_1, \dots, a_N, b_1, \dots, b_M)$</li>
<li>绝对值, $|x|$</li>
<li>行列式, $\mathrm{det} \mathbf{A}$</li>
</ul>
<p><strong>概率论</strong></p>
<ul>
<li>$x$ 从分布 $p(x)$ 中采样, $x\sim p(x)$. 在有歧义的地方用 $p_x(\cdot)$ 的下标来代表使用的是哪个概率密度函数</li>
<li>函数$f(x, y)$ 对变量 $x$ 的期望是 $\mathbb{E}_x[f(x, y)]$. 如果对哪个变量取平均没有歧义, 可以省去后缀写作 $\mathbb{E}[x]$</li>
<li>如果 $x$ 的分布取决于 $z$, 那 $x$ 的条件期望为 $\mathbb{E}_x[f(x)|z]$,</li>
<li>$x$ 的方差为 $\mathrm{var}[f(x)]$</li>
<li>两个变量的协方差是 $\mathrm{cov}[\mathbf{x}, \mathbf{y}]$, $\mathrm{cov}[\mathbf{x}] = \mathrm{cov}[\mathbf{x}, \mathbf{x}]$</li>
</ul>
<p>机器学习</p>
<ul>
<li>$\forall m\in \mathcal{M}$ 表示 $m$ 的所有值都在集合 $\mathcal{M}$ 里</li>
<li>$\mathbb{R}$ 代表实数</li>
<li>图中, 邻接点为 $\mathcal{N}_i$, 注意不要和高斯分布 $\mathcal{N}(x|\mu, \sigma^2)$ 混淆</li>
<li>$f[y]$ 代表 $y(x)$ 是一个函数. 有关函数的概念在附录 B 中有详细阐述</li>
<li>大括号${}$ 代表集合</li>
<li><p>$g(x) = \mathcal{O}(f(x))$ 代表 $|f(x|/g(x)|$ 在 $x\rightarrow \infty$ 时有界.</p>
<p>举个栗子, $g(x) = 3x^2 + 2$, 则 $g(x) = \mathcal{O}(x^2)$</p>
<blockquote>
<p>类似时间复杂度</p>
</blockquote>
</li>
<li><p>$\lfloor x\rfloor$ 表示向下取整</p>
</li>
<li>有 $N$ 个独立且同分布的值 $\mathbf{x}<em>1, \dots, \mathbf{x}_N$, 其中每个 $x$ 是一个 $D$ 维向量 $\mathbf{x} = (x_1,\dots, x_D)^{\mathrm{T}}$. 这些观测值 (observation value) 可以组合成一个维度为 $N\times D$ 的数据矩阵 $X$. 矩阵 $X$ 的第 $n$ 行对应于向量 $x_n^{\mathrm{T}}$. 因此, $\mathbf{X}$ 的第 $n, i$ 个元素为第 $n$ 个观测值的第 $i$ 个元素, 可以写作 $x</em>{ni}$. 对于一维变量,</li>
</ul>
<h1 id="Probablities"><a href="#Probablities" class="headerlink" title="Probablities"></a>Probablities</h1><p>uncertainty 分为 systematic uncertainty 和 stochastic uncertainty (也称 noise)</p>
<p>使用概率作为不确定性的度量是贝叶斯的观点</p>
<h2 id="The-Rules-of-Probablity"><a href="#The-Rules-of-Probablity" class="headerlink" title="The Rules of Probablity"></a>The Rules of Probablity</h2><h3 id="A-medical-screening-example"><a href="#A-medical-screening-example" class="headerlink" title="A medical screening example"></a>A medical screening example</h3><h3 id="The-sum-and-product-rules"><a href="#The-sum-and-product-rules" class="headerlink" title="The sum and product rules"></a>The sum and product rules</h3><p>变量的值通常以未知的方式在一个人和另一个人之间变化, 因此被称为随机变量</p>
<p>$X$ 取 $x_i$ 和 $Y$ 取 $y_j$ 的概率记作 $p(X=x_i, Y=y_j)$, 也称 $X=x_i$ 和 $Y=y_j$ 的联合概率</p>
<p>概率的求和法则:</p>
<script type="math/tex; mode=display">
p(X = x_i) = \sum^M_{i=1}p(X=x_i, Y=y_j)\\
\mathrm{otherwise,} p(X) = \sum_{Y}p(X, Y)</script><p>概率的乘积法则:</p>
<script type="math/tex; mode=display">
p(X=x_i, Y=y_j) = p(Y=y_j|X=x_i)p(X=x_i)\\
\mathrm{otherwise,}\quad p(Y|X) = p(Y|X)p(X)</script><h3 id="Bayes’-theorem"><a href="#Bayes’-theorem" class="headerlink" title="Bayes’ theorem"></a>Bayes’ theorem</h3><script type="math/tex; mode=display">
p(Y|X) = \frac{p(X|Y)p(Y)}{p(X)}</script><p>贝叶斯定理中的分母目的是对左边关于 $Y$ 的条件概率分布可取所有值作归一化</p>
<p>如果 $X$ 由许多个互斥的事件 $x_i$ 组成, 贝叶斯公式可以写成</p>
<script type="math/tex; mode=display">
p(Y|X) = \sum_{i=1}^N \frac{p(x_i|Y)p(Y)}{p(x_i)}</script><h3 id="Medical-screening-revisitied"><a href="#Medical-screening-revisitied" class="headerlink" title="Medical screening revisitied"></a>Medical screening revisitied</h3><h3 id="Prior-and-posterior-probablities"><a href="#Prior-and-posterior-probablities" class="headerlink" title="Prior and posterior probablities"></a>Prior and posterior probablities</h3><p>先验概率: 得到测试结果前就知道的概率.</p>
<p>后验概率: 如果一个人经过测试, 我们可以利用先验概率和贝叶斯定理计算后验概率 $p(C|T)$, 这是我们观察到检验结果 $T$ 后的概率</p>
<h3 id="Independent-variables"><a href="#Independent-variables" class="headerlink" title="Independent variables"></a>Independent variables</h3><p>两个变量是独立的, 一定有:</p>
<script type="math/tex; mode=display">
p(X, Y) = p(X)p(Y)</script><h2 id="Probability-Densities"><a href="#Probability-Densities" class="headerlink" title="Probability Densities"></a>Probability Densities</h2><p>Probability Densities 概率密度的提出是因为, 对于连续变量, 观测到一个特定值的概率在极高的精度下将趋近为 0.</p>
<p><strong>概率密度</strong>: 定义连续变量 $x$ 上的概率密度 $p(x)$, 使得当 $\delta x\rightarrow 0$ 时, $x$ 落在区间 $(x, x+\delta x)$ 的概率由 $p(x)\delta x$ 给出</p>
<script type="math/tex; mode=display">
p(x\in(a,b)) = \int_a^b p(x)\mathrm{d}x\\
且有\quad p(x)\geq0, \int_{-\infty}^{+\infty}p(x)\mathrm{d}x = 1</script><p>cumulative distribution function <strong>分布函数</strong>即</p>
<script type="math/tex; mode=display">
P(z) = \int_{-\infty}^{z}p(x)\mathrm{d}x</script><p>$P(z)$ 还满足 $P’(x) = p(x)$</p>
<p>这里用一张图片展示会更直接</p>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/image-20240801131829711.png" alt=""></p>
<blockquote>
<p>概率密度函数描述的是 $x$ 附近的概率, 叫它密度函数, 可以类比一根无限长的质量分布不均的线的密度, 这根线的任意一点质量均为 0, 但是线密度衡量了质量的分布情况”. 用一个更简单的例子来看, 一个点是没有长度的, 我们不能说一个点的长度是一个点的长度</p>
<p>$P(z)$ 还满足 $P’(x) = p(x)$ 的理解: 离散的角度看, $P(z)$ 是 $p(x)$ 的累加, 当加到 $p(x_0)$ 时, 此时的变化率为 $p(x_0)$; 从连续的角度是变限积分的事情了, “一个点的宽度为0（其实严格来说是测度）”</p>
<p>re: 感谢 xbd 同志的解释</p>
</blockquote>
<p>概率的求和乘积法则和贝叶斯定理也适用于概率密度的计算</p>
<script type="math/tex; mode=display">
p(\mathbf{x}) = \int p(\mathbf{x}, \mathbf{y})\mathrm{d}\mathbf{y}\\
p(\mathbf{x}, \mathbf{y}) = p(\mathbf{y}|\mathbf{x})p(\mathbf{x})\\
p(\mathbf{y}|\mathbf{x}) = \frac{p(\mathbf{x}|\mathbf{y})}{p(\mathbf{x})}\\
p(\mathbf{x}) = \int p(\mathbf{x}|\mathbf{y})p(\mathbf{y})\mathrm{d}\mathbf{y}</script><h3 id="Example-distributions"><a href="#Example-distributions" class="headerlink" title="Example distributions"></a>Example distributions</h3><p>以下给出一些经典的分布</p>
<ol>
<li><p>指数分布</p>
<script type="math/tex; mode=display">
p(x|\lambda) = \lambda\mathrm{exp}(-\lambda x)</script></li>
<li><p>拉普拉斯分布, 是指数分布的变体, 具有一个峰值</p>
<script type="math/tex; mode=display">
p(x|\mu, \gamma) = \frac{1}{2\gamma}\mathrm{exp}\left(-\frac{|x-\mu|}{\gamma}\right)</script></li>
<li><p>Dirac delta function, 只有 $x = 0$ 的值非零, 且该值极大. 这一函数可用来表示脉冲</p>
<script type="math/tex; mode=display">
p(x|\mu) = \delta(x-\mu)</script><blockquote>
<p>“Dirac delta函数并不是传统意义上的函数，因为它在单点处取无限大值，这在普通函数中是不允许的。它更多地被视为一种数学工具，用于描述瞬间的冲击或脉冲。”, 通常记作 $δ(x)$</p>
</blockquote>
<p>若有有限集 $\mathcal{D} = {x_1, \dots, x_N}$, 则可以使用 Dirac delta function 构造经验分布 (empirical distribution), 经验分布的概率密度函数积分为 1</p>
<script type="math/tex; mode=display">
p(x|\mathcal{D}) = \frac{1}{N}\sum^N_{n=1}\delta(x-x_n)</script><p>性质</p>
<ol>
<li><p>筛选性质. 狄拉克函数 $\delta(x-x_n)$ 作为被积函数的一部分时, 积分结果只取决于 delta 函数在特定点的值</p>
<script type="math/tex; mode=display">
\int_{-\infty}^{\infty}f(x)\delta(x-x_i)\mathrm{d}x = f(x_i)\\</script></li>
</ol>
</li>
</ol>
<blockquote>
<p>“经验分布函数（empirical distribution function），也称为经验分布或经验累积分布函数（empirical cumulative distribution function, ECDF），是统计学中用于估计未知总体分布函数的一种方法。它是基于样本数据构建的，反映了样本中观测值的累积频率。”, informally, 经验分布是和有限集 $\mathcal{D}$ 自身有关的</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/image-20240801154616873.png" alt=""></p>
<h3 id="Expectations-and-convariances"><a href="#Expectations-and-convariances" class="headerlink" title="Expectations and convariances"></a>Expectations and convariances</h3><h4 id="Expectations"><a href="#Expectations" class="headerlink" title="Expectations"></a>Expectations</h4><p>期望是 $f(x)$ 在概率分布 $p(x)$ 下的加权平均</p>
<script type="math/tex; mode=display">
\mathbb{E}[f] = \sum_{x}p(x)f(x)\\
\mathbb{E}[f] = \int p(x)f(x)\mathrm{d}x</script><p>多元分布密度函数的期望, 下标变量即取均值的变量 $\mathbb{E}_x[f(x, y)]$</p>
<blockquote>
<p>note: 此时上式是关于 $y$ 的函数</p>
</blockquote>
<p>条件概率的期望有</p>
<script type="math/tex; mode=display">
\mathbb{E}_x[f|y] = \sum_x p(x|y)f(x)\\
\mathbb{E}_x[f|y] = \int p(x|y)f(x)\mathrm{d} x</script><h4 id="Convariances"><a href="#Convariances" class="headerlink" title="Convariances"></a>Convariances</h4><p>方差是对 $f(x)$ 在 $\mathbb{E}[f(x)]$ 值附近变化的度量</p>
<script type="math/tex; mode=display">
\mathrm{var}[f] = \mathbb{E}\left[(f(x) - \mathbb{E}[f(x)])^2\right]</script><p>化简后得到</p>
<script type="math/tex; mode=display">
\mathrm{var}[f] = \mathbb{E}\left[f(x)^2\right] - \mathbb{E}\left[f(x)\right]^2\\
对于变量x有, \mathrm{var}[x] = \mathbb{E}\left[x^2\right] - \mathbb{E}\left[x\right]^2</script><p>协方差是两个随机变量 $x$ 和 $y$ 共同变化程度的度量</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathrm{cov}[x, y] &= \mathbb{E}_{x,y}\left[\{x-\mathbb{E}[x]\}\{y-\mathbb{E}[y]\}\right] \\
&= \mathbb{E}_{x,y}[xy] - \mathbb{E}
[x]\mathbb{E}[y]
\end{aligned}</script><p>若 $x$ 和 $y$ 独立, 则协方差为 0; 两向量的协方差为矩阵 (协方差矩阵)</p>
<blockquote>
<p>“协方差衡量的是两个变量之间的线性关系，即一个变量相对于另一个变量的变化趋势。如果两个变量倾向于一起增加或减少，协方差为正；如果一个增加时另一个减少，协方差为负；如果两者变化无关，协方差接近于零。”</p>
<p>“假设我们有两个向量 $\mathbf{X}=[X_1,X_2,…,X_n]$ 和 $\mathbf{Y}=[Y_1,Y_2,…,Y_n]$，它们的协方差矩阵 $C$ 是一个 2*2 矩阵”</p>
<p>协方差矩阵的对角线元素表示每个向量的方差，非对角线元素表示两个向量之间的协方差</p>
</blockquote>
<p>我们定义 $\mathrm{cov}[\mathbf{x}] \equiv \mathrm{cov}[\mathbf{x}, \mathbf{x}]$ ($\equiv$ 恒等于)</p>
<h2 id="The-Gaussian-Distribution"><a href="#The-Gaussian-Distribution" class="headerlink" title="The Gaussian Distribution"></a>The Gaussian Distribution</h2><p>高斯分布的定义</p>
<script type="math/tex; mode=display">
\mathcal{N}(x|\mu, \sigma) = \frac{1}{\sqrt{(2\pi\sigma^2)}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}</script><p>方差的倒数 $\beta = \frac{1}{\sigma^2}$ 称为精度, 显然高斯分布的概率密度函数是归一化的, 有 $\int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2)\mathrm{d}x = 1$</p>
<h3 id="Mean-and-variance"><a href="#Mean-and-variance" class="headerlink" title="Mean and variance"></a>Mean and variance</h3><p>均值 $\mu$, 方差 $\sigma^2$</p>
<blockquote>
<p>这里均值是 $x$ 的均值</p>
</blockquote>
<p>$x$ 的均值, 也即高斯分布的一阶矩</p>
<script type="math/tex; mode=display">
\mathbb{E}[x] = \int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2)x\mathrm{d}x = \mu</script><p>高斯分布的二阶矩</p>
<script type="math/tex; mode=display">
\mathbb{E}[x^2] = \int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2)x^2\mathrm{d}x = \mu^2 + \sigma^2</script><p>是故方差的计算也可以表示为</p>
<script type="math/tex; mode=display">
\mathrm{var}[x] = \mathbb{E}[x^2] - \mathbb{E}[x]^2 = \sigma^2</script><h3 id="Likelihood-function"><a href="#Likelihood-function" class="headerlink" title="Likelihood function"></a>Likelihood function</h3><p>给定一个有限集合 (a Finite set of observations) $\mathsf{x} = (x_1, \dots, x_N)$, 估计一个分布的问题被称为密度估计. 密度估计是一个不适定问题, 因为存在无穷多个概率分布可以采样出这个有限集合</p>
<blockquote>
<p>适定问题, 满足以下所有条件的问题</p>
<ol>
<li>解必须存在</li>
<li>解必须唯一</li>
<li>解能根据初始条件连续变化，不会发生跳变，即解必须稳定</li>
</ol>
</blockquote>
<p>从同一分布中独立抽取的数据点称为独立同分布, 通常记为 $\mathrm{i.i.d}$ 或 $\mathrm{IID}$</p>
<p>独立同分布的一组数据, 多个独立事件的联合概率是由每个事件的边缘概率的乘积给出, 所以我们可以将这组数据的概率密度相乘, 得到似然函数</p>
<script type="math/tex; mode=display">
p(\mathsf{x}|\mu, \sigma^2) = \prod^N_{n=1}\mathcal{N}(x_n|\mu, \sigma^2)</script><p>最大似然估计: 使用观测数据集 (an observed data set), 寻找使似然函数最大化的参数值,  确定概率分布中的参数.</p>
<blockquote>
<p>最大化给定数据的参数的概率似乎更自然, 而不是最大化给定参数的数据的概率. 事实上, 这两个标准是相关的 (?)</p>
<p>为什么似然函数是每个数据点概率密度的乘积呢? 如前所说, 这些数据点是独立同分布的, 且独立同分布的多个变量的联合概率密度函数等于各变量概率密度函数之积. 所以我想从采样自同一分布的多个数据点中估计原始分布, 也即估计其联合概率密度函数, 所以似然函数这样定义.</p>
</blockquote>
<p>我们想要最大化似然函数, 可以转化成最大化对数似然函数, 这是因为对数是其参数的单调递增函数, 这简化了后续的数学分析, 也即我们最大化以下函数</p>
<script type="math/tex; mode=display">
\mathrm{ln}p(\mathsf{x}|\mu, \sigma^2) = -\frac{1}{2\sigma^2}\sum^{N}_{n=1}(x_n-\mu)^2 - \frac{N}{2}\mathrm{ln}\sigma^2- \frac{N}{2}\mathrm{ln}(2\pi)</script><p>当上式最大时, 偏导为 0. 所以我们对两侧分别对 $\mu$ 和 $\sigma^2$ 求偏导, 从而得到两个参数的极大似然估计</p>
<script type="math/tex; mode=display">
\mu_{\mathrm{ML}} = \frac{1}{N}\sum^N_{n=1}x_n\\
\sigma^2_{\mathrm{ML}} = \frac{1}{N}\sum_{n=1}^{N}(x_n-\mu_{\mathrm{ML}})^2</script><h3 id="Bias-of-maximum-likelihood"><a href="#Bias-of-maximum-likelihood" class="headerlink" title="Bias of maximum likelihood"></a>Bias of maximum likelihood</h3><p>下面用单变量高斯分布说明极大似然估计的缺点</p>
<p>对于从一组观测数据极大似然估计出的高斯分布参数 $\mu$ 和 $\sigma^2$, 对于真实高斯分布参数 $\mu$ 和 $\sigma^2$, 很明显有</p>
<script type="math/tex; mode=display">
\mathbb{E}[\mu_{\mathrm{ML}}] = \mu\\
\mathbb{E}[\sigma^2_{\mathrm{ML}}] = \left(\frac{N-1}{N}\right)\sigma^2</script><p>使用极大似然估计, 得到的方差会略小于真实方差. 这种现象是极大似然估计的偏差 (bias)</p>
<p>原因: <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/20099757?utm_psn=1803168787333271553">参考链接</a></p>
<p>所以我们有两种方法得到极大似然估计的<strong>方差</strong></p>
<ol>
<li>有真实均值 $\mu$ 时, 可以通过 $\sigma^2<em>{\mathrm{ML}} = \frac{1}{N}\sum^N</em>{n=1}{x_i-\mu}$</li>
<li>没有真实均值时, 通过上面极大似然估计计算, 先计算 $\mu<em>{\mathrm{ML}}$, 再计算 $\sigma</em>{\mathrm{ML}}^2$</li>
</ol>
<h3 id="Linear-regression"><a href="#Linear-regression" class="headerlink" title="Linear regression"></a>Linear regression</h3><p>回归问题的目标是给定输入, 能得到想要的输出值</p>
<p>我们假设给定 $\mathsf{x}$ 值, 相应的 $\mathsf{t}$ 值服从高斯分布; $\mathsf{t}$ 的均值在 $x$ 时为多项式曲线 $y(x:\mathsf{w})$, 多项式系数 $\mathsf{w}$ 未知</p>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/image-20240803204132394.png" alt=""></p>
<blockquote>
<p>多项式曲线是由多项式函数描述的曲线</p>
<script type="math/tex; mode=display">
y(x) = a_nx^n + a_{n-1}x^{n-1}+\cdots+a_2x^2+a_1x+a_0</script></blockquote>
<p>符合以上描述的概率分布为</p>
<script type="math/tex; mode=display">
p(t|x, \mathsf{w}, \sigma^2) = \mathcal{N}(t|y(x,\mathsf{w}), \sigma^2)</script><p>极大似然函数为</p>
<script type="math/tex; mode=display">
p(\mathsf{t}|x, \mathsf{w}, \sigma^2) = \prod_{n=1}^N\mathcal{N}(t_n|y(x_n,\mathsf{w}), \sigma^2)</script><p>对数似然函数为</p>
<script type="math/tex; mode=display">
\mathrm{ln}p(\mathsf{t}|\mathsf{x}, \mathsf{w}, \sigma^2) = -\frac{1}{2\sigma^2}\sum^N_{n=1}\{y(x_n,\mathsf{w})-t_n\}^2 - \frac{N}{2}\mathrm{ln}\sigma^2-\frac{N}{2}\mathrm{ln}(2\pi)</script><p>我们想最大化有关 $\mathsf{w}$ 的对数似然函数, 也即求以下函数最小, 该函数为平方差函数 (此时函数随 $\mathsf{w}$ 单调, $\sigma^2$ 的值不影响)</p>
<script type="math/tex; mode=display">
E(\mathsf{w}) = \frac{1}{2}\sum^N_{n=1}\{y(x_n, \mathsf{w})-t_n\}^2</script><p>前面我们假设 $(\mathsf{x}, \mathsf{t})$ 是符合高斯分布的, 其实也是高斯分布上的点加了高斯扰动, 所以可以说, 我们去做拟合时</p>
<blockquote>
<p>假设噪声服从高斯分布的情况下, 拿既有数据去拟合其分布, 最大化似然函数等价于最小化平方和误差函数</p>
</blockquote>
<p>同理, 使用上面对数似然函数对 $\sigma^2$ 求导, 得到</p>
<script type="math/tex; mode=display">
\sigma^2_{\mathrm{ML}} = \frac{1}{N}\sum_{n=1}^N\{y(x_n,\mathsf{w}_{\mathrm{ML}})-t_n)\}</script><p>得到 $\mathsf{w}<em>{\mathrm{ML}}$ 和 $\sigma^2</em>{\mathrm{ML}}$ 后, 得到线性回归的概率分布</p>
<script type="math/tex; mode=display">
p(t|x, \mathsf{w}_{\mathrm{ML}}, \sigma^2_{\mathrm{ML}}) = \mathcal{N}(t|y(x, \mathsf{w}_{\mathrm{ML}}), \sigma^2_{\mathrm{ML}}))</script><h2 id="Transformation-of-Densities"><a href="#Transformation-of-Densities" class="headerlink" title="Transformation of Densities"></a>Transformation of Densities</h2><blockquote>
<p>TODO: 没懂</p>
</blockquote>
<p>这部分内容讨论概率密度函数在变量非线性变化下如何变换. 这部分内容在名为 “normalizing flows” 标准化流的生成模型中很重要.</p>
<p>我们对 $x$ 施加变换 $x = g(y)$, 则函数 $f(x)$ 变为新函数 $\tilde{f}(y) = f(g(y))$</p>
<blockquote>
<p>这部分内容看教材没看懂, 决定找<a target="_blank" rel="noopener" href="https://mqshen.gitbooks.io/prml/content/Chapter1/probability/probability_densities.html">参考资料 </a> :D</p>
<p>在变量的非线性变化下，概率密度由一个简单的函数通过 Jacobian 因子变换得到。现在，考虑概率密度 $p_x(x)$，与它对应的关于新变量 $y$ 的密度是 $p_y(y)$。观测区间 $ (x,x+δx)$ 变换为区间 $(y,y+δy)$，当 $δx$ 很小时，我们有 $p_x(x)δx≃p_y(y)δy$ 即：</p>
<script type="math/tex; mode=display">
\begin{aligned}

p_y(y) &= p_x(x)\left| \frac{\mathrm{d}x}{\mathrm{d}y}\right|\\

&= p_y(y)(g(y))\left| g'(y) \right|\\

&= p_x(g(y))sg'(y)\qquad (\mathrm{取s\in \{-1,+1\}, g'(y)=s\left| g'(y) \right|})


\end{aligned}</script><p>这一性质的表明, 概率密度函数的最大值取决于变量的选择</p>
<p><strong>与函数变换操作不同</strong>. 直接在原始变量 $x$ 上最大化密度函数 $p_x(x)$ 得到的结果，与先将 $x$ 变换到 $y$，然后在 $y$ 上最大化密度函数 $p_y(y)$，最后再将结果变换回 $x$ 得到的结果，这两者可能是不一样的。证明如下:</p>
<p>我们对 $\tilde{f}(\hat{y})$ 两边求导, 得到 $\tilde{f}’(\hat{y}) = f’(g(\hat{y}))g’(\hat{y})$, 而对 $p_y(y)$ 求导 (这里将正负号记为 $s$), 得到  $p’_y(y) = sp’_x(g(y)){g’(y)}^2 +sp_x(g(y))g’’(y) $, 可以看到 $p_y’(y)$ 有第二项, 这导致最大值所在位置依据变量的不同而有所不同</p>
</blockquote>
<p>一个简单的例子, 这里左侧绿色线是在 $y$ 上最大化密度函数 $p_y(y)$, 最后将结果变换回 $x$ 的结果, 而洋红色线是实际的 $p_y(y)$ 的线</p>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/image-20240807203905582.png" alt=""></p>
<h3 id="Multivariate-distributions"><a href="#Multivariate-distributions" class="headerlink" title="Multivariate distributions"></a>Multivariate distributions</h3><p>考虑一个 $D$ 维变量 $\mathbf{x} = (x_1, \dots, x_D)^\mathrm{T}$, 我们通过 $\mathbf{x} = \mathbf{g}(\mathbf{y})$, 得到 $\mathbf{y} = (y_1, \dots, y_D)^\mathrm{T}$, 这里我们的讨论仅限于二者处于相同维度的情况. 此时, 多变量概率密度函数的转换公式如下</p>
<script type="math/tex; mode=display">
p_\mathbf{y}(\mathbf{y}) = p_\mathbf{x}(\mathbf{x})|\mathrm{det}\mathbf{J}|</script><p>其中 $\mathbf{J}$ 是雅可比矩阵 (Jacobian matrix), 雅可比矩阵内的值由变化中的偏导数给出</p>
<script type="math/tex; mode=display">
\mathbf{J} = 
\begin{bmatrix}
 \frac{\partial g_1}{\partial y_1} & \cdots & \frac{\partial g_1}{\partial y_D}\\
 \vdots & \ddots & \vdots\\
 \frac{\partial g_n}{\partial y_1} & \cdots & \frac{\partial g_n}{\partial y_n}\\
 \end{bmatrix}</script><p>直观上来看, $x$ 到 $y$ 的变换是空间上的变化, 雅可比矩阵的绝对值代表着单位空间的变化比率, 意义是区域 $\Delta \mathbf{x} 和 \Delta \mathbf{y}$ 的概率质量相同</p>
<h2 id="Information-Theory"><a href="#Information-Theory" class="headerlink" title="Information Theory"></a>Information Theory</h2><h3 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h3><p>考虑一个离散的随机变量 $x$, 其信息量可以理解成 “令人惊讶的信息的多少”, 信息越不常见, 信息量越高.</p>
<p>为什么使用对数形式定义信息熵? 考虑两个不相关的事件 A 和 B, 由于不相关, 这里 A 与 B 共同出现的信息应等于二者独立出现的信息之和, 这符合对数形式的运算规则. 所以我们使用对数, 负号保证信息是非零的, 底数的选择是任意的, 这里选用 2 是约定俗成的, $h(x)$ 的单位是比特(bits)</p>
<script type="math/tex; mode=display">
h(x) = -\log_2p(x)</script><p>平均信息量是关于 $p(x)$ 的期望, 也叫做 随机变量 $x$ 的熵</p>
<script type="math/tex; mode=display">
\mathrm{H}[x] = -\sum_{x}p(x)\log_2p(x)</script><p>由于 $\lim_{\epsilon\rightarrow 0}(\epsilon\ln\epsilon) = 0$, 所以在 $p(x) = 0$ 时有 $p(x)\ln p(x)=0$, 此时没有信息量</p>
<p><strong>非均匀分布的熵比均匀分布的熵小</strong>, 比如我们用 3 位编码来表示 8 个事件, 给不太可能发生的事件用较长的编码, 这样我们能获得更短的平均编码长度</p>
<blockquote>
<p>这里 3 位编码和 8 个事件有什么关系呢? 假设八个事件的发生概率符合均匀分布,每个都是 $\frac{1}{8}$, 这时熵的值为 $\mathrm{H}[x] = -8\times\frac{1}{8}\log_2\frac{1}{8} = 3\mathrm{bits}$, 也就是说, 用 3 比特的信息就能表示所有时间, 也即用 3 位 2 进制数就能表示所有的事件. 不能用更短的 01 字符串来表示所有事件, 否则无法将所有事件无歧义的分解为不同的字符串 (编码不能有部分相同的地方, 如编码 0, 10, 110, 1110, 111100, 111110, 此时 11001110 仅有唯一表示方法, 是 c, a, d)</p>
</blockquote>
<p>后续的讨论中, 我们使用 $nats$ (“from natrual logarithm”) 作为熵的单位</p>
<h3 id="Physics-perspective"><a href="#Physics-perspective" class="headerlink" title="Physics perspective"></a>Physics perspective</h3><p>在物理学中, 熵是无序程度的度量. 当熵最小时, $p_0(x) = 1$, 其他 $p_i(x) = 0$; 当熵最大时, $p_i(x) = \frac{1}{M}$, 其中 $M$ 为可以取值的个数</p>
<blockquote>
<p>这里找熵的最大值时, 可以使用拉格朗日乘子法或者 Jensen 不等式导出. 使用拉格朗日乘子法时, 也即最大化以下公式</p>
<script type="math/tex; mode=display">
\tilde{\mathrm{H}} = -\sum_i p(x_i)\ln p(x_i) + \lambda\left(\sum_i p(x_i - 1)   \right)</script></blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/image-20240808115617466.png" alt=""></p>
<h3 id="Differenctial-entropy"><a href="#Differenctial-entropy" class="headerlink" title="Differenctial entropy"></a>Differenctial entropy</h3><p>从离散型随机变量的熵定义到连续型随机变量的熵, 我们得到微分熵. 假设 $p(x)$ 是连续型随机变量的概率密度函数, 我们将 $x$ 定义域分成若干宽度为 $\Delta$ 的区间. 根据均值定理, 对于每个区间, 一定存在一个值 $x_i$ 在范围 $i\Delta\leq x_i\leq (i+1)\Delta$ 内有</p>
<script type="math/tex; mode=display">
\int_{i\Delta}^{(i+1)\Delta}p(x)\mathrm{d}x = p(x_i)\Delta\\

\mathrm{H}_\Delta = -\sum_ip(x_i)\Delta\ln(p(x_i)\Delta) = -\sum_i p(x_i)\Delta\ln p(x_i)-\ln\Delta</script><blockquote>
<p>均值定理</p>
<ol>
<li>拉格朗日中值定理:</li>
</ol>
<p>如果函数  $f(x)$ 在闭区间  $[a, b]$ 上连续，并且在开区间  $(a, b)$ 上可导，那么在  $(a, b)$ 内至少存在一点 $c$，使得</p>
<script type="math/tex; mode=display">
f'(c) = \frac{f(b) - f(a)}{b - a}</script><ol>
<li>柯西均值定理</li>
</ol>
<p>如果函数  $f(x)$ 和  $g(x)$ 在闭区间  [a, b] 上连续，并且在开区间 (a, b) 上可导，且 $g’(x) \neq 0$ 对于所有  $x$属于 $(a, b)$，那么在 $(a, b)$ 内至少存在一点 $c$，使得</p>
<script type="math/tex; mode=display">
\frac{f'(c)}{g'(c)} = \frac{f(b) - f(a)}{g(b) - g(a)}</script></blockquote>
<p>我们有 $\sum_i p(x_i)\Delta = 1$, 当 $\Delta\rightarrow 0$ 时, 有</p>
<script type="math/tex; mode=display">
\lim_{\Delta\rightarrow 0}\left\{-\sum_i p(x_i)\Delta\ln p(x_i)   \right\} = -\int p(x)\ln p(x)\mathrm{d}x</script><p>这里右侧的量即微分熵. 注意到上面离散形式的熵和连续形式的熵差了一个 $-\ln \Delta$, 所以离散形式的熵在 $\Delta\rightarrow0$ 时发散, 这也说明精确指定一个连续变量需要很多信息量.</p>
<p>微分熵的定义如下</p>
<script type="math/tex; mode=display">
\mathrm{H}[x] = -\int p(\mathbf{x})\ln p(\mathbf{x})\mathrm{d}\mathbf{x}</script><h3 id="Maximum-entropy"><a href="#Maximum-entropy" class="headerlink" title="Maximum entropy"></a>Maximum entropy</h3><p>离散分布中, 熵的最大值对应于均匀分布, 那连续分布呢? 为了让我们的讨论具有实际意义, 连续分布的概率密度函数应当满足一阶矩为均值, 二阶矩为方差, 归一化的三个约束条件</p>
<script type="math/tex; mode=display">
\begin{aligned}

\int_{-\infty}^{+\infty} p(x)  \mathrm{d}x &= 1\\

\int_{-\infty}^{+\infty} xp(x)\mathrm{d}x &= \mu\\

\int_{-\infty}^{+\infty} (x-\mu)^2 \mathrm{d}x &= \sigma^2

\end{aligned}</script><p>在以上三个约束条件下, 我们可以使用拉格朗日乘子法来使熵最大化, 过程略. <strong>使微分熵最大的分布是高斯分布</strong>, 高斯分布的均匀熵为</p>
<script type="math/tex; mode=display">
\mathrm{H}[x] = \frac{1}{2}\{1+\ln(2\pi\sigma^2)  \}</script><p>离散熵一定为正, 微分熵可能为负, 且微分熵的值随方差的增加而增大</p>
<h3 id="Kullback-Leibler-divergence-KL-divergence"><a href="#Kullback-Leibler-divergence-KL-divergence" class="headerlink" title="Kullback-Leibler divergence (KL divergence)"></a>Kullback-Leibler divergence (KL divergence)</h3><p>考虑一个未知分布 $p(x)$, 我们使用分布 $q(x)$ 来建模 $p(x)$. 由于我们没有使用真实分布 $p(x)$ 来表示信息, 所以 $q(x)$ 在表示信息时会用到更多的信息量, 这里多出来的信息量可以表示为</p>
<script type="math/tex; mode=display">
\begin{aligned}


\mathrm{KL}(p\|q) &= -\int p(\mathrm{x})\ln q(x)\mathrm{dx} - (-\int p(\mathrm{x})\ln p(\mathrm{x})\mathrm{dx})\\

&= \int p(\mathrm{x})\left\{\ln\frac{p(\mathrm{x})}{q(\mathrm{x})}  \right\}\mathrm{dx}


\end{aligned}</script><p>此即相对熵或 KL 散度的定义. KL 散度不是一个对称量, $\mathrm{KL}(p|q)\neq \mathrm{KL}(q|p)$.</p>
<blockquote>
<p>凸函数: 函数的每一条弦都位于函数上方或与函数重合, 要求二阶导数为正</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/image-20240809113336033.png" alt=""></p>
<blockquote>
<p>由图, 应当有 $f(\lambda a+(1-\lambda)b)\leq \lambda f(a) + (1-\lambda)f(b)$</p>
<p>归纳证明得到琴生不等式</p>
<script type="math/tex; mode=display">
f\left(\sum^M_{i=1}\lambda_ix_i   \right) \leq \sum^M_{i=1}\lambda_if(x_i)</script><p>这里 $\lambda_i\geq 0, \sum_i \lambda_i = 1$</p>
<p>对于离散变量, 将琴生不等式中的 $\lambda_i$ 理解成概率分布中的不同取值的概率, 得到</p>
<script type="math/tex; mode=display">
f(\mathbb{E}[x])\leq\mathbb{E}[f(x)]</script><p>对于连续变量, 琴生不等式可以表示为</p>
<script type="math/tex; mode=display">
f\left(\int \mathbf{x}p(\mathbf{x})\mathrm{d}\mathbf{x}\right)\leq \int f(\mathbf{x})p(\mathbf{x})\mathrm{d}\mathbf{x}</script><p>凸函数要求函数的二阶导数为正, 反之称为凹函数.</p>
</blockquote>
<p>利用琴生不等式可以证明 KL 散度的非负性</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathrm{KL}(p\| q) &= -\int p(\mathbf{x})\ln\frac{q(\mathbf{x})}{p(\mathbf{x})}\mathrm{d}\mathbf{x}\\

&\geq -\ln\frac{q(\int \mathbf{x}p(\mathbf{x})\mathrm{d}\mathbf{x})}{p(\int \mathbf{x}p(\mathbf{x})\mathrm{d}\mathbf{x})}


\end{aligned}</script><blockquote>
<p>来自 Deepseekv2</p>
<p>考虑 $f(x) = -\log(x)$</p>
<script type="math/tex; mode=display">
\begin{aligned}

f\left(\mathbb{E}_P\left[\frac{Q(x)}{P(x)} \right]\right) &\leq \mathbb{E}_P\left[f\left(\frac{Q(x)}{P(x)}  \right) \right]\\

\mathbb{E}_P\left[\frac{Q(x)}{P(x)} \right] &= \sum_x P(x)\frac{Q(x)}{P(x)} = 1\\

f(1) &= -\log(1) = 0\\

\end{aligned}</script><p>因此</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}_P\left[-\log\left(\frac{Q(x)}{P(x)}  \right)  \right] &\geq -\log\left(\mathbb{E}_P\left[\frac{Q(x)}{P(x)}  \right]  \right)\\

\mathbb{E}_P\left[-\log\left(\frac{Q(x)}{P(x)}  \right)  \right] &\geq 0\\

\sum_xP(x)\log\left(\frac{P(x)}{Q(x)} \right)&\geq 0



\end{aligned}</script><p>总结: 琴生不等式更像是一种技巧, 将 $\log$ 放进期望里, 从而求解熵有关的证明</p>
</blockquote>
<p>如果我们使用的分布与真实分布不同, 必然导致编码效率的降低, 需要的额外信息量等于我们使用的分布与真实分布间的 KL 散度</p>
<script type="math/tex; mode=display">
\mathrm{KL}(p\|q)\simeq \frac{1}{N}\sum^N_{n=1}\left\{-\ln q(\mathbf{x}_n|\theta)+\ln p(\mathbf{x_n})   \right\}</script><h3 id="Conditional-entropy"><a href="#Conditional-entropy" class="headerlink" title="Conditional entropy"></a>Conditional entropy</h3><p>考虑联合概率分布 $p(\mathbf{x}, \mathbf{y})$, 如果 $\mathbf{x}$ 已知, 那么指定相应 $\mathbf{y}$ 值所需的额外信息为 $\ln p(\mathbf{y|\mathbf{x}})$. 其平均值, 也叫做条件熵的定义如下</p>
<script type="math/tex; mode=display">
\mathrm{H}[\mathbf{y}|\mathbf{x}] = -\int \int p(\mathbf{y}, \mathbf{x})\ln p(\mathbf{y}|\mathbf{x})\mathrm{d}\mathbf{y}\mathrm{d}\mathbf{x}</script><p>此外可以得到,</p>
<script type="math/tex; mode=display">
\mathrm{H}[\mathbf{x}, \mathbf{y}] = \mathrm{H}[\mathbf{y}|\mathbf{x}] + \mathrm{H}[\mathbf{x}]\\


-\int\int p(\mathbf{x}, \mathbf{y})\ln p(\mathbf{x}, \mathbf{y})\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{y} = -\int \int p(\mathbf{y}, \mathbf{x})\ln p(\mathbf{y}|\mathbf{x})\mathrm{d}\mathbf{y}\mathrm{d}\mathbf{x} -\int p(\mathbf{x})\ln p(\mathbf{x})\mathrm{d}\mathbf{x}</script><p>上面 $\mathrm{H}[\mathbf{x}]$ 写成边缘熵的形式是 $\mathrm{H}[\mathbf{x}] = -\int\int p(\mathbf{x}, \mathbf{y})\ln p(\mathbf{x})\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{y}$, 其中 $p(\mathbf{x})$ 是边缘概率密度函数, $p(\mathbf{x}) = \int p(\mathbf{x}, \mathbf{y})\mathrm{d}\mathbf{y}$</p>
<h3 id="Mutual-information"><a href="#Mutual-information" class="headerlink" title="Mutual information"></a>Mutual information</h3><p>两个变量独立时, 他们的联合概率分布等于他们边缘分布的乘积 $p(\mathbf{x}, \mathbf{y}) = p(\mathbf{x})p(\mathbf{y})$. 如果两个变量不独立, 可以通过联合分布与边缘分布的概率密度函数乘积间的 KL 散度来判断他们是否接近独立. $x$ 和 $y$ 间的互信息定义为</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathrm{I}[\mathbf{x}, \mathbf{y}] \equiv \mathrm{KL}(p(\mathbf{x}, \mathbf{y})\| p(\mathbf{x})p(\mathbf{y}) )

&= -\int\int p(\mathbf{x}, \mathbf{y})\ln \left(\frac{p(\mathbf{x})p(\mathbf{y})}{p(\mathbf{x}, \mathbf{y})}   \right)\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{y}


\end{aligned}</script><p>互信息和条件熵有关</p>
<script type="math/tex; mode=display">
\mathrm{I}[\mathbf{x}, \mathbf{y}] = \mathrm{H}[\mathbf{x}] - \mathrm{H}[\mathbf{x}|\mathbf{y}] = \mathrm{H}[\mathbf{y}] - \mathrm{H}[\mathbf{y}|\mathbf{x}]</script><p>互信息的意义是在得知 $\mathbf{y}$ 的值后, $\mathbf{x}$ 的不确定性减少. 可以从贝叶斯公式的角度理解, $p(\mathbf{x})$ 是先验分布, $p(\mathbf{x}|\mathbf{y})$ 是后验分布, 互信息是由于后验的存在导致的先验的不确定性的减少程度</p>
<h2 id="Bayesian-Probabilities"><a href="#Bayesian-Probabilities" class="headerlink" title="Bayesian Probabilities"></a>Bayesian Probabilities</h2><p>使用概率来表示不确定性是很好的选择. 如果使用数值, 那么常识性的公理都最终将等价于概率的加法和惩罚规则</p>
<h3 id="Model-parameters"><a href="#Model-parameters" class="headerlink" title="Model parameters"></a>Model parameters</h3><p>我们用 $\mathcal{D}$ 表示训练数据集, $\mathbf{w}$ 是使似然函数 $p(\mathcal{D}|\mathbf{w})$ 最大化的值. 似然函数的负对数被称为误差函数, 因为负对数是一个单调递减函数, 最大化似然函数等同于最小化误差, 这样得到 $\mathbf{w}_{\mathrm{ML}}$.</p>
<p>假设参数 $\mathbf{w}$ 满足先验概率 $p(\mathbf{w})$, 观测数据 $\mathcal{D}$ 的影响通过 $p(\mathcal{D}|\mathbf{w})$ 进行表达. 贝叶斯定理现在是</p>
<script type="math/tex; mode=display">
p(\mathbf{w}|\mathcal{D}) = \frac{p(\mathcal{D}|\mathbf{w})p(\mathbf{w})}{p(\mathcal{D})}</script><p>贝叶斯定理能让我们通过后验概率评估 $\mathbf{w}$ 的不确定性. 当 $p(\mathcal{D}|\mathbf{w})$ 看做关于 $\mathbf{w}$ 的函数时, 这也可以看做似然函数.</p>
<blockquote>
<p>看不懂 :D</p>
</blockquote>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>最大后验估计 (MAP 估计)</p>
<h3 id="Bayesian-machine-learning"><a href="#Bayesian-machine-learning" class="headerlink" title="Bayesian machine learning"></a>Bayesian machine learning</h3><p>贝叶斯的视角让我们看到正则化的作用, 并推导出正则化项的具体形式. 但贝叶斯定理寻找 $\mathbf{w}$ 的单一解, 没有考虑到 $\mathbf{w}$ 值的不确定性. 假设我们有训练数据集 $\mathcal{D}$, 我们想在给定 $\mathrm{x}$ 的情况下预测某个目标变量 $t$. 显然有</p>
<script type="math/tex; mode=display">
p(t|x, \mathcal{D}) = \int p(t|x, \mathbf{w})p(\mathbf{w}|\mathcal{D})\mathrm{d}\mathbf{w}</script><p>这里 $p(\mathbf{w}|\mathcal{D})$ 可以看做是加权参数. 这种方法与贝叶斯方法的关键差异是对参数空间的积分.</p>
<p>完全贝叶斯式的机器学习方法更具有可解释性. 例如多项式回归背景下的过拟合问题, 就是最大似然法的使用中出现的问题. 如果我们用贝叶斯方法对参数进行边缘化处理, 这一问题就不会出现. 比如回归问题中不同阶数的多项式回归, 最大似然法倾向于选择对训练数据拟合更好的模型, 但是这导致模型复杂度更好, 从而会产生过拟合. 完全贝叶斯的处理方法则是对所有可能的模型进行平均, 每个模型的贡献按其后验概率加权, 这使得复杂度适中的模型更容易被选中, 这相当于优雅的惩罚了复杂性.</p>
<p>但贝叶斯的方法也存在计算量巨大的问题. 所以可以考虑对神经网络使用最大似然技术, 并使用正则化技术来避免过拟合</p>
<h1 id="Standard-Distribution"><a href="#Standard-Distribution" class="headerlink" title="Standard Distribution"></a>Standard Distribution</h1><p>本章讨论一些特殊的概率分布及其性质. 这些分布可以在给定有限观测集 $\mathbf{x}_1, \dots, \mathbf{x}_N$ 时, 对随机变量 $\mathbf{x}$ 的概率分布 $p(\mathbf{x})$ 进行建模. 这一问题也称作<strong>密度估计</strong>. 这是一个不适定问题, 因为符合有限数据集的概率分布有无数种, 我们只需要找一个很合适的就可以.</p>
<p>本节我们从离散变量的分布讨论到连续变量的正态分布. 这些都是参数分布, 这种分布的可调参数较少, 主要通过最大化似然函数的方式进行参数估计. 本节假设观测是独立同分布的 $\mathrm{i.i.d.}$</p>
<p>这种基于参数的密度估计的方法有一定局限, 它假设了分布具有特定的函数形式. 非参数密度估计方法的分布形式则取决于数据集大小, 这些参数控制的是模型复杂度而非分布的形式. 本章介绍了基于直方图, 最近邻和核函数的三种非参数方法. 这种方法的局限在于要存储所有训练数据, 数据量大时方法很低效. 基于此发展出了神经网络</p>
<h2 id="Discrete-Variables"><a href="#Discrete-Variables" class="headerlink" title="Discrete Variables"></a>Discrete Variables</h2><h3 id="Bernoulli-distribution"><a href="#Bernoulli-distribution" class="headerlink" title="Bernoulli distribution"></a>Bernoulli distribution</h3><p>伯努利分布: 对于二元随机变量 $x\in {0,1}$, 其概率密度函数为</p>
<script type="math/tex; mode=display">
\mathrm{Bern}(x|\mu) = \mu ^x(1-\mu)^{1-x}</script><p>其性质有</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[x] &= \mu\\
\mathrm{var}[x] &= \mu (1-\mu)

\end{aligned}</script><p>最大化似然函数有</p>
<script type="math/tex; mode=display">
\mu_{\mathrm{ML}} = \frac{1}{N}\sum_{i=1}^Nx_n = \frac{m}{N}</script><p>这里 $m$ 为这一组数据集 $\mathcal{D} = {x_1, \dots, x_N}$ 中 $x=1$ 出现的次数</p>
<h3 id="Binomial-distribution"><a href="#Binomial-distribution" class="headerlink" title="Binomial distribution"></a>Binomial distribution</h3><p>二项分布: 对于二元随机变量 $x$, 在数据集大小为 $N$ 时, 观察到 $x=1$ 的次数的分布. 二项分布也可以看做在 $N$ 次伯努利实验中成功 $m$ 次的概率. 概率密度函数</p>
<script type="math/tex; mode=display">
\mathrm{Bin}(m|N, \mu) = \begin{pmatrix}
N\\m
\end{pmatrix}
\mu^m (1-\mu)^{N-m}\\
\begin{pmatrix}
N\\m
\end{pmatrix} = C_N^m =  \frac{N!}{(N-m)!m!}</script><blockquote>
<p>$A_N^m$ 是考虑顺序的排列数</p>
<script type="math/tex; mode=display">
A_N^m = \frac{N!}{(N-m)!}</script><p>$C_N^m$ 是不考虑顺序的排列数</p>
<script type="math/tex; mode=display">
C_N^m = \frac{N!}{(N-m)!m!}</script></blockquote>
<p>性质: (独立事件)</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[m] &= \sum_{m=0}^N m\mathrm{Bin}(m|N, \mu) = N\mu\\

\mathrm{var}[m] &= \sum_{m=0}^N (m-\mathbb{E}[m])^2\mathrm{Bin(m|N, \mu)} = N\mu (1-\mu)

\end{aligned}</script><h3 id="Multinomial-distribution"><a href="#Multinomial-distribution" class="headerlink" title="Multinomial distribution"></a>Multinomial distribution</h3><p>二元变量仅有两种可能值,  当变量可以表示多种可能值时, 有</p>
<script type="math/tex; mode=display">
p(\mathbf{x}|\mathbf{\mu}) = \prod_{k=1}^K \mu_k^{x_k}</script><p>这里 $\mu_k\geq 0$ 且 $\sum_k \mu_k = 1$, $x_k = {0, 1}$ 且仅有一个 $x$ 可以为1. 这一写法也可以看做伯努利分布的推广.</p>
<p>性质</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[\mathbf{x}|\mu] = \sum_\mathbf{x}p(\mathbf{x}|\mu)\mathbf{x} = \mu

\end{aligned}</script><p>假设一个包含 $N$ 个独立观测值 $\mathbf{x}_1, \dots, \mathbf{x}_N$ 的数据集 $\mathcal{D}$, 其似然函数为</p>
<script type="math/tex; mode=display">
p(\mathcal{D}|\mu) = \prod_{n=1}^N \prod_{k=1}^K \mu_k^{x_{nk}} = \prod_{k=1}^K \mu_k^{(\sum_n x_{nk})} = \prod_{k=1}^K \mu_k^{m_k}</script><p>显然似然函数仅通过 $K$ 个量依赖于 $N$ 个数据点. 这里变量 $m<em>k = \sum</em>{n=1}^N x<em>{nk}, \sum</em>{i=1}^K m_k = N$</p>
<blockquote>
<p><strong>这里</strong> <strong>K</strong> <strong>代表一共有</strong> <strong>K</strong> <strong>种可能的取值 (离散型随机变量的概率密度函数的似然函数中, 连乘的项的数量等于随机变量可以取值的数量),</strong> <strong>N</strong> <strong>代表一共进行了</strong> <strong>N</strong> <strong>次实验, 也即实际进行进行的实验的数量</strong></p>
</blockquote>
<p>拉格朗日乘子法最大化对数似然函数后, 得到</p>
<script type="math/tex; mode=display">
\mu_k^{\mathrm{ML}} = \frac{m_k}{N}</script><p>这表示 $N$ 个观测值中 $x_k=1$ 的比例 (这里 $\mu_k$ 是概率密度函数中的底数, 有 $k$ 种取值, 一一对应有 $\mu_k$, 这里最大似然估计是对底数估计)</p>
<p><strong>多项分布:</strong> 对于多个变量 $m_1, \dots, m_K$ 的联合分布, 考虑参数向量 $\bf{\mu}$ 和 $N$ 个观测值, 其似然函数也可以写成以下形式</p>
<script type="math/tex; mode=display">
\mathrm{Mult(m_1, m_2, \dots, m_K|\mu, N)} = \begin{pmatrix} N\\m_1m_2\dots m_K\end{pmatrix}\prod_{k=1}^K\mu_k^{m_k}</script><p>这里</p>
<script type="math/tex; mode=display">
\begin{pmatrix} N\\m_1m_2\dots m_K\end{pmatrix} = \frac{N!}{m_1!m_2!\dots m_K!}</script><blockquote>
<p>多项分布的概率密度函数可以更简单的理解: </p>
<p>每种结果出现的概率为 $\mu<em>k$, 第 $K$ 个结果出现过 $m_K$ 次的概率为 $\mu_k^{m<br>_K}$, 连乘有 $\prod</em>{k=1}^K\mu_k^{m<br>_k}$</p>
<p>在 $N$ 次试验中, 第 $K$ 种结果出现 $m_K$ 次, 这些次数排列组合为多项式系数</p>
<script type="math/tex; mode=display">
\begin{pmatrix} N\\m_1m_2\dots m_K\end{pmatrix} = \frac{N!}{m_1!m_2!\dots m_K!}</script><p>两个式子相乘就是多项分布的概率密度函数, 这样看更直观</p>
</blockquote>
<h2 id="The-Multivariate-Gaussian"><a href="#The-Multivariate-Gaussian" class="headerlink" title="The Multivariate Gaussian"></a>The Multivariate Gaussian</h2><p>单变量高斯分布</p>
<script type="math/tex; mode=display">
\mathcal{N}(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp \left\{-\frac{(x-\mu)^2}{2\sigma^2}    \right\}</script><p>在多元变量情况下, 我们考虑一个 $D$ 维的向量 $\mathbf{x}$, 多元高斯分布可以表示为</p>
<script type="math/tex; mode=display">
\mathcal{N}(\mathbf{x}|\mu, \Sigma) = \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^{\frac{1}{2}}}\exp \left\{-\frac{1}{2}(\mathbf{x}-\mu)^{\mathrm{T}}\Sigma^{-1}(\mathbf{x}-\mu)   \right\}</script><p>这里 $\mathbf{x}$ 是一个 $N$ 维随机变量, $\mu$ 是 $D$ 维均值向量, $\Sigma$ 是 $D\times D$ 协方差矩阵, $\det \Sigma$ 表示协方差矩阵的行列式</p>
<ol>
<li>熵最大时的分布是高斯分布</li>
<li>随机变量数量足够多时, 多个随机变量的和趋近于高斯分布</li>
</ol>
<p>我们在探讨以下问题时, 要关注联合分布的指数中的二次型, 由此确定具体分布的均值和协方差</p>
<h3 id="Geometry-of-the-Gaussian"><a href="#Geometry-of-the-Gaussian" class="headerlink" title="Geometry of the Gaussian"></a>Geometry of the Gaussian</h3><p><strong>考虑高斯分布有关项, 将高斯分布正交化变换到新坐标系中</strong></p>
<p>高斯分布有关 $\mathbf{x}$ 的项如下</p>
<script type="math/tex; mode=display">
\Delta^2 = (\mathbf{x}-\mu)^{\mathrm{T}}\Sigma^{-1}(\mathbf{x}-\mu)</script><p>$\Delta$ 称为 $\mu$ 到 $\mathbf{x}$ 的马氏距离 (Mahalanobis distance), 当 $\Sigma$ 是单位矩阵的时候, $\Delta$ 是欧氏距离. 高斯分布在 $\mathbf{x}$ 空间中使得此二次型为常数</p>
<blockquote>
<p>矩阵的二次型: 定义一个对称矩阵 $\mathbf{A}$, 元素为 $a_{ij}$, $\mathbf{x}$ 为列向量. 二次型可以写成</p>
<script type="math/tex; mode=display">
Q(\mathbf{x}) = \mathbf{x}^T \mathbf{A} \mathbf{x}</script><p>要求对称矩阵, 是因为对任意 $Q(\mathbf{x})=\mathbf{x}^T\mathbf{Ax}$, 无论 $\mathbf{A}$ 是否为对称矩阵, 总可以将其唯一地改写为矩阵的形式, 证明如下:</p>
<p>若 $\mathbf{A}$ 不对称, 可以构造对称矩阵 $\mathbf{B} = \frac{1}{2}(\mathbf{A}+\mathbf{A}^T)$, 反对称矩阵 $\mathbf{C} = \frac{1}{2}(\mathbf{A}-\mathbf{A}^T)$, 使得 $\mathbf{x}^T\mathbf{Ax} = \mathbf{x}^T(\mathbf{B}+\mathbf{C})\mathbf{x}$. 反对称矩阵有 $\mathbf{C}^T = -\mathbf{C}$, 其对角线元素均为 0; 而且 $\mathbf{x}$ 为 1×n 向量, $\mathbf{C}$ 为 n×n 矩阵, 所以二次型计算结果为一个标量值, 其转置等于自身, 则有 $(\mathbf{x}^T\mathbf{C}\mathbf{x})^T = \mathbf{x}^T\mathbf{C}\mathbf{x}$; 计算转置, 有 $(\mathbf{x}^T\mathbf{C}\mathbf{x})^T = \mathbf{x}^T\mathbf{C}^T\mathbf{x} = -\mathbf{x}^T\mathbf{C}\mathbf{x}$, 因此 $\mathbf{x}^T\mathbf{C}\mathbf{x})^T = \mathbf{x}^T\mathbf{C}\mathbf{x} = -\mathbf{x}^T\mathbf{C}\mathbf{x} = 0$</p>
</blockquote>
<p>这里协方差矩阵 $\Sigma$ 为实对称矩阵, 其特征值是实数, 特征向量可以构成一个归一化的正交集合 (施密特正交化). 再考虑协方差矩阵的特征向量方程有</p>
<blockquote>
<p>施密特正交化: 线代里的知识, 将一组线性无关的向量转换为一组正交向量</p>
<ol>
<li>初始化: 一个无关向量集合 $\left{\mathbf{v}_1, \mathbf{v}_2,\dots, \mathbf{v}_n \right}$</li>
<li>第一个正交向量: 第一个向量 $\mathbf{v}_1$ 作为第一个正交向量 $\mathbf{u}_1 = \mathbf{v}_1$</li>
<li>第二个正交向量: $\mathbf{u}_2$ 为 $\mathbf{v}_2$ 减去其在 $\mathbf{u}_1$ 上的投影</li>
</ol>
<script type="math/tex; mode=display">
\mathbf{u}_2 = \mathbf{v}_2 - \frac{\mathbf{v}_2\cdot \mathbf{u}_1}{\mathbf{u}_1\cdot \mathbf{u}_1}\mathbf{u}_1</script><ol>
<li>其他正交向量: 对第 $k$ 个正交向量 $\mathbf{u}_k$,</li>
</ol>
<script type="math/tex; mode=display">
  \mathbf{u}_k = \mathbf{v}_k - \sum_{i=1}^{k-1}\frac{\mathbf{v}_k\cdot \mathbf{u}_i}{\mathbf{u}_i\cdot \mathbf{u}_i}\mathbf{u}_i</script><ol>
<li>正交向量归一化为单位向量</li>
</ol>
<script type="math/tex; mode=display">
\mathbf{e}_i = \frac{\mathbf{u}_i}{\|\mathbf{u}_i\|}</script></blockquote>
<script type="math/tex; mode=display">
\Sigma\mathbf{u}_i = \lambda_i\mathbf{u}_i\qquad i=1,\dots,D\\

\mathbf{u}^{\mathrm{T}}_i\mathbf{u}_j = I_{ij}\\

I_{ij} = \begin{cases} 1,\quad \text{if } i=j\\ 0, \quad\text{otherwise}\end{cases}</script><blockquote>
<p>协方差矩阵的特征向量方程:</p>
<script type="math/tex; mode=display">
\mathbf{u}^{\mathrm{T}}_i\mathbf{u}_j = I_{ij}</script><p>方程表示协方差矩阵 $\Sigma$ 作用在特征向量 $\mathbf{u}$ 上时, 与特征向量与特征值的乘积结果相同.</p>
<p><strong>特征向量</strong> $\mathbf{u}$ <strong>在协方差矩阵</strong> $\Sigma$ <strong>作用下仅改变了长度, 长度变化由</strong> $\lambda$ <strong>决定, 方向保持不变</strong></p>
</blockquote>
<p>因此, 协方差矩阵可以以特征向量与特征值的形式展开</p>
<script type="math/tex; mode=display">
\Sigma = \sum_{i=1}^D \lambda_i\mathbf{u}_i\mathbf{u}_i^{\mathrm{T}}\\

\Sigma^{-1} = \sum_{i=1}^D \frac{1}{\lambda_i}\mathbf{u}_i\mathbf{u}_i^{\mathrm{T}}\\</script><p>因此, 高斯分布有关 $\mathbf{x}$ 的项可以变换为</p>
<script type="math/tex; mode=display">
\Delta^2 = \sum_{i=1}^D\frac{y_i^2}{\lambda_i}\\

\text{here } y_i = \mathbf{u}_i^{\mathrm{T}}(\mathbf{x}-\mu)</script><p>这里 $y_i$ 可以看做正交归一化向量 $\mathbf{u}_i$ 定义的新坐标系 (表达空间平移和旋转), 从而得到 $\mathbf{y} = (y_1, \dots, y_D)^{\mathrm{T}}$</p>
<script type="math/tex; mode=display">
\mathbf{y} = \mathbf{U(x)}-\mathbf{\mu}</script><p>这里 $\mathbf{U}$ 是一个矩阵, 每一行为 $\mathbf{u}_i^{\mathrm{T}}$, 这一矩阵的性质由 $\mathbf{u}_i^{\mathrm{T}}$ 可以推导出 $\mathbf{UU}^{\mathrm{T}} = \mathbf{U}^{\mathrm{T}}\mathbf{U} = \mathbf{I}$. 需要注意, 想要定义一个多维高斯分布, 必须保证协方差矩阵的所有特征值 $i$ 均为正, 否则无法归一化.</p>
<blockquote>
<p>正定矩阵: 特征值严格为正的矩阵</p>
</blockquote>
<p>从 $\mathbf{x}$ 坐标系变换到 $\mathbf{y}$ 坐标系上时, 雅可比矩阵 $\mathbf{J}$ 的元素定义为</p>
<script type="math/tex; mode=display">
J_{ij} = \frac{\partial x_i}{\partial y_j} = U_{ji}</script><p>这里 $U_{ji}$ 是矩阵 $\mathbf{U}^T$ 的元素. 利用 $\mathbf{U}$ 矩阵的正交归一化, 雅可比矩阵的行列式平方为</p>
<script type="math/tex; mode=display">
|\mathbf{J}| = |\mathbf{U}^{\rm{T}}|^2 = |\mathbf{U}^{\rm{T}}||\mathbf{U}| = |\mathbf{U}^{\rm{T}}\mathbf{U}| = |\mathbf{I}| = 1</script><p>同理, $\left|\Sigma\right|$ 能写成特征值的连乘形式, 因此</p>
<script type="math/tex; mode=display">
\left|\Sigma\right|^{\frac{1}{2}} = \prod_{j=1}^D \lambda_i^{\frac{1}{2}}</script><p>因此, $y_j$ 坐标系中, 高斯分布形式为</p>
<script type="math/tex; mode=display">
p(\mathbf{y}) = p(\mathbf{x})\left|\mathbf{J} \right| = \prod_{j=1}^D\frac{1}{(2\pi\lambda_j)^{\frac{1}{2}}}\exp\left\{-\frac{y_j^2}{2\lambda_j}  \right\}</script><h3 id="Moments"><a href="#Moments" class="headerlink" title="Moments"></a>Moments</h3><p><strong>一阶矩</strong></p>
<p>Moments, 矩. 一阶矩也即期望</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[\mathbf{x}] &= \frac{1}{(2\pi)^{\frac{D}{2}}} \frac{1}{(\Sigma)^{\frac{1}{2}}} \int \exp \left\{-\frac{1}{2} (\mathbf{x}-\mu)^{\rm{T}}\Sigma^{-1}(\mathbf{x}-\mu)\mathbf{x}   \right\}\mathrm{d}\mathbf{x}\\

&= \frac{1}{(2\pi)^{\frac{D}{2}}} \frac{1}{(\Sigma)^{\frac{1}{2}}} \int \exp  \left\{-\frac{1}{2}\mathbf{z}^{\rm{T}}\Sigma^{-1}\mathbf{z}    \right\}(\mathbf{z}+\mu)\mathrm{d}\mathbf{z}

\end{aligned}</script><p>这里我们使用变量替换 $\mathbf{z} = \mathbf{x}-\mu$, 考虑到指数是偶函数, 我们有</p>
<script type="math/tex; mode=display">
\mathbb{E}[\mathbf{x}] = \mu</script><p><strong>二阶矩</strong></p>
<p>下面我们考虑高斯分布的二阶矩, 单变量高斯分布的二阶矩有</p>
<script type="math/tex; mode=display">
\mathbb{E}[\mathbf{x}^2] = \mu^2 + \sigma^2</script><p>对于多元高斯分布, $\mathbb{E}[x_ix_j]$ 决定了共有 $D^2$ 个二阶矩的值. 所以多元高斯变量的二阶矩矩阵形状为 $D\times D$. 与一阶矩的计算相似, 二阶矩可以写成</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \mathbb{E}[\mathbf{x}\mathbf{x}^{\rm{T}}] &= \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{\left|\Sigma \right|^{\frac{1}{2}}}\int \exp \left\{-\frac{1}{2}(\mathbf{x}-\mu)^{\rm{T}}\Sigma^{-1}(\mathbf{x}-\mu)\mathbf{x}\mathbf{x}^{\rm{T}}   \right\}\mathrm{d}\mathbf{x}\\

    &= \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{\left|\Sigma \right|^{\frac{1}{2}}}\int \exp\left\{-\frac{1}{2}\mathbf{z}^{\rm{T}}\Sigma^{-1}\mathbf{z}   \right\} (\mathbf{z}+\mathbf{\mu})(\mathbf{z}+\mathbf{\mu})^{\rm{T}}\mathrm{d}\mathbf{z}

\end{aligned}</script><p>这里 $\mathbf{z} = \mathbf{x}-\mu$, 这里积分内的交叉项 $\mu\mathbf{z}^{\rm{T}}$ 和 $\mu^{\rm{T}}\mathbf{z}$ 由于对称性消失, $\mu\mu^{\rm{T}}$ 是常数项, $\mathbf{z}\mathbf{z}^{\rm{T}}$ 项可以利用协方差矩阵的特征向量展开 $\Sigma\mathbf{u}_i = \lambda_i\mathbf{u}_i$ 展开, 得到</p>
<script type="math/tex; mode=display">
\mathbf{z} = \sum_{j=1}^D \mathbf{u}_j^{\rm{T}}\mathbf{z}\mathbf{u}_j</script><script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[\mathbf{x}\mathbf{x}^{\rm{T}}] &= \mu\mu^{\rm{T}} + \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{\left|\Sigma \right|^{\frac{1}{2}}}\int \exp \left\{-\frac{1}{2}\mathbf{z}^{\rm{T}}\Sigma^{-1}\mathbf{z}   \right\}\mathbf{xx}^{\mathrm{T}}\mathrm{d}\mathbf{z}\\

&= \mu\mu^{\rm{T}} + \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{\left|\Sigma \right|^{\frac{1}{2}}} \sum_{i=1}^D\sum_{j=1}^D \mathbf{u}_i\mathbf{u}_j^{\rm{T}}\int \exp \left\{-\sum_{k=1}^D\frac{y_k^2}{2\lambda_k} \right\}y_iy_j\mathrm{d}\mathbf{y}\\

&= \sum_{i=1}^D\mathbf{u}_i\mathbf{u}_i^{\rm{T}}\lambda_i\\

&= \Sigma

\end{aligned}</script><p>因此, 多元高斯分布的二阶矩为</p>
<script type="math/tex; mode=display">
\mathbb{E}[\mathbf{x}\mathbf{x}^{\rm{T}}] = \mu\mu^{\rm{T}} + \Sigma</script><p>由一阶矩和二阶矩, 我们可以计算多元高斯分布的协方差矩阵</p>
<script type="math/tex; mode=display">
\rm{cov}[\mathbf{x}] = \mathbb{E}[\left(\mathbf{x}-\mathbb{E}[\mathbf{x}]\right)\left(\mathbf{x}-\mathbb{E}[\mathbf{x}]\right)^{\rm{T}}] = \mathbb{E}[\mathbf{x}\mathbf{x}^{\rm{T}}] - \mathbb{E}[\mathbf{x}]\mathbb{E}[\mathbf{x}]^{\rm{T}} = \Sigma</script><h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><p>这里我们主要讨论参数量 (自由度) 与计算时间的限制关系.</p>
<p><strong>参数量与计算复杂度</strong></p>
<p>确定一个多元高斯正态分布, 所需的参数量是 $\Sigma$ 和 $\mu$ 总共的参数量, 共为 $\frac{D(D+1)}{2} + D = \frac{D(D+3)}{2}$, $D$ 很大时总参数量成平方增长, 此时大规模矩阵的计算会很耗时. 能不能将计算时间改为线性增长呢?</p>
<p>参数量主要来自于协方差矩阵 $\Sigma$, 为了减少参数量, 我们可以假设协方差矩阵是对角矩阵 $\Sigma = \rm{diag}(\sigma_i^2)$, 这样参数量就减少为 $D+D=2D$ 个. 这种假设称为<strong>独立同分布假设</strong>. 这种假设的缺点是, 不能捕捉到变量间的相关性.</p>
<p>我们进一步约束协方差矩阵为对角矩阵, 但是每个维度的方差都相等 $\Sigma = \sigma^2\mathbf{I}$, 这样参数量就减少为 $D+1$ 个. 这种假设称为<strong>各向同性假设</strong>. 这种假设的缺点是, 不能捕捉到不同维度的方差不同的情况.</p>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/20240906103653.png" alt=""></p>
<blockquote>
<p>可以用一个二维图来理解多元高斯分布的无限制假设, 独立同分布假设和各向同性假设的关系. 假设这里圆心都固定在原点</p>
<ol>
<li>椭圆的长轴和短轴位置和长短都不确定</li>
<li>椭圆的一个轴的位置被定住了, 所以表达空间上相较 a 受限</li>
<li>椭圆两个轴的位置都被定住了, 变成了同心圆, 表达空间进一步被限制</li>
</ol>
</blockquote>
<p><strong>单峰限制</strong></p>
<p>高斯分布难以近似多峰分布, 所以表示比较有限.</p>
<p>可通过引入潜在变量 hidden variables/unobserved variables 解决以上问题</p>
<h3 id="Conditional-distribution"><a href="#Conditional-distribution" class="headerlink" title="Conditional distribution"></a>Conditional distribution</h3><p>如果两个随机变量符合联合高斯正态分布, 那么这两个随机变量间的条件分布是高斯分布, 任一组变量的变元分布也是高斯分布.</p>
<p>多元高斯条件分布函数的均值和方差</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mu_{a|b} &= \mu_a + \Sigma_{ab}\Sigma_{bb}^{-1}(\mathbf{x}_b-\mu_b)\\

\Sigma_{a|b} &= \Sigma_{aa} - \Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}

\end{aligned}</script><blockquote>
<p>推导如下</p>
<p>假设 $\mathbf{x}$ 符合高斯分布 $\mathcal{N}(\mathbf{x}|\mu, \Sigma)$, 变量 $\mathbf{x}$ 分为两个子集 $\mathbf{x}_a$ 和 $\mathbf{x}_b$, 分别包括前 $M$ 个后 $D-M$ 个变量, 即</p>
<script type="math/tex; mode=display">
\mathbf{x} = \begin{pmatrix} \mathbf{x}_a\\ \mathbf{x}_b\end{pmatrix}</script><p>同理, 均值向量和协方差矩阵也可以分为两部分</p>
<script type="math/tex; mode=display">
\mu = \begin{pmatrix} \mu_a\\ \mu_b\end{pmatrix}\\

\Sigma = \begin{pmatrix} \Sigma_{aa} & \Sigma_{ab}\\ \Sigma_{ba} & \Sigma_{bb}\end{pmatrix}</script><p>为了方便后续计算, 我们定义 precision matrix 精度矩阵如下</p>
<blockquote>
<p>精度矩阵: 协方差矩阵的逆矩阵, 为了方便表示</p>
</blockquote>
<script type="math/tex; mode=display">
\Lambda \equiv \Sigma^{-1}</script><p>同理有精度矩阵的分块矩阵</p>
<script type="math/tex; mode=display">
\Lambda = \begin{pmatrix} \Lambda_{aa} & \Lambda_{ab}\\ \Lambda_{ba} & \Lambda_{bb}\end{pmatrix}</script><blockquote>
<p>对称矩阵的逆也是对称的, 所以 $\Lambda<em>{ab} = \Lambda</em>{ba}^{\rm{T}}$</p>
</blockquote>
<p>首先找到条件分布 $p(\mathbf{x}_a|\mathbf{x}_b)$ 的表达式</p>
<blockquote>
<p><strong>想要找条件分布</strong> $p(\mathbf{x}_a|\mathbf{x}_b)$ <strong>的表达式, 可以固定联合分布</strong> $p(\mathbf{x}_a. \mathbf{x}_b)$, 然后归一化 $\mathbf{x}_b$. 显然这样的操作比较困难</p>
</blockquote>
<p>我们依然从二次型出发,</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \ -\frac{1}{2}(\mathbf{x}-\mu)^{\rm{T}}\Sigma^{-1}(\mathbf{x}-\mu) &= -\frac{1}{2}(\mathbf{x}_a-\mu_a)^{\rm{T}}\Lambda_{aa}(\mathbf{x}_a - \mu_a) - \frac{1}{2}(\mathbf{x}_b-\mu_b)^{\rm{T}}\Lambda_{ba}(\mathbf{x}_a-\mu_a) - \frac{1}{2}(\mathbf{x}_a-\mu_a)^{\rm{T}}\Lambda_{ab}(\mathbf{x}_b-\mu_b) - \frac{1}{2}(\mathbf{x}_b-\mu_b)^{\rm{T}}\Lambda_{bb}(\mathbf{x}_b-\mu_b)\\
\end{aligned}</script><p>我们要找的是条件分布 $p(\mathbf{x}_a|\mathbf{x}_b)$ 的表达式哦, 所以上面这个式子可以看做关于 $\mathbf{x}_a$ 的函数</p>
<blockquote>
<p>中间两项可以相加, 看做一次项, 第四项可以看做常数项, 所以这是一个 “二次型+一次项” 的结构</p>
</blockquote>
<p>接下来验证我们得到的条件分布公式是否正确, 我们可以计算条件分布的均值和协方差矩阵. 在多元高斯正态分布中, 注意到高斯分布指数项展开有</p>
<script type="math/tex; mode=display">
-\frac{1}{2}(\mathbf{x}-\mu)^{\rm{T}}\Sigma^{-1}(\mathbf{x}-\mu) = -\frac{1}{2}\mathbf{x}^{\rm{T}}\Sigma^{-1}\mathbf{x} + \mathbf{x}^{\rm{T}}\Sigma^{-1}\mu + \rm{const}</script><p>在上面的形式中, 二次项参数矩阵为 $\Sigma^{-1}$, 一次项参数矩阵为 $\Sigma^{-1}\mu$, 由此可以得到 $\mu$. 同理, 条件分布中也可以得到二次项的参数矩阵 $\Sigma<em>{a|b} = \Lambda</em>{aa}^{-1}$, 一次项的参数矩阵满足条件 $\mathbf{x}<em>a^{\rm{T}}\left{\Lambda</em>{aa}\mu<em>a-\Lambda</em>{ab}(x<em>b-\mu_b) \right}$, 因此 $\mu</em>{a|b} = \mu<em>a - \Lambda</em>{aa}^{-1}\Lambda_{ab}(\mathbf{x}_b-\mu_b)$</p>
<p>以上的推导中, 条件分布的均值矩阵和协方差矩阵是通过原始联合分布的精度矩阵的的分块矩阵表示的, 我们也可以用相应的原始分块协方差矩阵来表示.</p>
<blockquote>
<p>分块矩阵的逆的恒等式</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
\mathbf{A}& \mathbf{B}\\
\mathbf{C}& \mathbf{D}
\end{pmatrix}^{-1} = 

\begin{pmatrix}

\mathbf{M} & -\mathbf{MBD}^{-1}\\
-\mathbf{D}^{-1}\mathbf{CM} & \mathbf{D}^{-1}+\mathbf{D}^{-1}\mathbf{CMBD}^{-1}

\end{pmatrix}</script><p>这里我们定义 $\mathbf{M} = (\mathbf{A}-\mathbf{BD}^{-1}\mathbf{C})^{-1}$, $\mathbf{M}^{-1}$ 被称为上式左侧关于子矩阵 $\mathbf{D}$ 的 Schur 补</p>
</blockquote>
<p>我们利用协方差矩阵与精度矩阵的关系, 与上述分块矩阵的逆的恒等式, 得到精度矩阵的分块矩阵元素</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbf{\Lambda}_{aa} &= (\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1}\\

\mathbf{\Lambda}_{ab} &= -(\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1}\Sigma_{ab}\Sigma_{bb}^{-1}

\end{aligned}</script><p>将以上结果代入 $\mu<em>{a|b}$ 和 $\Sigma</em>{a|b}$, 得到</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mu_{a|b} &= \mu_a + \Sigma_{ab}\Sigma_{bb}^{-1}(\mathbf{x}_b-\mu_b)\\

\Sigma_{a|b} &= \Sigma_{aa} - \Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}

\end{aligned}</script><p>对比可以发现, 使用分块精度矩阵比分块协方差矩阵表示的形式更简单</p>
</blockquote>
<h3 id="Marginal-distribution"><a href="#Marginal-distribution" class="headerlink" title="Marginal distribution"></a>Marginal distribution</h3><p>边缘分布定义为</p>
<script type="math/tex; mode=display">
p(\mathbf{x}_a) = \int p(\mathbf{x}_a, \mathbf{x}_b)\mathrm{d}\mathbf{x}_b</script><p>考虑上一节中将二次型转换为分块精度矩阵表示</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \ -\frac{1}{2}(\mathbf{x}-\mu)^{\rm{T}}\Sigma^{-1}(\mathbf{x}-\mu) &= -\frac{1}{2}(\mathbf{x}_a-\mu_a)^{\rm{T}}\Lambda_{aa}(\mathbf{x}_a - \mu_a) - \frac{1}{2}(\mathbf{x}_b-\mu_b)^{\rm{T}}\Lambda_{ba}(\mathbf{x}_a-\mu_a) - \frac{1}{2}(\mathbf{x}_a-\mu_a)^{\rm{T}}\Lambda_{ab}(\mathbf{x}_b-\mu_b) - \frac{1}{2}(\mathbf{x}_b-\mu_b)^{\rm{T}}\Lambda_{bb}(\mathbf{x}_b-\mu_b)\\
\end{aligned}</script><p>我们先只考虑包括 $\mathbf{x}_b$ 的项, 然后通过平方来简化积分, 找到仅包括 $\mathbf{x}_b$ 的项</p>
<script type="math/tex; mode=display">
\begin{aligned}

-\frac{1}{2}\mathbf{x}_b^{\mathrm{T}}\Lambda_{bb}\mathbf{x}_b+\mathbf{x}_b^{\text{T}}\mathbf{m} &= -\frac{1}{2}(\mathbf{x}_b-\Lambda_{bb}^{-1}\mathbf{m})^{\text{T}}\Lambda_{bb}(\mathbf{x}_b)-\Lambda_{bb}^{-1\mathbf{m}}+\frac{1}{2}\mathbf{m}^{\text{T}}\Lambda_{bb}^{-1}\mathbf{m}\\

\mathbf{m} &= \Lambda_{bb}\mu_b-\Lambda_{ba}(\mathbf{x}_a-\mu_a)

\end{aligned}</script><p>…… 边缘分布为</p>
<script type="math/tex; mode=display">
p(\mathbf{x}_a) = \mathcal{N}(\mathbf{x}_a|\mu_a, \Sigma_{aa})</script><p>下图为边缘分布 $p(\mathbf{x}_a)$ 和条件分布 $p(\mathbf{x}_a|\mathbf{x}_b=0.7)$ 的图示</p>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/image-20240924143640604.png" style="zoom:33%;" /></p>
<h3 id="Bayes’-theorem-1"><a href="#Bayes’-theorem-1" class="headerlink" title="Bayes’ theorem"></a>Bayes’ theorem</h3><p>条件分布的均值 $\mu<em>{a|b} = \mu_a + \Sigma</em>{ab}\Sigma_{bb}^{-1}(\mathbf{x}_b-\mu_b)$ 是关于 $\mu_b$ 的线性函数. 这里我们假设给定一个高斯边缘分布 $p(\mathbf{x})$ 和一个高斯条件分布 $p(\mathbf{y}|\mathbf{x})$, 后者是前者的线性函数, 且协方差与 $\mathbf{x}$ 无关, 这里边缘分布和条件分布表示如下, $\Lambda$ 和 $\mathbf{L}$ 为精度矩阵, 假设 $\mathbf{x}$ 的维度为 $M$, $\mathbf{y}$ 的维度为 $D$, 矩阵 $\mathbf{A}$ 为 $D\times M$</p>
<script type="math/tex; mode=display">
\begin{aligned}

p(\mathbf{x}) &= \mathcal{N}(\mathbf{x}|\mu, \Lambda^{-1})\\

p(\mathbf{y}|\mathbf{x}) &= \mathcal{N}(\mathbf{y}|\mathbf{Ax}+\mathbf{b}, \mathbf{L}^{-1})

\end{aligned}</script><p>经推导, 贝叶斯定理的另两项为</p>
<script type="math/tex; mode=display">
\begin{aligned}

p(\mathbf{y}) &= \mathcal{N}(\mathbf{y}|\mathbf{A\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A\Lambda}^{-1}\mathbf{A}^{\mathrm{T}})\\

p(\mathbf{x}|\mathbf{y}) &= \mathcal{N}(\mathbf{x}|\Sigma\left\{\mathbf{A}^{\text{T}}\mathbf{L}(\mathbf{y}-\mathbf{b})+\Lambda\mu \right\}, \Sigma)

\end{aligned}</script><p>此处 $\Sigma = (\Lambda+\mathbf{A}^{\text{T}}\mathbf{LA})^{-1}$</p>
<h3 id="Maximum-likelihood"><a href="#Maximum-likelihood" class="headerlink" title="Maximum likelihood"></a>Maximum likelihood</h3><p>给定数据集 $\mathbf{X} = (\mathbf{x}_1, \dots, \mathbf{x}_N)^{\text{T}}$, 假设观测值 ${\mathbf{x}_n}$ 独立的从多元高斯分布中独立抽取, 我们使用最大似然估计来估计分布的参数, 对数似然函数如下</p>
<script type="math/tex; mode=display">
\ln p(\mathbf{X}|\mu, \Sigma) = \frac{ND}{2}\ln(2\pi)-\frac{N}{2}\ln|\Sigma|-\frac{1}{2}\sum_{n=1}^N(\mathbf{x}_n-\mu)^{\text{T}}\Sigma^{-1}(\mathbf{x}_n-\mu)</script><p>对数似然函数值关于数据集仅与以下两个量有关, 被称为高斯分布的<strong>充分统计量</strong></p>
<script type="math/tex; mode=display">
\sum_{n=1}^N\mathbf{x}_n\qquad \sum_{n=1}^N\mathbf{x}_n\mathbf{x}_n^{\text{T}}</script><p>对数似然函数分别对 $\mu$ 求导 ($\Sigma$ 的值比较难求, 可以用其他方法求得), 得到</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mu_{\text{ML}} &= \frac{1}{M}\sum_{n=1}^N\mathbf{x}_n\\

\Sigma_{\text{ML}} &= \frac{1}{N}\sum_{n=1}^N (\mathbf{x}_n-\mu_{\text{ML}})(\mathbf{x}_n-\mu_{\text{ML}})^{\text{T}}

\end{aligned}</script><p>期望有</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[\mu_{\text{ML}}] &= \mu\\

\mathbb{E}[\Sigma_{\text{ML}}] &= \frac{N-1}{N}\Sigma

\end{aligned}</script><blockquote>
<p>上面是有偏估计, 对方差的修正为</p>
<script type="math/tex; mode=display">
s\tilde{\Sigma} = \frac{1}{N-1}\sum_{n=1}^N(\mathbf{x}_n-\mu_{\text{ML}})(\mathbf{x}_n-\mu_{\text{ML}})^{\text{T}} = \Sigma</script></blockquote>
<h3 id="Sequential-estimation"><a href="#Sequential-estimation" class="headerlink" title="Sequential estimation"></a>Sequential estimation</h3><p>上面最大似然估计的方法将整个训练数据集看成整体考虑. 此外, 也可以使用顺序方法 (Sequential methods), 一次处理一个数据点, 然后丢弃该数据. 这种方法对于在线应用和大数据集非常重要</p>
<p>我们假设基于 $N$ 个观测值的样本均值为 $\mu_\text{ML}^{(N)}$, 考虑最后一个样本带来的均值变化</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mu_\text{ML}^{(N)} &= \frac{1}{N}\sum_{n=1}^N\mathbf{x}_n\\

&= \frac{1}{N}\mathbf{x}_N + \frac{1}{N}\sum_{n=1}^{N-1}\mathbf{x}_n\\

&= \frac{1}{N}\mathbf{x}_N + \frac{N-1}{N}\mu_{\text{ML}}^{(N-1)}\\

&= \mu_{\text{ML}}^{(N-1)} +\frac{1}{N}(\mathbf{x}_N - \mu_{\text{ML}}^{(N-1)})

\end{aligned}</script><p>以上公式也有一个很好的解释, 数据点 $\mathbf{x}<em>N$ 将使原有均值向真实均值移动一丢丢, 移动的比例是 $\frac{1}{N}$, 步长是 $\mathbf{x}_N-\mu</em>{\text{ML}}^{(N-1)}$. 可以看到, 当样本数量增加时, 新数据点的影响会变小</p>
<h3 id="Mixtures-of-Gaussians"><a href="#Mixtures-of-Gaussians" class="headerlink" title="Mixtures of Gaussians"></a>Mixtures of Gaussians</h3><p>单纯的高斯分布难以建模真实数据集, 例如数据集中有两个显著的聚类特征. 此时多个高斯分布的叠加能更好地表示数据集分布. <strong>利用多个高斯分布, 调整它们的均值, 协方差和线性组合系数,  可以近似任何连续分布</strong>. $K$ 个高斯分布叠加的分布密度函数为</p>
<script type="math/tex; mode=display">
p(\mathbf{x}) = \sum_{k=1}^K \pi_k\mathcal{N}(\mathbf{x}|\mu_k, \Sigma_k)\\
\sum_{k=1}^K\pi_k = 1\\
0\leq \pi_k\leq 1</script><blockquote>
<p>举个栗子, 多个一维高斯正态分布相加得到红色曲线</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/image-20240924210224819.png" alt=""></p>
<p>根据概率的求和与乘积规则,边缘密度函数可以表示为</p>
<script type="math/tex; mode=display">
p(\mathbf{x}) = \sum_{k=1}^K p(k)p(\mathbf{x}|k)</script><p>这里 $\pi_k=p(k)$ 可以视作第 $k$ 个分量的先验概率. 这里后验概率 $p(k|\mathbf{x})$ 可通过以下公式得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \gamma_k(\mathbf{x}) &= p(k|\mathbf{x})\\
   &= \frac{p(k)p(\mathbf{x}|k)}{\sum_lp(l)p(\mathbf{x}|l)}\\
   &= \frac{\pi_k \mathcal{N}(\mathbf{x}|\mu_k, \Sigma_k)}{\sum_l\pi_l\mathcal{N}(\mathbf{x}|\mu_l, \Sigma_l)}
\end{aligned}</script><p>由于模型有很多的参数, 使用最大似然估计难以得到闭式解. 在第15章中, 可以使用最大化期望的方法来估计</p>
<h2 id="Periodic-Variables"><a href="#Periodic-Variables" class="headerlink" title="Periodic Variables"></a>Periodic Variables</h2><p>周期性变量用极坐标系表示会更好, $0\leq \theta\leq 2\pi$, 比如特定地理位置的盛行风向和日历时间. 极坐标系的原点设置很重要, 以既有 0° 为原点, 假设有 1° 和 359° 两个观测数据点, 此时观测到的样本均值为 180°, 标准差为 179°; 如果以既有 180° 为原点, 样本均值为 0°, 标准差为 1°, 所以周期性变量需要一种特殊的定义方法</p>
<p>为了更方便地处理数据, 选择坐标系原点很关键</p>
<h3 id="Von-Mises-distribution"><a href="#Von-Mises-distribution" class="headerlink" title="Von Mises distribution"></a>Von Mises distribution</h3><p>考虑周期变量 $\theta$ 的一组观测数据 $\mathcal{D} = {\theta<em>1, \dots, \theta_N }$, 每个观测点可以视为单位圆上的点, 用二维单位向量 $\mathbf{x}_1, \dots, \mathbf{x}_N, |\mathbf{x_i}=1 |$ 表示. 将向量取均值有 $\overline{\mathbf{x}} = \frac{1}{N}\sum</em>{n=1}^{N}\mathbf{x}_n$, 极坐标系下有</p>
<script type="math/tex; mode=display">
\overline{\mathbf{x}} = (\overline{r}\cos \overline{\theta}, \overline{r}\sin{\overline{\theta}}) = \left(\frac{1}{N}\sum_{n=1}^N\cos\theta_n, \frac{1}{N}\sum_{n=1}^N\sin\theta_n \right)</script><p>用均值向量的角度作为平均角度, 所以平均角度为</p>
<script type="math/tex; mode=display">
\overline{\theta} = \tan^{-1}\left\{\frac{\sum_n\sin\theta_n}{\sum_n\cos\theta_n}   \right\}</script><p>上面的结果也是极大似然估计的结果, 下证</p>
<blockquote>
<p>以下证明限制在单变量分布上, 类似的周期性分布也可以在任意维度的超球面上找到. 首先, <strong>引入一个称为冯·米赛斯分布的高斯分布</strong>的周期性推广:</p>
<p>考虑周期为 $2\pi$ 的分布 $p(\theta)$, $p(\theta)$ 非负归一化且有周期性, 所以符合以下条件</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\theta)&\geq0\\
\int_{0}^{2\pi}p(\theta)\mathrm{d}\theta &= 1\\
p(\theta+2\pi) &= p(\theta)\

\end{aligned}</script><p>考虑二元高斯分布变量 $\mathbf{x} = (\mathbf{x}_1, \mathbf{x}_2)$, 均值为 $\mu = (\mu_1, \mu_2)$, 协方差矩阵 $\Sigma = \sigma^2\mathbf{I}$, 这里 $\mathbf{I}$ 是一个 2×2 矩阵, 可以得到该高斯分布的概率密度函数</p>
<script type="math/tex; mode=display">
p(x_1, x_2) = \frac{1}{2\pi\sigma^2}\exp\left\{-\frac{(x_1-\mu_1)^2+(x_2-\mu_2)^2}{2\sigma^2}   \right\}</script><p>下面把上述公式转换为极坐标系$x_1 = r\cos\theta, x_2 = r\sin\theta$, 同理$\mu_1 = r_0\cos\theta_0, \mu_2 = r_0\sin\theta_0$, 带换进上面概率密度函数, 在单位圆 $r=1$ 的条件下只分析指数部分有</p>
<script type="math/tex; mode=display">
-\frac{1}{2\sigma^2}\left\{(r\cos\theta-r_0\cos\theta_0)^2+(r\sin\theta-r_0\sin\theta_0)^2  \right\}\\

\begin{aligned}



&= -\frac{1}{2\sigma^2}\left\{1+r_0^2-2r_0\cos\theta\cos\theta0-2r_0\sin\theta\sin\theta_0  \right\}\\
&= \frac{r_0}{\sigma^2}\cos(\theta-\theta_0)+\mathrm{const}


\end{aligned}</script><p>上面常数项是独立于 $\theta$ 的部分, 不予考虑. 上面化简用到了以下公式</p>
<script type="math/tex; mode=display">
\cos^2A+\sin^2A = 1\\
\cos A\cos B + \sin A \sin B = \cos(A-B)</script><p>定义 $m = \frac{r_0}{\sigma^2}$, 得到 $p(\theta)$ 在 $r=1$ 下的表达式</p>
<script type="math/tex; mode=display">
p(\theta|\theta_0, m) = \frac{1}{2\pi I_0(m)}\exp\left\{m\cos(\theta-\theta_0)  \right\}\\

I_0(m) = \frac{1}{2\pi}\int_0^{2\pi}\exp\left\{m\cos\theta  \right\}\mathrm{d}\theta</script><p>上面的公式叫做冯·米赛斯分布或环形正态分布, 这里 $\theta_0$ 对应分布的均值, $m$ 称为集中参数, 类似于高斯分布的精度 (方差的倒数), $I_0(m)$ 称为归一化系数, 是第一类零阶修正贝塞尔函数 (?), $m$ 越大也即方差越小, 分布越接近高斯分布</p>
<p><img src="https://cdn.jsdelivr.net/gh/421zuoduan/blog-imgs@main/imgs/20250310141202.png" alt="环形正态分布"></p>
<p><strong>考虑环形高斯正态分布的最大似然估计</strong>, 参数 $\theta_0$, 集中参数 $m$下有</p>
<script type="math/tex; mode=display">
\ln p(\mathcal{D}|\theta_0, m) = -N\ln(2\pi) - N\ln I_0(m)+m\sum_{n=1}^N\cos(\theta_n-\theta_0)</script><p>对 $\theta_0$ 求导有</p>
<script type="math/tex; mode=display">
\sum_{n=1}^N\sin(\theta_n-\theta_0) = 0</script><blockquote>
<p>这里求导用到了三角恒等式</p>
<script type="math/tex; mode=display">
\sin(A-B) = \cos B\sin A-cos A\sin B</script></blockquote>
<p>于是得到 </p>
<script type="math/tex; mode=display">
\theta_0^{\mathrm{ML}} = \tan^{-1}\left\{\frac{\Sigma_n\sin\theta_n}{\Sigma_n\cos\theta_n}  \right\}</script><p>同理对 $m$ 求导, 利用 $I’_0(m) = I_1(m)$, 再代入 $\theta_0^{\mathrm{ML}}$ 有</p>
<script type="math/tex; mode=display">
\begin{aligned}

A(m_{\mathrm{ML}}) &= \frac{1}{N}\sum_{n=1}^N\cos(\theta_n-\theta_0^{\mathrm{ML}}) = \frac{I_1(m)}{I_0(m)}\\

&= \left(\frac{1}{N}\sum_{n=1}^N\cos\theta_n\right)\cos\theta_0^{\mathrm{ML}} + \left(\frac{1}{N}\sum_{n=1}^N\sin\theta_n\right)\sin\theta_0^{\mathrm{ML}} 

\end{aligned}</script><p>环形正态分布的局限在于它是单峰的, 环形正态分布的混合模型可用于建模周期性变量. 还有一些其他可用于建模周期性变量的方法, 如直方图, 不再展开</p>
</blockquote>
<h2 id="The-Exponential-Family"><a href="#The-Exponential-Family" class="headerlink" title="The Exponential Family"></a>The Exponential Family</h2><h2 id="Nonparam"><a href="#Nonparam" class="headerlink" title="Nonparam"></a>Nonparam</h2>
  </article>

  <!-- prev and next -->
  <div class="flex justify-between mt-4 pt-4
    border-t border-[var(--c-sep)] text-sm
    gap-2 text-[var(--c-50)]
  ">
    <div>
      
        <a href="/2024/08/01/Article/2024/2024-08-01-01-46/"
          class="
            transition-all
            flex justify-center
            hover:-translate-x-1
            hover:text-[var(--c-80)]
          ">
          <iconify-icon width="20" icon="mingcute:left-fill" data-inline="false">
          </iconify-icon>
          八月一日夏季午夜梦回
        </a>
      
    </div>
    <div>
      
        <a href="/2024/07/22/Article/F/%E6%88%91%E4%B8%8D%E7%88%B1%E4%BD%A0/"
          class="
            flex 
            justify-center
            hover:translate-x-1 
            transition-transform
            hover:text-[var(--c-100)]
          "
        >
          我不爱你了
          <iconify-icon width="20" icon="mingcute:right-fill" data-inline="false"></iconify-icon>
        </a>
      
    </div>
  </div>

  <!-- comment -->
  <div class="article-comments mt-12">
    
  <script src="https://giscus.app/client.js"
  data-repo="421zuoduan/blog-giscus-discussion"
  data-repo-id="R_kgDONKEKag"
  data-category="Announcements"
  data-category-id="DIC_kwDONKEKas4Cj9R8"
  data-mapping="pathname"
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="1"
  data-input-position="top"
  data-theme="preferred_color_scheme"
  data-lang="zh-CN"
  data-loading="lazy"
  crossorigin="anonymous"
  async>
</script>
<script>
  window.onload = function () {
    console.log("giscus loaded");
    const isDark = document.documentElement.classList.contains("dark");
    const giscusFrame = document.querySelector("iframe.giscus-frame");
    giscusFrame.contentWindow.postMessage(
      {
        giscus: {
          setConfig: {
            theme: isDark ? "dark" : "light",
          },
        },
      },
      "https://giscus.app"
    );
  };
</script>


  </div>
</section>
<!-- js inspect -->

<script src="/lib/clipboard.min.js"></script>


<script async src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
  $(document).ready(() => {
    const maraidConfig = {
      theme: "default",
      logLevel: 3,
      flowchart: { curve: "linear" },
      gantt: { axisFormat: "%m/%d/%Y" },
      sequence: { actorMargin: 50 },
    };
    mermaid.initialize(maraidConfig);
  });
</script>



<script src="/lib/fancybox/fancybox.umd.min.js"></script>

<script>
  $(document).ready(() => {
    $('.post-content').each(function(i){
      $(this).find('img').each(function(){
        if ($(this).parent().hasClass('fancybox') || $(this).parent().is('a')) return;
        var alt = this.alt;
        var title = this.title;
        if (alt) $(this).after('<span class="fancybox-alt">' + alt + '</span>');
        if (title) $(this).after('<span class="fancybox-title">' + title + '</span>');
        $(this).wrap('<a class="fancybox-img" href="' + this.src + '" data-fancybox=\"gallery\" data-caption="' + alt + '"></a>')
      });
      $(this).find('.fancybox').each(function(){
        $(this).attr('rel', 'article' + i);
      });
    });

    Fancybox.bind('[data-fancybox="gallery"]', {
        // options
    })
  })
</script>

<!-- tocbot begin -->

<script src="/lib/tocbot/tocbot.min.js"></script>

<script>
  $(document).ready(() => {
      tocbot.init({
        // Where to render the table of contents.
        tocSelector: '.post-toc',
        // Where to grab the headings to build the table of contents.
        contentSelector: '.post-content',
        // Which headings to grab inside of the contentSelector element.
        headingSelector: 'h1, h2, h3',
        // For headings inside relative or absolute positioned containers within content.
        hasInnerContainers: true,
    });
  })
</script>
<!-- tocbot end -->

  </main>
  <footer class="flex flex-col mt-18 mb-12 items-center
  text-[var(--c-50)] text-sm">
  <div class="flex flex-row items-center my-12">
    
    
        
        
            
            
        
        <a class="
            hover:text-[var(--c-theme)]
            hover:bg-[var(--c-20)]
            rounded-lg
            p-2
            my-1
            flex flex-row items-center
            group" title="Github" target="_blank" rel="noopener" href="https://www.github.com/421zuoduan">
            <iconify-icon width="28" icon="mingcute:github-fill"></iconify-icon>
        </a>
    
        
        
            
            
        
        <a class="
            hover:text-[var(--c-theme)]
            hover:bg-[var(--c-20)]
            rounded-lg
            p-2
            my-1
            flex flex-row items-center
            group" title="ZhiHu" target="_blank" rel="noopener" href="https://www.zhihu.com/people/ren-jian-lan-xue">
            <iconify-icon width="28" icon="ri:zhihu-line"></iconify-icon>
        </a>
    

  </div>
  <!-- busuanzi -->
  <div class="mb-6">
    
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!-- Busuanzi Analytics -->
<div class="flex flex-col items-center mb-2">
  <div class="flex flex-row items-center">
    <iconify-icon width="16" icon="ic:round-person" width="18"></iconify-icon>
    <span class="mr-1">访客 Visitors: </span>
    <span id="busuanzi_value_site_uv"></span>
  </div>
  <div class="flex flex-row items-center">
    <iconify-icon width="16" icon="carbon:view-filled" width="18"></iconify-icon>
    <span class="mx-1">浏览量 Page Views:</span>
    <span id="busuanzi_value_site_pv"></span>
  </div>
</div>
<!-- End Busuanzi Analytics -->


  </div>
  <!-- copyright -->
  <div class="flex flex-row items-center gap-2">
    <a class="hover:underline"
      target="_blank"
      href="https://creativecommons.org/licenses/by-nc-sa/4.0/"
    >
      CC BY-NC-SA 4.0
    </a>
    <span>© 2022-2024</span>
    <a class="hover:underline"
    href="https://github.com/chen-yingfa" 
    target="_blank" 
    rel="noopener noreferrer">陈英发</a>
  </div>
  <!-- powered by -->
  <div class="flex items-center gap-1">
    <span>Powered by</span>
    <a class="hover:underline" 
    href="https://hexo.io/" target="_blank" rel="noopener noreferrer">Hexo</a>
    <span>&</span>
    <a href="https://github.com/chen-yingfa/hexo-theme-fengye" 
    class="hover:underline"
    target="_blank"
      rel="noopener noreferrer"
      >
      枫叶 Fengye
    </a>
  </div>

</footer>

  <div class="
    back-to-top
    fixed right-6
    z-1024
    -bottom-20
    rounded-lg
    font-bold
    py-1 px-2
    text-[var(--c-80)]
    bg-[var(--c-20)]
    cursor-pointer
    text-center
    drop-shadow-md
  ">
    <span class="flex justify-center items-center text-sm">
      <span id="scrollpercent"><span>0</span> %</span>
      <iconify-icon width="18" icon="mingcute:arrow-to-up-fill" id="go-top"></iconify-icon>
    </span>
  </div>
  
<script src="/js/main.js"></script>


  <div class="fixed top-0 bottom-0 left-0 right-0 pointer-events-none print:hidden" id="maple"></div>
</body>

</html>
