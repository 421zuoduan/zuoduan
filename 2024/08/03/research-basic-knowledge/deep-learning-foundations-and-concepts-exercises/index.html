<!DOCTYPE html>
<html lang="zh-cn,en,default">
<head>
  <meta charset="UTF-8">




<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

  <meta name="author" content="崔若晨 Ruochen Cui">


  <meta name="subtitle" content="崔若晨">


  <meta name="description" content="电子科技大学四年级本科生

A fourth year undergraduate student at UESTC
">


  <meta name="keywords" content="Ruochen Cui,崔若晨,Artificial Intelligence">


<title>《Deep Learning Foundations and Concepts》课后习题 | Ruochen Cui</title>



<link rel="icon" href="/web_ico.ico">


<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"
/>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">


<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/css/search.css">



<script src="/lib/jquery.min.js"></script>


<script src="/lib/iconify-icon.min.js"></script>


<script src="https://cdn.tailwindcss.com?plugins=typography"></script>
<script>
  tailwind.config = {
    darkMode: "class",
  };
</script>

<script>
  (function () {
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const setting = localStorage.getItem("hexo-color-scheme") || "auto";
    if (setting === "dark" || (prefersDark && setting !== "light"))
      document.documentElement.classList.toggle("dark", true);
    let isDark = document.documentElement.classList.contains("dark");
  })();

  $(document).ready(function () {
    // init icon
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const isDark = document.documentElement.classList.contains("dark");
    $("#theme-icon").attr("icon", isDark ? "ic:round-dark-mode" : "ic:round-light-mode");

    function toggleGiscusTheme() {
      const isDark = document.documentElement.classList.contains("dark");
      const giscusFrame = document.querySelector("iframe.giscus-frame");
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage(
          {
            giscus: {
              setConfig: {
                theme: isDark ? "dark" : "light",
              },
            },
          },
          "https://giscus.app"
        );
      }
    }


    // toggle dark mode
    function toggleDark() {
      let isDark = document.documentElement.classList.contains("dark");
      const setting = localStorage.getItem("hexo-color-scheme") || "auto";
      isDark = !isDark;
      document.documentElement.classList.toggle("dark", isDark);
      $("#theme-icon").attr("icon", isDark ? "ic:round-dark-mode" : "ic:round-light-mode");
      if (prefersDark === isDark) {
        localStorage.setItem("hexo-color-scheme", "auto");
      } else {
        localStorage.setItem("hexo-color-scheme", isDark ? "dark" : "light");
      }
      toggleGiscusTheme();
    }
    $("#toggle-dark").click(toggleDark);

    // listen dark mode change
    window
      .matchMedia("(prefers-color-scheme: dark)")
      .addEventListener("change", (e) => {
        const setting = localStorage.getItem("hexo-color-scheme") || "auto";
        if (setting === "auto") {
          document.documentElement.classList.toggle("dark", e.matches);
          $("#theme-icon").attr(
            "icon",
            e.matches ? "ic:round-dark-mode" : "ic:round-light-mode"
          );
          toggleGiscusTheme();
        }
      });
  });
</script>




<meta name="generator" content="Hexo 7.1.1"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
<body 
  class="
    bg-[var(--c-0)]
    text-[var(--c-80)]
  ">
  <!-- The navigation bar -->
<header class="
    flex flex-row items-center
    w-full
    pr-4
    z-10
    border-b-[1px]
    border-b-[var(--c-border)]
    dark:bg-[var(--c-0)]
    dark:border-b-[var(--c-0)]
    gap-2
    h-[var(--h-header)]
    text-[var(--c-80)]
">
  <!-- Left part -->
  <div class="overflow-hidden h-full flex flex-row items-center">
    <!-- Site Title on the top left -->
    <a href="/" class="
            whitespace-nowrap
            text-2xl
            text-[var(--c-theme)]
            hover:text-[var(--c-theme)]
            pl-4
            font-black
            bg-gradient-to-r from-cyan-500
            to-blue-500 bg-clip-text text-transparent
          ">
      Ruochen Cui
    </a>
  </div>
  <!-- Div for pushing items to both sides -->
  <div class="flex-1"></div>
  <!-- Right part -->
  <div class="flex flex-row items-center z-20 h-full">
    <!-- Page links -->
    <div class="hidden sm:flex flex-row h-full">
      
      
      
      
      
      
      <a href="/./archives" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:inbox-fill" width="22">
        </iconify-icon>
        
        
        <p>Posts</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./publications" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:science-fill" width="22">
        </iconify-icon>
        
        
        <p>Publications</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./about" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:user-info-fill" width="22">
        </iconify-icon>
        
        
        <p>About</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./categories" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:classify-2-fill" width="22">
        </iconify-icon>
        
        
        <p>Categories</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./tags" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:tag-fill" width="22">
        </iconify-icon>
        
        
        <p>Tags</p>
        
      </a>
      
      
      
      
      
      
      <a href="/./index" class="
                        flex flex-row items-center
                        gap-1
                        hover:underline
                        hover:bg-[var(--c-20)]
                        hover:text-[var(--c-theme)]
                        transition-all
                        px-2
                        py-1
                        my-1
                        rounded-lg
                        group
                    ">
        
        <iconify-icon class="group-hover:scale-125 transition-transform" icon="mingcute:home-2-fill" width="22">
        </iconify-icon>
        
        
      </a>
      
    </div>
    <!-- Icons on the right -->
    <div class="flex flex-row items-center">

      <!-- TODO: Add search icon here -->

      <!-- Dark/light toggle icon -->
      <a class="flex group p-1" title="toggle theme" id="toggle-dark">
        <iconify-icon class="transition-transform
                    group-hover:rotate-[45deg]
                    group-hover:scale-125
                    group-hover:text-[var(--c-theme)]" width="24" id="theme-icon">
        </iconify-icon>
      </a>
      <!-- Icon for dropout menu on small screens -->
      <div class="flex p-1 mx-1 sm:hidden">
        <a class="w-5 h-5" aria-hidden="true" id="open-menu">
          <iconify-icon width="24" icon="mingcute:menu-fill" class="transition-transform hover:scale-125 hover:rotate-[5deg]">
          </iconify-icon>
        </a>
        <a class="w-5 h-5 hidden" aria-hidden="true" id="close-menu">
          <iconify-icon width="24" icon="mingcute:close-circle-fill" class="transition-transform hover:scale-125 hover:rotate-[80deg]">
          </iconify-icon>
        </a>
      </div>
    </div>
  </div>
</header>

<!-- Dropdown menu on small screens -->
<div id="menu-panel" class="
        h-0
        overflow-hidden
        sm:hidden
        w-full
        z-10
        rounded
    ">
  <div id="menu-content" class="
        flex
        flex-row
        justify-center
        items-center
        font-bold
        text-xl
        border-b-[1px]
        relative
        z-20
        border-[var(--c-sep)]
        px-2
        py-2
        -translate-y-full
        transition-transform
        duration-200
        ">
    
    
    
    <a href="/./archives" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:inbox-fill" width="22">
      </iconify-icon>
      <p>
        posts
      </p>
    </a>
    
    
    
    
    <a href="/./publications" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:science-fill" width="22">
      </iconify-icon>
      <p>
        publications
      </p>
    </a>
    
    
    
    
    <a href="/./about" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:user-info-fill" width="22">
      </iconify-icon>
      <p>
        about
      </p>
    </a>
    
    
    
    
    <a href="/./categories" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:classify-2-fill" width="22">
      </iconify-icon>
      <p>
        categories
      </p>
    </a>
    
    
    
    
    <a href="/./tags" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:tag-fill" width="22">
      </iconify-icon>
      <p>
        tags
      </p>
    </a>
    
    
    
    
    <a href="/./index" class="
                flex flex-row items-center
                gap-2
                h-12
                hover:underline
                hover:bg-[var(--c-20)]
                px-3
                py-1
                rounded-lg
            ">
      <iconify-icon icon="mingcute:home-2-fill" width="22">
      </iconify-icon>
      <p>
        home
      </p>
    </a>
    
    
  </div>
</div>
  <main>
    <!-- css -->

<link rel="stylesheet" href="/lib/fancybox/fancybox.min.css">

  
<link rel="stylesheet" href="/lib/tocbot/tocbot.min.css">

    <!-- toc -->
    
  <!-- tocbot -->
<nav class="post-toc toc text-sm w-40 relative top-32 right-4 opacity-70 hidden lg:block" style="position: fixed !important;"></nav>


<section class="px-6 max-w-prose mx-auto md:px-0">
  <!-- Post header before content -->
  <header class="py-4">
    <div class="flex flex-col gap-2 pt-4 md:pt-6">
      <!-- Title -->
      <div id="article-title" class="leading-snug">
        <p class="text-3xl font-bold text-[var(--c-100)] mb-4">《Deep Learning Foundations and Concepts》课后习题</p>
      </div>
      <!-- Meta data -->
      <div>
        <section class="
          flex flex-col gap-x-2 gap-y-1 text-sm text-[var(--c-100)]">
          <div class="flex flex-wrap items-center gap-x-2 gap-y-1">
            <!-- Dates -->
            <div class="flex items-center gap-1">
              <iconify-icon width="18" icon="mingcute:add-circle-fill" ></iconify-icon>
              Created: <time class="w-max">2024-08-03</time>
            </div>
            <div class="flex items-center gap-1">
              <iconify-icon width="18" icon="mingcute:refresh-3-fill" ></iconify-icon>
              Edited: <time class="w-max">2024-12-29</time>
            </div>
          </div>
          <div class="flex flex-wrap items-center gap-x-3 gap-y-3">
            <!-- Author -->
            
              <span class="flex items-center gap-1 group">
                <iconify-icon width="18" icon="mingcute:user-edit-fill" ></iconify-icon>
                <p>myself</p>
              </span>
            

            <!-- Word count -->
            <span class="flex items-center gap-1">
              <iconify-icon width="18" icon="mingcute:book-2-fill" ></iconify-icon>
              <span>6.4k words, 36 min</span>
            </span>
            <!-- Categories -->
            
              <!-- <span class="text-gray-400">·</span> -->
              <span class="flex flex-row items-center gap-1 group hover:underline">
                <iconify-icon class="transition-all group-hover:scale-125 mr-0"
                  width="18"
                  icon="mingcute:classify-2-fill">
                </iconify-icon>
                <a class="article-category-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a>
              </span>
            
          </div>
        </section>
      </div>
      <!-- tags -->
      <div>
        
<div class="flex flex-wrap gap-1">
  
    
      <a href="/tags/math/" 
        class="
          tag
          text-sm
          rounded-full
          px-[5px]
          border-[1px]
          border-[var(--c-theme)]
          text-[var(--c-theme)]
          bg-[var(--c-0)]
          dark:bg-[var(--c-0)]
          dark:drop-shadow-none
          hover:bg-[var(--c-theme)]
          hover:text-[var(--c-0)]
          dark:hover:text-[var(--c-theme-2)]
        ">
        math
      </a>
    
      <a href="/tags/note/" 
        class="
          tag
          text-sm
          rounded-full
          px-[5px]
          border-[1px]
          border-[var(--c-theme)]
          text-[var(--c-theme)]
          bg-[var(--c-0)]
          dark:bg-[var(--c-0)]
          dark:drop-shadow-none
          hover:bg-[var(--c-theme)]
          hover:text-[var(--c-0)]
          dark:hover:text-[var(--c-theme-2)]
        ">
        note
      </a>
    
  
</div>
      </div>
    </div>
  </header>
  <!-- content -->
  <article class="post-content prose m-auto dark:prose-invert">
    <h1 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h1><h2 id="课本延伸"><a href="#课本延伸" class="headerlink" title="课本延伸"></a>课本延伸</h2><script type="math/tex; mode=display">
\begin{aligned}
\text{Given}\quad p(T=1|C=1) &= 0.9\\
p(T=1|C=0) &= 0.03\\
p(C=1) &= 0.001\\
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\mathrm{have}\quad p(T=1) &= p(T=1|C=0)p(C=0) + p(T=1|C=1)p(C=1)\\
&= 0.03087\\
\quad p(C=0|T=1) &=\frac{p(T=1|C=0)p(C=0)}{p(T=1)}\\


\quad p(C=1|T=1) &= \frac{p(T=1|C=1)p(C=1)}{p(T=1)}\\
&= \frac{0.9\times0.001}{0.03087} \simeq 0.02915
\end{aligned}</script><h2 id="Efron-Dice"><a href="#Efron-Dice" class="headerlink" title="Efron Dice"></a>Efron Dice</h2><p>Assume values of Blue as $R$, Yellow as $Y$, Green as $G$, Blue as $B$.</p>
<script type="math/tex; mode=display">
p(B>Y) = p(B=4, Y=3) =p(B=4)p(Y=3) = \frac{2}{3}\\
p(G>B) = p(G=5, B=4)+p(G=5,B=0) = \frac{1}{2}+\frac{1}{6} = \frac{2}{3} \\
p(R>G) = p(R=6, G=5)+p(R=6, G=1)+p(R=2, G=1) = \frac{1}{6}+\frac{1}{6}+\frac{1}{3} = \frac{2}{3}\\
p(Y>R) = p(Y=3, R=2) = \frac{2}{3}</script><h2 id="一维卷积"><a href="#一维卷积" class="headerlink" title="一维卷积"></a>一维卷积</h2><p>As $\mathbf{u}$ and $\mathbf{v}$ are independent, we have</p>
<script type="math/tex; mode=display">
p_{\mathbf{u,v}}(u, v) = p_{\mathbf{u}}(u)p_{\mathbf{v}}(v)\\

p_{\mathbf{u,v}}(u, y-u) = p_{\mathbf{u}}(u)p_\mathbf{v}(y-u)</script><p> To get $\mathbf{y}$ with $\mathbf{u}$ and $\mathbf{v}$, we sum over them, which can be done through the marginal density function and integral. Therefore,</p>
<script type="math/tex; mode=display">
p_\mathbf{y}(y) = \int p_\mathbf{u}(u)p_\mathbf{v}(y-u)\mathrm{d}u</script><h2 id="均匀分布的均值和方差"><a href="#均匀分布的均值和方差" class="headerlink" title="均匀分布的均值和方差"></a>均匀分布的均值和方差</h2><p>Uniform distribution:</p>
<script type="math/tex; mode=display">
p(x) = \frac{1}{d-c}, x\in (c, d)</script><p>It is normalized because</p>
<script type="math/tex; mode=display">
\begin{aligned}
\int_{-\infty}^{+\infty}p(x) &= \int_{-\infty}^{+\infty}\frac{1}{d-c}\mathrm{d} x\quad x\in(c, d)\\
&=\int_c^d \frac{1}{d-c}\mathrm{d} x\\
&=\frac{d-c}{d-c} = 1
\end{aligned}</script><p>The mean of uniform distribution is <script type="math/tex">, the variance is</script></p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}(x) &= \int_{-\infty}^{+\infty}p(x)x\mathrm{d}x\quad x\in(c,d)\\
&= \int_{-\infty}^{+\infty}\frac{1}{d-c}x\mathrm{d}x\\
&= \frac{1}{d-c}\int_c^d x\mathrm{d}x\\
&= \frac{1}{2(d-c)}x^2\bigg|_c^d\\
&=\frac{c+d}{2}
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}(x^2) &= \int_{-\infty}^{+\infty}p(x)x\mathrm{d}x\quad x\in(c,d)\\
&= \int_{-\infty}^{+\infty}\frac{1}{d-c}x^2\mathrm{d}x\\
&= \frac{1}{d-c}\int_c^d x^2\mathrm{d}x\\
&= \frac{1}{3(d-c)}x^3\bigg|_c^d\\
&=\frac{c^2+cd+d^2}{3}
\end{aligned}</script><h2 id="指数分布和拉普拉斯分布概率密度函数归一化"><a href="#指数分布和拉普拉斯分布概率密度函数归一化" class="headerlink" title="指数分布和拉普拉斯分布概率密度函数归一化"></a>指数分布和拉普拉斯分布概率密度函数归一化</h2><p>Exponential distribution:</p>
<script type="math/tex; mode=display">
p(x|\lambda) = \lambda\mathrm{exp}(-\lambda x), \qquad x\geq 0</script><p>Therefore, </p>
<script type="math/tex; mode=display">
\begin{aligned}
\int_{-\infty}^{+\infty} p(x|\lambda) &= \int_{-\infty}^{+\infty}\lambda\mathrm{exp}(-\lambda x)\mathrm{d}x,\qquad x\geq 0\\
&= -\int_{-\infty}^{+\infty}\mathrm{exp}(-\lambda x)\mathrm{d}(-\lambda x)\\
&= -e^{-\lambda x}\bigg|_0^{+\infty}\\
&= -0-(-1) = 1
\end{aligned}</script><p>Laplace distribution:</p>
<script type="math/tex; mode=display">
p(x|\mu, \gamma) = \frac{1}{2\gamma}\mathrm{exp}\left(-\frac{|x-\mu|}{\gamma}\right)</script><p>Therefore,</p>
<script type="math/tex; mode=display">
\begin{aligned}
\int_{-\infty}^{+\infty} p(x|\lambda) &= \int_{-\infty}^{+\infty}\frac{1}{2\gamma}\mathrm{exp}\left(-\frac{|x-\mu|}{\gamma}\right)\mathrm{d}x\\

&= \frac{1}{2\gamma}\int_{-\infty}^{\mu}\mathrm{exp}\left(\frac{x-\mu}{\gamma}\right)\mathrm{d}x+\frac{1}{2\gamma}\int_{\mu}^{+\infty}\mathrm{exp}\left(-\frac{x-\mu}{\gamma}\right)\mathrm{d}x\\

&=\frac{1}{2\gamma}\cdot \gamma\mathrm{exp}\left(\frac{x-\mu}{\gamma}\right)\bigg|_{-\infty}^{\mu} - \frac{1}{2\gamma}\cdot\gamma\mathrm{exp}\left(-\frac{x-\mu}{\gamma}\right)\bigg|_{\mu}^{+\infty}\\

&=\frac{1}{2} + \frac{1}{2} = 1

\end{aligned}</script><h2 id="狄拉克函数的归一化"><a href="#狄拉克函数的归一化" class="headerlink" title="狄拉克函数的归一化"></a>狄拉克函数的归一化</h2><p>Dirac delta function</p>
<blockquote>
<p>“we can think of this as an infinitely narrow and infinitely tall spike located at $x=\mu$ with the property of having unit area”</p>
</blockquote>
<script type="math/tex; mode=display">
p(x|\mu) = \delta(x-\mu)</script><p>with a set of observations of $x$ by $\mathcal{D} = {x_1, \dots, x_N}$, it can be represented here, </p>
<script type="math/tex; mode=display">
p(x|\mathcal{D}) = \frac{1}{N}\sum^N_{n=1}\delta(x-x_n)</script><p>Therefore,</p>
<script type="math/tex; mode=display">
\begin{aligned}
\int_{-\infty}^{+\infty} p(x)\mathrm{d}x &=\int_{-\infty}^{+\infty} \frac{1}{N}\sum^N_{n=1}\delta(x-x_n)\mathrm{d}x\\

&= \frac{1}{N}\sum^N_{n=1}\int_{-\infty}^{+\infty}\delta(x-x_n)\mathrm{d}x\\

&=\frac{1}{N}\sum^N_{n=1}1\\
&=1

\end{aligned}</script><h2 id="使用经验密度函数证明公式-2-40"><a href="#使用经验密度函数证明公式-2-40" class="headerlink" title="使用经验密度函数证明公式 2.40"></a>使用经验密度函数证明公式 2.40</h2><script type="math/tex; mode=display">
p(x|\mathcal{D}) = \frac{1}{N}\sum^N_{n=1}\delta(x-x_n)</script><script type="math/tex; mode=display">
\begin{aligned}

E[f] &= \int p(x)f(x)\mathrm{d}x \\


&= \int(\frac{1}{N}\sum^N_{n=1}\delta(x-x_n))f(x)\mathrm{d}x\\

&= \frac{1}{N}\int\sum_N^{n=1}\delta(x-x_n)f(x)\mathrm{d}x   \\

&\simeq \frac{1}{N}\int \sum_{n=1}^N f(x_n)\mathrm{d}x \\
\end{aligned}</script><h2 id="证明方差公式"><a href="#证明方差公式" class="headerlink" title="证明方差公式"></a>证明方差公式</h2><script type="math/tex; mode=display">
\begin{aligned}

\mathrm{var}[f] &= \mathbb{E}\left[(f-\mathbb{E}[f])^2\right]\\

&= \mathbb{E}\left[f^2-2\cdot\mathbb{E}[f]f+\mathbb{E}[f]^2\right]\\

&= \mathbb{E}[f^2]-2\mathbb{E}[f]\cdot f+\mathbb{E}[f]^2\\

&= \mathbb{E}[f]^2-2\mathbb{E}[f]^2+\mathbb{E}[f]^2\\

&= \mathbb{E}[f^2] - \mathbb{E}[f]^2\\

即\mathrm{var}[f] &= \mathbb{E}[f(x)^2]-\mathbb{E}[f(x)]^2

\end{aligned}</script><h2 id="两变量独立-协方差为-0"><a href="#两变量独立-协方差为-0" class="headerlink" title="两变量独立, 协方差为 0"></a>两变量独立, 协方差为 0</h2><script type="math/tex; mode=display">
\begin{aligned}

\mathrm{cov}[x, y] &= \mathbb{E}_{x,y}\left[\{x-\mathbb{E}[x]\}\{y-\mathbb{E}[y]\}\right]\\

&= \mathbb{E}_{x,y}[xy-x\mathbb{E}[y]-y\mathbb{E}[x]+\mathbb{E}[x]\mathbb{E}[y]]   \\

&= \mathbb{E}_{x,y}[xy]-\mathbb{E}[x]\mathbb{E}[y]\\

&= \mathbb{E}[x]\mathbb{E}[y] - \mathbb{E}[x]\mathbb{E}[y]\\

&= 0

\end{aligned}</script><h2 id="两独立的随机变量-均值与方差的加减关系"><a href="#两独立的随机变量-均值与方差的加减关系" class="headerlink" title="两独立的随机变量, 均值与方差的加减关系"></a>两独立的随机变量, 均值与方差的加减关系</h2><script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[x+z] &= \int p(x+z)(x+z)\mathrm{d}(x,z)\\

&= \int\int p(x)p(z)x\mathrm{d}x\mathrm{d}z + \int\int p(x)p(z)z\mathrm{d}x\mathrm{d}z\\

&= \int p(z)\int p(x)x\mathrm{d}x\mathrm{d}z + \int p(x)\int p(z)z\mathrm{d}z\mathrm{d}x\\

&= \int p(z)\mathbb{E}[x]\mathrm{d}z + \int p(x)\mathbb{E}[z]\mathrm{d}x\\

&= \mathbb{E}[x] + \mathbb{E}[z]

\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\mathrm{var}[x+z] &= \mathbb{E}[\{x+z-\mathbb{E}[x+z]\}^2]   \\

&= \mathbb{E}[(x+z)^2-2(x+z)\mathbb{E}[x+z]+\mathbb{E}[x+z]^2]\\

&= \mathbb{E}[x^2+z^2+2xz-2(x+z)\mathbb{E}[x+z]+\mathbb{E}[x+z]^2]\\

&= \mathbb{E}[x^2 + z^2 + 2xz -2x\mathbb{E}[x]-2z\mathbb{E}[z]-2x\mathbb{E}[z]-2z\mathbb{E}[x] + \mathbb{E}[x]^2 + \mathbb{E}[z]^2 + 2\cdot \mathbb{E}[x]\mathbb{E}[z]]\\


&= \mathbb{E}[x^2-2\cdot x\mathbb{E}[x] + \mathbb{E}[x]^2 + z^2-2\cdot z\mathbb{E}[z] + \mathbb{E}[z]^2 + 2\mathbb{E}[xz]+2\mathbb{E}[x]\mathbb[E][z]-z\mathbb{E}[x]-x\mathbb{E}[z]]     \\

&= \mathbb{E}[x^2-2\cdot x\mathbb{E}[x] + \mathbb{E}[x]^2 + z^2-2\cdot z\mathbb{E}[z] + \mathbb{E}[z]^2\\

&= \mathbb{E}[\{x-\mathbb{E}[x]\}^2] + \mathbb{E}[\{z-\mathbb{E}[z]\}^2]   \\

&= \mathrm{var}[x] + \mathrm{var}[z]

\end{aligned}</script><h2 id="证明全期望公式与条件期望和条件方差的的分解公式"><a href="#证明全期望公式与条件期望和条件方差的的分解公式" class="headerlink" title="证明全期望公式与条件期望和条件方差的的分解公式"></a>证明全期望公式与条件期望和条件方差的的分解公式</h2><p>proof  $\mathbb{E}[x] = \mathbb{E}_y[\mathbb{E}_x[x|y]]$</p>
<script type="math/tex; mode=display">
\mathbb{E}_x[x|y] = \int xp(x|y)\mathrm{d}x\\

\mathbb{E}_y[\mathbb{E}_x[x|y]] = \int y\int xp(x|y)\mathrm{d}x\mathrm{d}y</script><script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[x] &= \int xp(x)\mathrm{d}x\\

&= \int x\left[\int p(x|y)p(y)\mathrm{d}y\right]\mathrm{d}x \\

&= \int x\int p(x|y)p(y)\mathrm{d}y\mathrm{d}x\\

&= \int p(y)\left[\int xp(x|y)\mathrm{d}x\right]\mathrm{d}y\\

&= \int p(y)\mathbb{E}_x[x|y]\mathrm{d}y\\

&= \mathbb{E}_y[\mathbb{E}_x[x|y]]

\end{aligned}</script><p>proof $\mathrm{var}[x] =  \mathbb{E}_y[\mathrm{var}_x[x|y]] + \mathrm{var}[\mathbb{E}_x[x|y]]$</p>
<script type="math/tex; mode=display">
\mathrm{var}_x[x|y] = \mathbb{E}[x^2|y] -\mathbb{E}[x|y]^2 \\

\mathbb{E}_y[\mathrm{var}_x[x|y]] = \mathbb{E}_y[\mathbb{E}[x^2|y] -\mathbb{E}[x|y]^2]</script><script type="math/tex; mode=display">
\begin{aligned}

\mathrm{var}[x|y] &= \mathbb{E}[(x-\mathbb{E}[x|y])^2|y] \\

&= \mathbb{E}[(x^2-2\cdot x\mathbb{E}[x|y]+\mathbb{E}[x|y]^2)|y]\\

&= \mathbb{E}[x^2|y] -2\cdot \mathbb{E}[x\mathbb{E}[x|y]|y] + \mathbb{E}[\mathbb{E}[x|y]^2|y]  \\

&= \mathbb{E}[x^2|y]-2\cdot \mathbb{E}[x]\mathbb{E}[x|y] + \mathbb{E}[x|y]^2 \\

\end{aligned}</script><p>notice that $\mathbb{E}[x|y]$ with respect to $y$, therefore</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathrm{var}[x|y] &= \mathbb{E}[x^2|y] -2\mathbb{E}[x|y]^2 + \mathbb{E}[x|y]^2\\

&= \mathbb{E}[x^2|y] - \mathbb{E}[x|y]^2
\end{aligned}</script><p>and we proof the formula of total expectation here</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}_y[\mathbb{E}_x[x|y]] &= \int p(y)\left[\int p(x)p(x|y)\mathrm{d}x)\right]\mathrm{d}y\\

&= \int p(x)\left[\int p(y)p(x|y)\mathrm{d}y\right]\mathrm{d}x\\

&= \int p(x) \mathbb{E}_y\mathrm{d}x\\

&= \mathbb{E}[x]

\end{aligned}</script><p>hence, we have</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}_y[\mathrm{var}_x[x|y]] + \mathrm{var}_y[\mathbb{E}_x[x|y]] &= \int p(y)(\mathbb{E}[x^2|y] - \mathbb{E}[x|y]^2 )\mathrm{d}y + \mathbb{E}_y[x|y]^2 - \mathbb{E}_y[x|y]^2  \\

&= \mathbb{E}_y[\mathbb{E}_x[x^2|y]]-\mathbb{E}_y[\mathbb{E}_x[x|y]^2] + \mathbb{E}_y[\mathbb{E}_x[x|y]^2] - \mathbb{E}_y[\mathbb{E}_x[x|y]]^2  \\

&= \mathbb{E}_y\mathbb{E}_x[x^2|y]-\mathbb{E}_y[\mathbb{E}_x[x|y]^2] + \mathbb{E}_y[x|y]^2 - \mathbb{E}_y\mathbb{E}_x[x|y]^2 \\

&= \mathbb{E}_y\mathbb{E}_x[x^2|y] - \mathbb{E}_y\mathbb{E}_x[x|y]^2\\

&= \mathbb{E}[x^2] - \mathbb{E}[x]^2\\

&= \mathrm{var}[x]

\end{aligned}</script><h2 id="极坐标证明正态分布概率密度函数归一化"><a href="#极坐标证明正态分布概率密度函数归一化" class="headerlink" title="极坐标证明正态分布概率密度函数归一化"></a>极坐标证明正态分布概率密度函数归一化</h2><p>We wanna proof the formula here with another form in polar coordinates $(r, \theta)$.</p>
<script type="math/tex; mode=display">
\int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2)\mathrm{d}x = 1</script><p>In polar coordinates $(r, \theta)$,</p>
<script type="math/tex; mode=display">
\begin{aligned}

I^2 &= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\mathrm{exp}\left(-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2}y^2 \right)\mathrm{d}x\mathrm{d}y \\

&= \int_{0}^{2\pi}\int_0^{+\infty} \mathrm{exp}\left(-\frac{1}{2\sigma^2}r^2\mathrm{cos}^2\theta - \frac{1}{2\sigma^2}r^2\mathrm{sin}^2\theta \right)r\mathrm{d}\theta\mathrm{d}r\\

&= \int_{0}^{2\pi}\int_0^{+\infty} \mathrm{exp} \left(-\frac{1}{2\sigma^2 }r^2  \right) r\mathrm{d}\theta\mathrm{d}r\\

&= -\sigma^2\mathrm{exp}\left(-\frac{1}{2\sigma^2 }r^2\right)\bigg|^{+\infty}_0\\

&= 2\pi\sigma^2

\end{aligned}</script><p>next, we get</p>
<script type="math/tex; mode=display">
I = (2\pi\sigma^2)^{\frac{1}{2}}</script><p>Finally, we proof that the Gaussian distribution is normalized</p>
<script type="math/tex; mode=display">
\begin{aligned}

\int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2) &= \int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}\mathrm{d}(x-\mu) \\

&= \frac{1}{\sqrt{2\pi\sigma^2}}\cdot \sqrt{2\pi \sigma^2}\\

&= 1



\end{aligned}</script><h2 id="QAQ"><a href="#QAQ" class="headerlink" title="QAQ"></a>QAQ</h2><p>univariate Gaussian distribution</p>
<script type="math/tex; mode=display">
\mathcal{N}(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}</script><p>Therefore,</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[x] &= \int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2)x\mathrm{d}x  \\

&= \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\} x\mathrm{d}x  \\

&= \frac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^{+\infty} \mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\} x\mathrm{d}x  \\

&= \frac{1}{\sqrt{2\pi\sigma^2}}\int_{-\infty}^{+\infty} \mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\} (x-\mu + \mu)\mathrm{d}(x-\mu)\\

&= \frac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^{+\infty} \mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\} (x-\mu)\mathrm{d}(x-\mu) + \frac{1}{\sqrt{2\pi\sigma^2}}\int_{-\infty}^{+\infty}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}\mu\mathrm{d}x    \\

&= -\frac{\sqrt{2}\sigma}{\sqrt{\pi}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}\bigg|_{-\infty}^{+\infty} + \frac{\mu}{\sqrt{2\pi\sigma^2}}\int_{-\infty}^{+\infty}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}\mathrm{d}(x-\mu)\\

&= 0 + \mu \times 1 = \mu

\end{aligned}</script><script type="math/tex; mode=display">
\int_{-\infty}^{+\infty} \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}\mathrm{d}x = 1\\

\text{(differentiating both sides of the normalization condition)}\\

\frac{\mathrm{d}}{\mathrm{d}\sigma^2}\int_{-\infty}^{+\infty} \cdots = 0\\

\int_{-\infty}^{+\infty} \left[\frac{1}{-2\sigma^3} + \frac{(x-\mu)^2}{2\sigma^5} \right]\frac{1}{\sqrt{2\pi}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}\mathrm{d}x = 0\\

\int_{-\infty}^{+\infty} \frac{(x-\mu)^2-\sigma^2}{2\sigma^4}\cdot\frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}\mathrm{d}x = 0\\

\int_{-\infty}^{+\infty} -\frac{1}{2\sigma^2}\mathcal{N}(x|\mu, \sigma^2)\mathrm{d}x + \int_{-\infty}^{+\infty} \frac{(x-\mu)^2}{2\sigma^4}\mathcal{N}(x|\mu, \sigma^2)\mathrm{d}x = 0\\

\int_{-\infty}^{+\infty} \frac{(x-\mu)^2}{2\sigma^4}\mathcal{N}(x|\mu, \sigma^2)\mathrm{d}x = \frac{1}{2\sigma^2}\\

\int_{-\infty}^{+\infty} (x-\mu)^2\mathcal{N}(x|\mu, \sigma^2)\mathrm{d}x = \sigma^2\\

\int_{-\infty}^{+\infty} (x^2-2\mu x+\mu^2)\mathcal{N}(x|\mu, \sigma^2)\mathrm{d}x = \sigma^2\\

\int_{-\infty}^{+\infty} x^2\mathcal{N}(x|\mu, \sigma^2)\mathrm{d}x - 2\mu \int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2)x\mathrm{d}x + \mu^2\int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2)\mathrm{d}x = \sigma^2\\

\mathbb{E}[x^2]-2\mu\mathbb{E}[x]+\mu^2\times 1 = \sigma^2\\

\mathbb{E}[x^2] = \mu^2 + \sigma^2</script><blockquote>
<p>莱布尼兹法则: 求解含参函数的积分对参数的导数. 如果函数在闭区间上连续且对参数 $t$ 可微, 求导符号可在积分内或积分外交换位置</p>
</blockquote>
<p>Therefore, we get</p>
<script type="math/tex; mode=display">
\mathbb{E}[x^2] = \int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2)x^2\mathrm{d}x = \mu^2 + \sigma^2\\

\mathbb{E}[x^2] = \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\} x^2\mathrm{d}x\\</script><h2 id="证明多元高斯分布的众数即-mu"><a href="#证明多元高斯分布的众数即-mu" class="headerlink" title="证明多元高斯分布的众数即$\mu$"></a>证明多元高斯分布的众数即$\mu$</h2><p>To find the mode of the Gaussian distribution, we need to find $x$ when probability density is the max. Therefore, we diff the probability density with respect to $x$, and </p>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\mathrm{d}}{\mathrm{d}x}\frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\} &= -\frac{x-\mu}{\sigma^2}\cdot \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}\\

&= \frac{\mu-x}{\sqrt{2\pi}\sigma^3}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}


\end{aligned}</script><p>we want derivative to be 0. Informally, $x = \mu$</p>
<h2 id="验证极大似然估计的结论"><a href="#验证极大似然估计的结论" class="headerlink" title="验证极大似然估计的结论"></a>验证极大似然估计的结论</h2><p>we have log likelihood function here, and we want to verify the formula of $\mu<em>{\mathrm{ML}}$ and  $\sigma^2</em>{\mathrm{ML}}$</p>
<script type="math/tex; mode=display">
\mathrm{ln}p(\mathsf{x}|\mu, \sigma^2) = -\frac{1}{2\sigma^2}\sum^{N}_{n=1}(x_n-\mu)^2 - \frac{N}{2}\mathrm{ln}\sigma^2- \frac{N}{2}\mathrm{ln}(2\pi)\\

\mu_{\mathrm{ML}} = \frac{1}{N}\sum^N_{n=1}x_n\\

\sigma^2_{\mathrm{ML}} = \frac{1}{N}\sum^N_{n=1}(x_n-\mu_{\mathrm{ML}})^2</script><p>we set the derivatives of the log likelihood function with respect to $\mu$</p>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\mathrm{d}}{\mathrm{d}\mu}\mathrm{ln}p(\mathsf{x}|\mu, \sigma^2) &= \frac{1}{\sigma^2}\sum^N_{n=1}(x_n-\mu)    \\

&= 0

\end{aligned}</script><p>hence, we have $\mu<em>{\mathrm{ML}} = \frac{1}{N}\sum^N</em>{n=1}$</p>
<p>Next, we set the derivatives of the log likelihood function with respect to $\sigma^2$</p>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\mathrm{d}}{\mathrm{d}\sigma^2}\mathrm{ln}p(\mathsf{x}|\mu, \sigma^2) &= \frac{1}{2\sigma^4}\sum^N_{n=1}(x_n-\mu_{\mathrm{ML}})^2 - \frac{N}{2\sigma^2}    \\

&= 0

\end{aligned}</script><p>hence, we have $\sigma^2<em>{\mathrm{ML}} = \frac{1}{N}\sum^N</em>{n=1}(x<em>n-\mu</em>{\mathrm{ML}})$</p>
<h2 id="QAQ-1"><a href="#QAQ-1" class="headerlink" title="QAQ"></a>QAQ</h2><p>We have</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[x] &= \int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2)x\mathrm{d}x = \mu  \\

\mathbb{E}[x^2] &= \int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2)x^2\mathrm{d}x = \mu^2+\sigma^2  \\

\end{aligned}</script><p>and we wanna proof</p>
<script type="math/tex; mode=display">
\mathbb{E}[\mu_{\mathrm{ML}}] = \mu\\
\mathbb{E}[\sigma^2_{\mathrm{ML}}] = \left(\frac{N-1}{N}\right)\sigma^2</script><p>If $n=m$, there is $x_n = x_m$. Hence, we have</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[x_nx_m] &= \mathbb{E}[x_n^2]\\

&= \mathbb{E}[x_n]^2 + \mathrm{var}[x_n]\\

&= \mu^2 + \sigma^2

\end{aligned}</script><p>from which we can see that $I_{nm}=1$</p>
<p>If $n\neq m$, $x_n$ and $x_m$ are independent. Hence, we have</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[x_nx_m] &= \mathbb{E}[x_n]\mathbb{E}[x_m]\\

&= \mu^2


\end{aligned}</script><p>from which  we can see that $I_{nm} = 0$</p>
<p>Next we wanna proof $\mathbb{E}[\mu<em>{\mathrm{ML}}] = \mu$ and $\mathbb{E}[\sigma^2</em>{\mathrm{ML}}] = \left(\frac{N-1}{N}\right)\sigma^2$</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[\mu_{\mathrm{ML}}] &= \mathbb{E}\left[\frac{1}{N}\sum^N_{i=1}x_i\right]\\

&= \frac{1}{N}\sum^N_{i=1}\mathbb{E}[x_i]\\

&= \frac{1}{N}\cdot N\mu\\

&= \mu

\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[\sigma^2_{\mathrm{ML}}] &= \mathbb{E}\left[\frac{1}{N}\sum^N_{i=1}\left(x_i-\mu_{\mathrm{ML}}  \right)^2  \right]\\

&= \frac{1}{N}\mathbb{E}\left[\sum_{i=1}^N (x_i^2 - 2\mu_{\mathrm{ML}}x_i + \mu_{\mathrm{ML}}^2)  \right]\\

&= \frac{1}{N}\sum^N_{i=1}\mathbb{E}\left[x_i^2  \right] - 2\frac{1}{N}\sum^N_{i=1}\mathbb{E}[\mu_{\mathrm{ML}}\cdot N\mu_{\mathrm{ML}}] + \mathbb{E}[\mu^2_\mathrm{ML}]\\

&= \mu^2 + \sigma^2 - \mathbb{E}[\mu^2_\mathrm{ML}]\\

&= \mu^2 + \sigma^2 - \mathrm{var}[\mu_{\mathrm{ML}}] - \mathbb{E}[\mu_{\mathrm{ML}}]^2\\


&= \sigma^2 - \mathrm{var}\left[\frac{1}{N}\sum^N_{i=1}x_i  \right]\\

&= \sigma^2 - \frac{1}{N^2}\mathrm{var}\left[\sum^N_{i=1} x_i  \right]\\

&= \sigma^2 - \frac{1}{N^2}\cdot N\sigma^2\\

&= \left(\frac{N-1}{N}\right)\sigma^2






\end{aligned}</script><h2 id="正态分布方差的无偏估计"><a href="#正态分布方差的无偏估计" class="headerlink" title="正态分布方差的无偏估计"></a>正态分布方差的无偏估计</h2><p>We have</p>
<script type="math/tex; mode=display">
\hat{\sigma}^2 = \frac{1}{N}\sum^N_{n=1}(x_n-\mu)^2</script><p>therefore,</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[\hat{\sigma}^2] &= \mathbb{E}\left[\frac{1}{N}\sum^N_{n=1}(x_n-\mu)^2\right]\\

&= \frac{1}{N}\sum^N_{i=1}\mathbb{E}\left[x_i^2-2\mu x_i + \mu^2\right]\\

&= \frac{1}{N}\sum^N_{i=1}\left(\mathbb{E}[x_i^2] - 2\mu\mathbb{E}[x_i] + \mu^2 \right)\\

&= \frac{1}{N}\sum_{i=1}^N \left(\mu^2 + \sigma^2 - 2\mu\cdot\mu + \mu^2 \right)\\

&= \sigma^2

\end{aligned}</script><h2 id="正态分布的极大似然估计"><a href="#正态分布的极大似然估计" class="headerlink" title="正态分布的极大似然估计"></a>正态分布的极大似然估计</h2><script type="math/tex; mode=display">
\mathrm{ln}p(\mathsf{t}|\mathsf{x}, \mathsf{w}, \sigma^2) = -\frac{1}{2\sigma^2}\sum^N_{n=1}\{y(x_n,\mathsf{w})-t_n\}^2 - \frac{N}{2}\mathrm{ln}\sigma^2-\frac{N}{2}\mathrm{ln}(2\pi)</script><p>we set the derivatives of formula here with respect to $\sigma^2$</p>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\mathrm{d}}{\mathrm{d}\sigma^2}\mathrm{ln}p(\mathsf{t}|\mathsf{x}, \mathsf{w}, \sigma^2) &= \frac{1}{2\sigma^4}\sum^N_{n=1}\{y(x_n,\mathsf{w})-t_n\}^2 - \frac{N}{2\sigma^2}  \\

&= 0


\end{aligned}</script><p>therefore, we get $\sigma^2<em>{\mathrm{ML}} = \frac{1}{N}\sum</em>{n=1}^N {y(x<em>n,\mathsf{w}</em>{\mathrm{ML}})-t_n}^2$</p>
<hr>
<h2 id="任何概率密度函数都可以由一个非零固定概率密度函数经单调函数变换得到"><a href="#任何概率密度函数都可以由一个非零固定概率密度函数经单调函数变换得到" class="headerlink" title="任何概率密度函数都可以由一个非零固定概率密度函数经单调函数变换得到"></a>任何概率密度函数都可以由一个非零固定概率密度函数经单调函数变换得到</h2><blockquote>
<p>from Deepseekv2</p>
</blockquote>
<p>We have </p>
<script type="math/tex; mode=display">
\begin{aligned}

p_y(y) &= p_x(x)\left| \frac{\mathrm{d}x}{\mathrm{d}y} \right|\\

&= p_x(g(y))\left| \frac{\mathrm{d}g}{\mathrm{d}y} \right|

\end{aligned}</script><p>Therefore, </p>
<script type="math/tex; mode=display">
\begin{aligned}

p(y) &= p_x(f(x))\left|\frac{\mathrm{d}f}{\mathrm{d}x}  \right|\\

&= p_x(f(x))f'(x)


\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}

p(y) &= q(x)\left|\frac{\mathrm{d}x}{\mathrm{d}y}  \right|\\

&= q(x)\frac{1}{f'(x)}\\

&= q(f^{-1}(y))\frac{1}{f'(f^{-1}(y))}\\

p(f(x)) &= q(x)\frac{1}{f'(x)}

\end{aligned}</script><p>Next we differentiate both sides of the equation with respect to $y$</p>
<script type="math/tex; mode=display">
p'(f(x))f'(x) = q'(x)\frac{1}{f'(x)} - q(x)\frac{1}{[f'(x)]^2}f''(x)\\</script><h2 id="Evaluation-Jacobian-matrix-for-the-transformation"><a href="#Evaluation-Jacobian-matrix-for-the-transformation" class="headerlink" title="Evaluation Jacobian matrix for the transformation"></a>Evaluation Jacobian matrix for the transformation</h2><p>Equation 2.78 and 2.79 is</p>
<script type="math/tex; mode=display">
\begin{aligned}

y_1 &= x_1 + \tanh(5x_1)\\

y_2 &= x_2 + \tanh(5x_2)+\frac{x_1^2}{3}

\end{aligned}</script><p>Therefore, </p>
<script type="math/tex; mode=display">
\mathbf{J} = 

\begin{bmatrix}

6-5\tan ^2 5x_1 & 0\\

\frac{2x_1}{3} & 6-5\tan ^2 5x_2


\end{bmatrix}\\</script><blockquote>
<p>$\tanh x = \frac{\sin x}{\cos x}$, and $(\tanh x)’ = 1-(\tan x)^2$</p>
</blockquote>
<h2 id="熵的数学表达式的证明"><a href="#熵的数学表达式的证明" class="headerlink" title="熵的数学表达式的证明"></a>熵的数学表达式的证明</h2><p>As it is independent identically distributed, we have</p>
<script type="math/tex; mode=display">
\begin{aligned}

h(p\times p) &= h(p) + h(p)\\

&= 2h(p)


\end{aligned}</script><blockquote>
<p>归纳法: </p>
<ol>
<li>证明 $n=1$ 时结论成立</li>
<li>证明 $n=k$ 时结论成立</li>
</ol>
</blockquote>
<p>We notice that it is right when $n=1$ and $n=2$, then we set $n=k$</p>
<script type="math/tex; mode=display">
\begin{aligned}

h(p^k) &= h(p^{k-1}\times p)\\

&= h(p^{k-1}) + h(p) \\

\end{aligned}</script><p>from which we can infer that $h(p^n) = nh(p)$.</p>
<p>We assume $q = p^\frac{n}{m}$, then we have</p>
<script type="math/tex; mode=display">
\begin{aligned}

h(q^m) &= mh(q)\\

&= nh(p)\\

\end{aligned}</script><p>therefore we have $h(p^{\frac{n}{m}}) = \frac{n}{m}h(p)$, in another word,</p>
<script type="math/tex; mode=display">
h(p^x) = xh(p)</script><blockquote>
<p>Here is copied from Deepseekv2</p>
</blockquote>
<p>we set $p = e^u$ and we have $h(e^{ux}) = xh(e^u)$. We assume $H(u) = h(e^u)$, then we have $H(ux) = uH(u)$. We can see that it is satisfied by $H(u) = ku$, here $k$ is a constant. Therefore,</p>
<script type="math/tex; mode=display">
h(p) = h(e^u) = H(u) = ku\Rightarrow h(p) = k\ln p</script><p>from which we show that $h(p)$ must take the form $h(p) \propto \ln p$</p>
<h2 id="拉格朗日乘子法最大化熵"><a href="#拉格朗日乘子法最大化熵" class="headerlink" title="拉格朗日乘子法最大化熵"></a>拉格朗日乘子法最大化熵</h2><p>We define the entropy as </p>
<script type="math/tex; mode=display">
\mathrm{H}[p] = -\sum_i p(x_i)\ln p(x_i)</script><blockquote>
<p>拉格朗日乘子法（Lagrange Multiplier Method）是一种在数学优化问题中寻找多元函数在其变量受到一个或多个约束条件下的局部极值的方法。</p>
<p>假设我们有一个目标函数 $f(x_, x_2, \dots, x_n)$ 和多个约束条件 $g_i(x_1, x_2, \dots, x_n) = 0$, 我们希望找到使 $f$ 取得极值的点, 同时满足这些约束条件.</p>
<p>拉格朗日乘子法引入拉格朗日乘子 $\lambda_i$ 来将约束优化问题转化为无约束优化问题. 定义拉格朗日函数 $\mathcal{L}$ 为:</p>
<script type="math/tex; mode=display">
\mathcal{L}(x_1, \dots, x_n, \lambda_1, \dots, \lambda_n) = f(x_1, \dots, x_n) + \sum_{i=1}^N\lambda_ig_i(x_1, \dots, x_n)</script><p>然后通过求解以下方程组来找到极值点</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}}{\partial x_j} = 0 \quad \text{for}\quad j = 1, 2, \dots, n\\
\frac{\partial \mathcal{L}}{\partial \lambda_i} = 0 \quad \text{for}\quad j = 1, 2, \dots, m\\</script><p>本质上, 拉格朗日乘子法将一个带有约束的优化问题转化为一个无约束的优化问题，并通过求解偏导数来找到极值点。</p>
</blockquote>
<p>with Lagrange multiplier, we have</p>
<script type="math/tex; mode=display">
\begin{aligned}

\tilde{H} &= -\sum_i p(x_i)\ln p(x_i) + \lambda\left(\sum_i p(x_i) - 1  \right)\\

\frac{\partial\tilde{H}}{\partial p(x_i)} &= -\ln p(x_i) - 1 +\lambda  = 0\\

\frac{\partial\tilde{H}}{\partial \lambda} &= \sum_ip(x_i)-1 = 0\\

\end{aligned}</script><p>from which we get</p>
<script type="math/tex; mode=display">
\sum p(x_i) = 1\\
\ln p(x_i) = \lambda-1</script><p>here we have $p(x_i) = e^{\lambda-1}$, therefore</p>
<script type="math/tex; mode=display">
\begin{aligned}

\sum_{i=1}^M p(x_i) &= Me^{\lambda-1}\\

&= 1

\end{aligned}</script><p>hence, $p(x<em>i) = e^{\lambda-1} = \frac{1}{M}$. Finally, we have $\mathrm{H}</em>{\max}[x] = -\sum_{i=1}^M \frac{1}{M}\ln \frac{1}{M} = \ln M$</p>
<h2 id="琴生不等式证明熵的最大值"><a href="#琴生不等式证明熵的最大值" class="headerlink" title="琴生不等式证明熵的最大值"></a>琴生不等式证明熵的最大值</h2><p>We have Jensen’ s inequality in form of following</p>
<script type="math/tex; mode=display">
f\left(\sum_{i=1}^M\lambda_ix_i  \right) \leq \sum_{i=1}^M  \lambda_if(x_i)</script><script type="math/tex; mode=display">
\mathrm{H}[x] = -\sum_{i=1}^M p(x_i)\ln p(x_i)</script><p>With Jensen’ s inequality as respect to function $-\ln x$, we have</p>
<script type="math/tex; mode=display">
\begin{aligned}

-\ln\left( \sum_{i=1}^M p(x_i)p(x_i)  \right) &\leq -\sum^M_{i=1} p(x_i)\ln p(x_i)\\

-\ln \sum_{i=1}^M p^2(x_i) &\leq -\mathrm{H}[x]\\

\mathrm{H}[x] &\leq \ln \sum^M_{i=1}p^2(x_i)\\



\end{aligned}</script><p>As for $\ln \sum_{i=1}^M p^2(x_i)$, we notice that $p(x_i)\leq 1$</p>
<script type="math/tex; mode=display">
\begin{aligned}

\ln \sum_{i=1}^M p^2(x_i) \leq \ln M


\end{aligned}</script><p>hence, we have $\mathrm{H}[x]\leq\ln M$</p>
<h2 id="变分法证明泛函熵最大值时为高斯分布"><a href="#变分法证明泛函熵最大值时为高斯分布" class="headerlink" title="变分法证明泛函熵最大值时为高斯分布"></a>变分法证明泛函熵最大值时为高斯分布</h2><blockquote>
<p>变分法是数学中的一种分析方法，主要研究泛函的极值问题。泛函是函数的函数，即它的输入是函数，输出是一个实数。变分法的核心问题是如何找到使得泛函取得极值的函数。</p>
<p>变分法的基本概念</p>
<ol>
<li><strong>泛函</strong>：泛函 $J$ 是一个映射，它将一个函数空间中的函数 $y(x)$ 映射到一个实数 $J[y]$。</li>
<li><strong>极值问题</strong>：寻找使得泛函 $J[y]$ 取得极值的函数 $y(x)$。</li>
<li><strong>欧拉-拉格朗日方程</strong>：变分法中的关键工具，用于找到使得泛函 $J[y]$ 取得极值的函数 $y(x)$ 满足的微分方程。</li>
</ol>
<p>求解步骤</p>
<ol>
<li><p>定义泛函: 首先需要明确要优化的泛函 $J[y]$，它是一个将函数 $y(x)$ 映射到实数的函数。泛函通常表示为一个积分形式，可能包含 $y(x)$​ 及其导数，例如：</p>
<script type="math/tex; mode=display">
 J(y) = \int_a^b L(x, y(x), y'(x))\mathrm{d}x</script><p> 其中 $L$ 是给定函数 (拉格朗日函数), $y(x)$ 是待求函数</p>
</li>
<li><p>设定边界条件: 确定函数 $y(x)$ 在积分区间的边界上的值, 即 $y(a)$ 和 $y(b)$</p>
</li>
<li><p>应用欧拉-拉格朗日方程: 为了找到使得泛函 $J[y]$ 取得极值的函数 $y(x)$, 需要求解欧拉-拉格朗日方程</p>
<script type="math/tex; mode=display">
 \frac{\partial L}{\partial y}-\frac{\mathrm{d}}{\mathrm{d}x}\left(\frac{\partial L}{\partial y'} \right) = 0</script></li>
<li><p>求解欧拉-拉格朗日方程: 通过求解欧拉-拉格朗日方程, 找到满足该方程的函数 $y(x)$</p>
</li>
<li><p>验证极值条件: 解出函数 $y(x)$ 后, 需要验证它是否确实使泛函 $J[y]$ 取得极值.</p>
</li>
<li><p>应用边界条件: 将边界条件代入解出的函数 $y(x)$ 中, 确定常数或其他参数的值</p>
</li>
</ol>
</blockquote>
<p>Formula 2.96 is</p>
<script type="math/tex; mode=display">
f(x) = -\int ^\infty_{-\infty}p(x) \ln p(x) \mathrm{d}x+\lambda_1\left(\int ^\infty_{-\infty} p(x)\mathrm{d}x-1  \right) + \lambda_2\left(\int ^\infty_{-\infty} xp(x)\mathrm{d}x-\mu  \right) + \lambda_3\left(\int ^\infty_{-\infty} (x-\mu)^2p(x)\mathrm{d}x-\sigma^2  \right)</script><p>therefore, we have</p>
<blockquote>
<p>在微积分中，对于含有积分的表达式进行求导时，我们通常使用莱布尼茨积分法则（Leibniz integral rule），也称为积分换元法或积分求导法则。莱布尼茨积分法则允许我们对含有参数的积分进行求导，前提是积分内的函数和积分限都依赖于该参数。</p>
<p>如果 $F(x, \mu)$ 是一个依赖于参数 $\mu$ 的函数, 且在 $\mu$ 上是可微的, 那么对于积分</p>
<script type="math/tex; mode=display">
I(\mu) = \int_{a(\mu)}^{b(\mu)} F(x, \mu)\mathrm{d}x</script><p>其导数为</p>
<script type="math/tex; mode=display">
\frac{\mathrm{d}I}{\mathrm{d}\mu} = \int _{a(\mu)}^{b(\mu)} \frac{\partial F}{\partial \mu}\mathrm{d}x + F(b(\mu), \mu)\frac{\mathrm{d}b(\mu)}{\mathrm{d}\mu}-F(a(\mu), \mu)\frac{\mathrm{d}a(\mu)}{\mathrm{d}\mu}</script><p>当积分为不定积分时 (没有上下限), $a’(\mu) = 0, b’(\mu) = 0$</p>
<p>例如, 对于以下函数</p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{2\sigma^2}\int p(x)(x-\mu)^2\mathrm{d}x</script><p>我们想要对这个表达式关于 $\mu$ 求导, 其中 $F(x, \mu) = p(x)(x-\mu)^2$, 所以求导结果为</p>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\partial f}{\partial \mu} &= \frac{1}{2\sigma^2}\int \frac{\partial}{\partial \mu}\left(p(x)(x-\mu)^2 \right)\mathrm{d}x\\

&= \frac{1}{2\sigma^2}\int p(x)\frac{\partial}{\partial \mu}\left((x-\mu)^2 \right)\mathrm{d}x\\

&= -\frac{1}{\sigma^2}\int p(x)(x-\mu)\mathrm{d}x


\end{aligned}</script></blockquote>
<script type="math/tex; mode=display">
\frac{\partial f}{\partial p(x)} = -\ln p(x)-1 + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2 = 0\\
p(x) = \mathrm{exp}\left\{-1+\lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2   \right\}</script><p>here we show that the stationary point of the functional (2.96) is given by (2.97).</p>
<p>Similar with exercise 22, </p>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\partial f}{\partial p(x)} &= -\ln p(x)-1 + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2 = 0\\

\frac{\partial f}{\partial \lambda_1} &= \int_{-\infty}^{\infty}p(x)\mathrm{d}x - 1 = 0\\

\frac{\partial f}{\partial \lambda_2} &= \int_{-\infty}^{\infty}xp(x)\mathrm{d}x - \mu = 0\\

\frac{\partial f}{\partial \lambda_3} &= \int_{-\infty}^{\infty} (x-\mu)^2p(x)\mathrm{d}x - \sigma^2 = 0


\end{aligned}</script><p>hence,</p>
<script type="math/tex; mode=display">
\int_{-\infty}^{\infty}\mathrm{exp}\left\{-1+\lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2   \right\}\mathrm{d}x = 1\\

\int_{-\infty}^{\infty}x\mathrm{exp}\left\{-1+\lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2   \right\}\mathrm{d}x = \mu\\

\int_{-\infty}^{\infty}(x-\mu)^2\mathrm{exp}\left\{-1+\lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2   \right\}\mathrm{d}x = \sigma^2</script><blockquote>
<p>计算阵亡</p>
</blockquote>
<h2 id="计算正态分布的微分熵"><a href="#计算正态分布的微分熵" class="headerlink" title="计算正态分布的微分熵"></a>计算正态分布的微分熵</h2><p>We have</p>
<script type="math/tex; mode=display">
\int_{-\infty}^{\infty}xp(x)\mathrm{d}x = \mu\\
\int_{-\infty}^{\infty}(x-\mu)^2p(x)\mathrm{d}x = \sigma^2</script><p>The entropy of the univariate Gaussian is</p>
<script type="math/tex; mode=display">
\begin{aligned}


\mathrm{H}[x] &= -\int p(x)\log p(x)\mathrm{d}x\\

&= -\int \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2} \right\}\log\frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2} \right\}\mathrm{d}x\\

&= -\frac{1}{\sqrt{2\pi\sigma^2}}\int \mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2} \right\}\cdot\left\{ \log\frac{1}{\sqrt{2\pi\sigma^2}}- \left(\frac{(x-\mu)^2}{2\sigma^2}  \right)\right\}\mathrm{d}x\\

&= -\frac{\log\frac{1}{\sqrt{2\pi\sigma^2}}}{\sqrt{2\pi\sigma^2}}\int \mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2} \right\}\mathrm{d}x + \frac{1}{\sqrt{2\pi\sigma^2}}\int \mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2} \right\}\frac{(x-\mu)^2}{2\sigma^2}\mathrm{d}x \\

&= \log \sqrt{2\pi\sigma^2} + \frac{1}{2}\\

&= \frac{1}{2}\left\{1+\ln(2\pi\sigma^2) \right\}



\end{aligned}</script><h2 id="证明两分布相同时-KL-散度最小"><a href="#证明两分布相同时-KL-散度最小" class="headerlink" title="证明两分布相同时 KL 散度最小"></a>证明两分布相同时 KL 散度最小</h2><p>We have</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathrm{KL}(p\|q) &= \int p(x)\log \frac{p(x)}{q(x)}\mathrm{d}x\\

&= \int p(x)\log p(x)\mathrm{d}x - \int p(x)\log \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2} \right\}\mathrm{d}x\\

&= \mathrm{H}[x]-\int p(x)\left\{\log \frac{1}{\sqrt{2\pi\sigma^2}} - \frac{(x-\mu)^2}{2\sigma^2} \right\}\mathrm{d}x\\

&= \mathrm{H}[x] + \log\sqrt{2\pi\sigma^2} + \frac{1}{2\sigma^2}\int p(x)(x-\mu)^2\mathrm{d}x\\

\end{aligned}</script><p>therefore,</p>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\partial \mathrm{KL}(p\| q)}{\partial \mu} &= \frac{1}{2\sigma^2}\int \frac{\partial}{\partial \mu}\left(p(x)(x-\mu)^2 \right)\mathrm{d}x\\

&= \frac{1}{2\sigma^2}\int p(x)\frac{\partial}{\partial \mu}\left((x-\mu)^2 \right)\mathrm{d}x\\

&= -\frac{1}{\sigma^2}\int p(x)(x-\mu)\mathrm{d}x = 0\\

\end{aligned}</script><p>hence, we have $\mu = \mathbb{E}[x]$. Similarly,</p>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\partial \mathrm{KL}(p\| q)}{\partial \sigma^2} &= \frac{1}{2\sigma^2} - \frac{1}{2\sigma^4} \int p(x)(x-\mu)^2\mathrm{d}x\\

&= 0



\end{aligned}</script><p>and we get $\Sigma = \sigma^2 = \mathrm{var}[x]$</p>
<h2 id="计算两个高斯分布间的-KL-散度"><a href="#计算两个高斯分布间的-KL-散度" class="headerlink" title="计算两个高斯分布间的 KL 散度"></a>计算两个高斯分布间的 KL 散度</h2><script type="math/tex; mode=display">
\begin{aligned}

\mathrm{KL}(p\|q) &= \int p(x) \log \frac{p(x)}{q(x)}\mathrm{d}x\\

&= \int \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2} \right\}\log \frac{s\cdot\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2}  \right\}}{\sigma\cdot\mathrm{exp}\left\{-\frac{(x-m)^2}{2s^2}  \right\}}\mathrm{d}x\\

&= \int \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left\{-\frac{(x-\mu)^2}{2\sigma^2} \right\}\left(\log\frac{s}{\sigma} + \frac{(x-m)^2}{2s^2} - \frac{(x-\mu)^2}{2\sigma^2} \right)\mathrm{d}x\\

&= \log\frac{s}{\sigma} + \frac{1}{2s^2}\int p(x)(x-m)^2\mathrm{d}x - \frac{1}{2\sigma^2}\int p(x)(x-\mu)^2\mathrm{d}x\\

&= \log\frac{s}{\sigma} + \frac{1}{2s^2}(\mu^2+\sigma^2 - 2m+m^2) - \frac{1}{2}\\

\end{aligned}</script><h2 id="证明阿尔法族偏差与-KL-散度的关系"><a href="#证明阿尔法族偏差与-KL-散度的关系" class="headerlink" title="证明阿尔法族偏差与 KL 散度的关系"></a>证明阿尔法族偏差与 KL 散度的关系</h2><p>We have alpha family of divergences</p>
<script type="math/tex; mode=display">
\mathrm{D}_\alpha(p\|q) = \frac{4}{1-\alpha^2}\left(1-\int p(x)^{\frac{1+\alpha}{2}}q(x)^{\frac{1-\alpha}{2}}\mathrm{d}x  \right), -\infty<\alpha<\infty</script><p>As instruction said,</p>
<script type="math/tex; mode=display">
\begin{aligned}

\lim_{\alpha\rightarrow 1}\mathrm{D}_{\alpha}(p\|q) &= \lim_{\alpha\rightarrow 1} \frac{4}{1-\alpha^2}\left(1-\int p(x)^{\frac{1+\alpha}{2}}q(x)^{\frac{1-\alpha}{2}}\mathrm{d}x  \right)\\

&= \lim_{\alpha\rightarrow 1} \frac{4}{1-\alpha^2}\left(1-\int p(x)\mathrm{d}x  \right)\\

&= \lim_{\alpha\rightarrow 1}

\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}

\mathrm{D}_\alpha(p\|q) &= \frac{4}{1-\alpha^2}\left(1-\int p(x)^{\frac{1+\alpha}{2}}q(x)^{\frac{1-\alpha}{2}}\mathrm{d}x  \right)\\

&= \frac{2}{1+\alpha}\frac{2}{1-\alpha}\left(1-\int p(x)\left(\frac{q(x)}{p(x)}^{\frac{1-\alpha}{2}}  \right)\mathrm{d}x  \right)\\

&= \frac{2}{1+\alpha}\frac{2}{1-\alpha}\int p(x)\left(1-\left(\frac{q(x)}{p(x)} \right)^{\frac{1-\alpha}{2}}\mathrm{d}x    \right)\\

\end{aligned}</script><p>therefore, we have</p>
<script type="math/tex; mode=display">
\lim_{\alpha\rightarrow1}\mathrm{D}_\alpha (p\|q) = \lim_{\alpha\rightarrow1} 2 \int p(x) \frac{1-\left(\frac{q(x)}{p(x)} \right)^{\frac{1-\alpha}{2}}}{1-\alpha}\mathrm{d}x</script><p>here we assume $\frac{q(x)}{p(x)}$ as $y$, $1-\alpha$ as $\beta$, $\sqrt{y}$ as $t$. hence, what we need to do is to prove the following formula</p>
<script type="math/tex; mode=display">
\lim_{\alpha\rightarrow 1} \frac{1-y^{\frac{1-\alpha}{2}}}{1-\alpha} = \frac{1}{2}\ln y</script><p>and here is the proof</p>
<script type="math/tex; mode=display">
\begin{aligned}

\lim_{\alpha\rightarrow 1} \frac{1-y^{\frac{1-\alpha}{2}}}{1-\alpha} &= \lim_{\beta\rightarrow 0}\frac{1-\sqrt{y}^\beta}{\beta}\\

&= \lim_{\beta\rightarrow0}\frac{1-t^\beta}{\beta}\\

&= \lim_{\beta\rightarrow 0}\frac{\beta\ln t}{\beta}\\

&= \ln t = \frac{1}{2}\ln y


\end{aligned}</script><blockquote>
<p>$x\rightarrow 0$ 时, $\ln (1+x)\sim x$, $1-a^x\sim x\ln a$</p>
</blockquote>
<p>$\mathrm{KL}(q| p)$ corresponding to $\alpha\rightarrow -1$ is right oppsite to the steps above.</p>
<h2 id="联合分布的微分熵小于两个变量各自熵之和"><a href="#联合分布的微分熵小于两个变量各自熵之和" class="headerlink" title="联合分布的微分熵小于两个变量各自熵之和"></a>联合分布的微分熵小于两个变量各自熵之和</h2><script type="math/tex; mode=display">
\begin{aligned}

\mathrm{H}[x, y]-\mathrm{H}[x]-\mathrm{H}[y] &= -\int \int p(x, y)\log p(x, y)\mathrm{d}x\mathrm{d}y +\int p(x)\log p(x)\mathrm{d}x + \int p(y)\log p(y)\mathrm{d}y      \\

&= \mathrm{H}[x] + \mathrm{H}[y|x] - \mathrm{H}[x]-\mathrm{H}[y]\\

&= \mathrm{H}[y|x]  - \mathrm{H}[y]\\

&\leq 0


\end{aligned}</script><p>therefore, we have $\mathrm{H}[x, y]\leq \mathrm{H}[x]+\mathrm{H}[y]$, with equlity if, and only if , $x$ and $y$ are statistically independent.</p>
<h2 id="熵的非奇异线性变化"><a href="#熵的非奇异线性变化" class="headerlink" title="熵的非奇异线性变化"></a>熵的非奇异线性变化</h2><blockquote>
<p>连续随机变量 $x$ 经过线性变换 $y=Ax$ 后, 新的随机变量 $y$ 的概率密度函数 $p(y)$ 可以通过变量变换公式得到, 如果$A$ 是一个非奇异矩阵</p>
<script type="math/tex; mode=display">
p(y) = p(x)|\det A|^{-1}</script><p>重积分换元法，也称为多重积分的变量替换或坐标变换，是单变量积分中的换元积分法在多维空间中的推广。在多重积分中，换元法用于简化积分计算，特别是在处理复杂区域或复杂被积函数时。</p>
<script type="math/tex; mode=display">
\int\int_D f(x, y)\mathrm{d}x\mathrm{d}y = \int\int_{D'}f(x(u, v), y(u, v))\left| \frac{\partial(x, y)}{\partial (u, v)} \right| \mathrm{d}u \mathrm{d}v</script><p>这里偏导求得是雅可比矩阵</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{aligned}

\mathrm{H}[\mathbf{y}] &= \mathrm{H}[\mathbf{Ax}]\\

&= -\int p(\mathbf{Ax})\ln p(\mathbf{Ax})\mathrm{d}\mathbf{Ax}\\

&= -|\det \mathbf{A}|\int p(\mathbf{x})|\det\mathbf{A}|^{-1}\ln p(\mathbf{x})|\det\mathbf{A}|^{-1}\mathrm{d}\mathbf{x}\\


&= -|\det \mathbf{A}|\int p(\mathbf{x})|\det \mathbf{A}|^{-1}\left(\ln p(\mathbf{x})- \ln |\det\mathbf A|\right)\mathrm{d}x\\

&= \mathrm{H}[\mathbf{x}] + \ln |\det \mathbf{A}|



\end{aligned}</script><hr>
<h2 id="条件熵为-0-时条件概率的特点"><a href="#条件熵为-0-时条件概率的特点" class="headerlink" title="条件熵为 0 时条件概率的特点"></a>条件熵为 0 时条件概率的特点</h2><script type="math/tex; mode=display">
\begin{aligned}

\mathrm{H}[y|x] &= -\sum_x\sum_y p(x, y)\log p(y|x)\mathrm{d}x\mathrm{d}y = 0

\end{aligned}</script><p>which means $\log p(y|x) = 0$, therefore $p(y|x) = 1$. This indicates that for each $x$ there is only one value of $y$ such that $p(y|x)\neq 0$</p>
<h2 id="证明凸函数的二阶导数为正"><a href="#证明凸函数的二阶导数为正" class="headerlink" title="证明凸函数的二阶导数为正"></a>证明凸函数的二阶导数为正</h2><blockquote>
<p>learnt from others</p>
</blockquote>
<p>Assume that $\forall x_1&lt;x_2, 0&lt;\alpha&lt;1$ and</p>
<script type="math/tex; mode=display">
f(\alpha x_1+(1-\alpha)x_2)\leq \alpha f(x_1)+(1-\alpha)f(x_2)</script><p>Therefore</p>
<script type="math/tex; mode=display">
\frac{1}{2}f(x) + \frac{1}{2}f(x+2\Delta x) \geq f\left(\frac{1}{2}x + \frac{1}{2}(x+2\Delta x)\right) =   f(x+\Delta x)</script><p>Hence, </p>
<blockquote>
<p>拉格朗日中值定理</p>
<script type="math/tex; mode=display">
f'(c) = \frac{f(b)-f(a)}{b-a}</script></blockquote>
<script type="math/tex; mode=display">
\begin{aligned}

f(x+2\Delta x) - f(x+\Delta x) &\geq f(x+\Delta x) - f(x)\\

f'(\xi_1)\Delta x &\geq f'(\xi_2)\Delta x\\

f'(\xi_1)&\geq f'(\xi_2)

\end{aligned}</script><p>here $\xi_1\in (x+\Delta x, x+2\Delta x)$, $\xi_2\in(x, x+\Delta x)$. And we have $f’’(\xi_3)\geq 0, \xi_3\in (\xi_2, \xi_1)$. And we have</p>
<script type="math/tex; mode=display">
\lim_{\Delta x\rightarrow0}f''(\xi_3) = f''(x)\geq 0</script><blockquote>
<p>保号性: 函数在某点的极限存在, 那么该函数在该点附近符号与该点极限值符号相同</p>
</blockquote>
<p>Next we wanna proof in reverse. Assume that we have $f’’(x)\geq 0$, we set $\forall \alpha\in[0, 1]$, $\forall x&gt;x_0$,</p>
<script type="math/tex; mode=display">
\begin{aligned}

F(x) &= f\left(\alpha x+(1-\alpha)x_0\right) - \alpha f(x) - (1-\alpha)f(x_0) \\

F'(x) &= \alpha f'(\alpha x+(1-\alpha)x_0) - \alpha f'(x)\\

&= \alpha\left(f'(\alpha x+(1-\alpha)x_0)-f'(x)\right) 


\end{aligned}</script><p>As $f’’(x)\geq 0$ and $\alpha x+(1-\alpha )x_1=x+(1-\alpha)(x_1-x)\leq x$. Hence, we have $F’(x)\leq 0$</p>
<script type="math/tex; mode=display">
\begin{aligned}

F(x)&\leq F(x_0)\\

&= 0

\end{aligned}</script><p>in another word, it’ s</p>
<script type="math/tex; mode=display">
f\left(\alpha x+(1-\alpha)x_0\right) \leq \alpha f(x) + (1-\alpha)f(x_0)</script><h2 id="归纳法证明凸函数不等式蕴含琴生不等式"><a href="#归纳法证明凸函数不等式蕴含琴生不等式" class="headerlink" title="归纳法证明凸函数不等式蕴含琴生不等式"></a>归纳法证明凸函数不等式蕴含琴生不等式</h2><p>Here formula 2.101 is</p>
<script type="math/tex; mode=display">
f(\lambda a+(1-\lambda)b)\leq \lambda f(a) + (1-\lambda)f(b)</script><p>it can be written into</p>
<script type="math/tex; mode=display">
f(\lambda x_1+(1-\lambda)x_2)\leq \lambda f(x_1)+(1-\lambda)f(x_2)</script><p>Formula 2.102 is Jensen’s inequality</p>
<script type="math/tex; mode=display">
f\left(\sum_{i=1}^M \lambda_i x_i \right)\le\sum_{i=1}^M \lambda_i f(x_i)</script><p>If $M=2$, assume that we have $f(\sum<em>i^M\lambda_i x_i)\leq\sum</em>{i=1}^M\lambda<em>i f(x_i)$, here $\sum</em>{i=1}^M\lambda_i = 1$. It is obvious that formular 2.101 holds for $M=2$</p>
<p>If $M=k$, assume that we have $f(\sum<em>i^M\lambda_i x_i)\leq\sum</em>{i=1}^M\lambda<em>i f(x_i)$, here $\sum</em>{i=1}^M\lambda_i = 1$</p>
<p>If $M=k+1$, assume that we have</p>
<script type="math/tex; mode=display">
f(\sum_{i=1}^{M-1}\lambda_i x_i + \lambda_Mx_M)\leq\sum_{i=1}^M\lambda_i f(x_i)</script><p>Next we assume $\sum<em>{i=1}^{M-1}\lambda_i x_i = (1-\lambda_M)x_0 = \left(\sum</em>{i=1}^{M-1}\lambda_i\right) x_0$, hence,</p>
<script type="math/tex; mode=display">
\begin{aligned}

\sum_{i=1}^M\lambda_i f(x_i) &= f\left(\sum_{i=1}^{M-1}\lambda_i x_i + \lambda_Mx_M\right)\\

&\leq (1-\lambda_M)f(x_0) + \lambda_M f(x_M)\\

\end{aligned}</script><p>and we have $x<em>0 = \frac{\sum</em>{i=1}^{M-1}\lambda<em>ix_i} {\sum</em>{i=1}^{M-1}\lambda<em>i}$. Here we assume $\sum</em>{i=1}^{M-1}\lambda_i$ as $t$. Therefore,</p>
<script type="math/tex; mode=display">
\begin{aligned}

(1-\lambda_M)f(x_0) + \lambda_M f(x_M) &= t\cdot f\left(\sum_{i=1}^{M-1} \frac{\lambda_ix_i}{t}   \right) + \lambda_M f(x_M)\\

&\leq t\cdot \sum_{i=1}^{M-1}\frac{\lambda_i}{t} f(x_i) + \lambda_M f(x_M)\\

&= \sum_{i=1}^{M}\lambda_if(x_i)


\end{aligned}</script><p>hence, we show that the inequality 2.101 for convex functions implies the result 2.102</p>
<h2 id="经验分布与真实分布的-KL-散度等于负对数似然函数"><a href="#经验分布与真实分布的-KL-散度等于负对数似然函数" class="headerlink" title="经验分布与真实分布的 KL 散度等于负对数似然函数"></a>经验分布与真实分布的 KL 散度等于负对数似然函数</h2><p>Formula 2.37 is</p>
<script type="math/tex; mode=display">
p(x|\mathcal{D}) = \frac{1}{N}\sum_{n=1}^N\delta(x-x_n)</script><p>therefore,</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathrm{KL}(p\|q) &= \int p(x)\ln \left\{\frac{p(x)}{q(x)} \right\}\mathrm{d}x\\


&= \int \frac{1}{N}\sum_{n=1}^N\delta(x-x_n)\ln\left\{\frac{\frac{1}{N}\sum_{n=1}^N\delta(x-x_n)} {q(x|\theta)} \right\}\\

&= \frac{1}{N} \sum_{i=1}^N\ln\frac{\frac{1}{N}} { q(x|\theta)}\mathrm{d}x\\

&= -\frac{1}{N}\sum_{i=1}^N\ln q(x|\theta) - \ln N

\end{aligned}</script><h2 id="证明条件熵的性质"><a href="#证明条件熵的性质" class="headerlink" title="证明条件熵的性质"></a>证明条件熵的性质</h2><p>Here formula 2.107 is</p>
<script type="math/tex; mode=display">
\mathrm{H}[y|x] = -\int \int p(x, y)\ln p(y|x)\mathrm{d}y \mathrm{d}x</script><p>we have</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathrm{H}[y|x] + \mathrm{H}[x] &= -\int\int p(x, y)\ln p(y|x)\mathrm{d}x\mathrm{d}y - \int p(x)\ln p(x)\mathrm{d}x\\

&= -\int\int p(x, y)\ln \frac{p(x, y)}{p(x)}\mathrm{d}x\mathrm{d}y - \int   \left(\int p(x, y)\mathrm{d}y\right)\ln p(x)\mathrm{d}x\\

&= -\int\int p(x, y)\ln \frac{p(x, y)}{p(x)}\mathrm{d}x\mathrm{d}y - \int \int p(x, y)\ln p(x)\mathrm{d}x\\

&= -\int\int p(x, y)\ln p(x, y)\mathrm{d}x\mathrm{d}y\\

&= \mathrm{H}[x, y]


\end{aligned}</script><h2 id="计算二元变量的各种熵"><a href="#计算二元变量的各种熵" class="headerlink" title="计算二元变量的各种熵"></a>计算二元变量的各种熵</h2><ol>
<li><p>$\mathrm{H}[x]$<br> $$<br> \begin{aligned}</p>
<p> \mathrm{H}[x] &amp;= -p(x=0)\ln p(x=0) - -p(x=1)\ln p(x=1) \</p>
<p> &amp;= -\frac{2}{3}\times\ln \frac{2}{3} - \frac{1}{3}\times\ln \frac{1}{3}\</p>
<p> &amp;= \ln 3 - \frac{2}{3}\ln2</p>
</li>
</ol>
<pre><code>\end&#123;aligned&#125;
$$
</code></pre><ol>
<li><p>$\mathrm{H}[y]$</p>
<script type="math/tex; mode=display">
 \begin{aligned}

 \mathrm{H}[y] &= -p(y=0)\ln p(y=0) - -p(y=1)\ln p(y=1)\\

 &= -\frac{2}{3}\times\ln \frac{2}{3} - \frac{1}{3}\times\ln \frac{1}{3}\\

 &= \ln 3 - \frac{2}{3}\ln2

 \end{aligned}</script></li>
<li><p>$\mathrm{H}[y|x]$</p>
<script type="math/tex; mode=display">
 \begin{aligned}

 \mathrm{H}[y|x=0] &= -p(y=0|x=0)\ln p(y=0|x=0)
  - p(y=1|x=0)\ln p(y=1|x=0)\\

  &= -\frac{1}{2}\ln \frac{1}{2} - \frac{1}{2}\ln \frac{1}{2}\\

  &= \ln 2\\

 \mathrm{H}[y|x=1] &= -p(y=0|x=1)\ln p(y=0|x=1)
  - p(y=1|x=1)\ln p(y=1|x=1)\\

  &= 0\\

 \mathrm{H}[y|x] &= p(x=0)\times\mathrm{H}[y|x=0] + p(x=1)\times\mathrm{H}[y|x=1]\\

 &= \frac{2}{3}\ln 2

 \end{aligned}</script></li>
<li><p>$\mathrm{H}[x|y]$</p>
<script type="math/tex; mode=display">
 \begin{aligned}

 \mathrm{H}[x|y=0] &= -p(x=0|y=0)\ln p(x=0|y=0) - p(x=1|y=0)\ln p(x=1|y=0)\\

 &= 0\\

 \mathrm{H}[x|y=1] &= -p(x=0|y=1)\ln p(x=0|y=1) - p(x=1|y=1)\ln p(x=1|y=1)\\

 &= -\frac{1}{2}\ln \frac{1}{2} - \frac{1}{2}\ln \frac{1}{2}\\

 &= \ln 2\\

 \mathrm{H}[x|y] &= p(y=0)\mathrm{H}[x|y=0] + p(y=1)\mathrm{H}[x|y=1]\\

 &= \frac{2}{3}\ln 2

 \end{aligned}</script></li>
<li><p>$\mathrm{H}[x, y]$</p>
<script type="math/tex; mode=display">
 \begin{aligned}

 \mathrm{H}[x, y] &= -p(x=0, y=0)\ln p(x=0, y=0) - p(x=0, y=1)\ln p(x=0, y=1) - p(x=1, y=0)\ln p(x=1, y=0) - p(x=1, y=1)\ln p(x=1, y=1)     \\

 &= -\frac{1}{3}\ln \frac{1}{3} -\frac{1}{3}\ln \frac{1}{3} -\frac{1}{3}\ln \frac{1}{3}\\

 &= \ln 3

 \end{aligned}</script></li>
<li><p>$\mathrm{I}[x, y]$</p>
<script type="math/tex; mode=display">
 \begin{aligned}

 \mathrm{I}[x, y] &= \mathrm{H}[y] - \mathrm{H}[y|x]\\

 &= \ln 3 - \frac{2}{3}\ln 2 - \frac{2}{3}\ln 2\\

 &= \ln 3 - \frac{4}{3}\ln2

 \end{aligned}</script></li>
</ol>
<h2 id="琴生不等式证明一组实数的算术平均数大于等于其几何平均数"><a href="#琴生不等式证明一组实数的算术平均数大于等于其几何平均数" class="headerlink" title="琴生不等式证明一组实数的算术平均数大于等于其几何平均数"></a>琴生不等式证明一组实数的算术平均数大于等于其几何平均数</h2><blockquote>
<p>算术平均数</p>
<script type="math/tex; mode=display">
\overline{x} = \frac{1}{N}\sum_{i=1}^N x_i</script><p>几何平均数</p>
<script type="math/tex; mode=display">
\overline{x} = \left(\prod_{i=1}^N x_i\right)^{\frac{1}{N}}</script></blockquote>
<p>Here is the Jensen’s inequality</p>
<script type="math/tex; mode=display">
f\left(\sum_{i=1}^N \lambda_ix_i \right)\leq \sum_{i=1}^N \lambda_i f(x_i)</script><p>therefore,</p>
<script type="math/tex; mode=display">
\begin{aligned}

f\left(\left(\prod_{i=1}^N x_i\right)^{\frac{1}{N}} \right) &= \ln \left(\prod_{i=1}^N x_i\right)^{\frac{1}{N}}\\

&= \frac{1}{N}\ln \left(\prod_{i=1}^N x_i\right)\\

&= \frac{1}{N}\sum_{i=1}^N\ln x_i\\

&\leq 

\end{aligned}</script><p>Since $f(x)=\ln x$ is a concave function, we have</p>
<script type="math/tex; mode=display">
\begin{aligned}

f\left(\frac{1}{N}\sum_{i=1}^N x_i \right) &\geq \frac{1}{N}\sum_{i=1}^N f(x_i)\\

\ln \left(\frac{1}{N}\sum_{i=1}^N x_i \right) &\geq \frac{1}{N}\sum_{i=1}^N \ln x_i = \frac{1}{N}\left(\ln \prod_{i=1}^N x_i\right)\\

\end{aligned}</script><p>As $f(x)=\ln x$ and $g(x)=x$ are monotonically increasing function, we have</p>
<script type="math/tex; mode=display">
\frac{1}{N}\sum_{i=1}^N x_i\geq \left(\prod_{i=1}^N x_i\right)^{\frac{1}{N}}</script><h2 id="证明互信息的公式"><a href="#证明互信息的公式" class="headerlink" title="证明互信息的公式"></a>证明互信息的公式</h2><script type="math/tex; mode=display">
\begin{aligned}

\mathrm{I}[x, y] &= \mathrm{H}[x] - \mathrm{H}[x|y]\\

&= -\int p(x)\ln p(x)\mathrm{d}x - \int\int p(x, y)\ln p(x|y)\mathrm{d}x\mathrm{d}y\\

&= -\int \left(\int p(x, y)\mathrm{d}y \right)\ln p(x)\mathrm{d}x - \int\int p(x, y)\ln p(x|y)\mathrm{d}x\mathrm{d}y\\

&= -\int \int p(x, y)\ln p(x)\mathrm{d}x\mathrm{d}y - \int\int p(x, y)\ln \frac{p(x,y)}{p(y)}\mathrm{d}x\mathrm{d}y\\

&= -\int\int p(x, y)\ln \left(\frac{p(x)p(y)}{p(x, y)} \right)\mathrm{d}x\mathrm{d}y

\end{aligned}</script><h2 id="独立是两变量不相关的充分条件-不相关不是独立的充分条件"><a href="#独立是两变量不相关的充分条件-不相关不是独立的充分条件" class="headerlink" title="独立是两变量不相关的充分条件, 不相关不是独立的充分条件"></a>独立是两变量不相关的充分条件, 不相关不是独立的充分条件</h2><blockquote>
<p>协方差矩阵, 用以描述一组随机变量间的线性关系. 协方差矩阵是对称矩阵, 每个元素表示两个随机变量间的协方差. 协方差的计算公式为</p>
<script type="math/tex; mode=display">
\sigma_{ij} = \mathrm{Cov}(x_i, x_j) = \mathbb{E}[(x_i-\mu_i)(x_j-\mu_j)]</script></blockquote>
<p>Assume $i\neq j$,</p>
<script type="math/tex; mode=display">
\begin{aligned}

\sigma_{ij} &= \mathbb{E}[(x_i-\mu_i)(x_j-\mu_j)]\\

&= \mathbb{E}[x_ix_j-\mu_ix_j-\mu_jx_i+\mu_i\mu_j]\\

&= \mathbb{E}[x_ix_j] - \mu_i\mathbb{E}[x_j] - \mu_j\mathbb{E}[x_i] + \mu_i\mu_j\\

&= \mu_i\mu_j-\mu_i\mu_j-\mu_i\mu_j+\mu_i\mu_j\\

&= 0

\end{aligned}</script><p>therefore, the covariance matrix is either a diagonal matrix or a zero matrix.</p>
<p>Next, we assume $y_1 = y$</p>
<script type="math/tex; mode=display">
\begin{aligned}

p(y_2|y_1) &= p(y_1^2|y_1)

\end{aligned}</script><p>Obviously, it is dependent on $y_1$. $y_1$ and $y_2$ are not independent. We have $\mathbb{E}[y_1]=0$, $\mathbb{E}[y_2] = 0$</p>
<script type="math/tex; mode=display">
\begin{aligned}

\sigma &= \mathbb{E}[(y_1-\mu_1)(y_2-\mu_2)]\\

&= \mathbb{E}[y_1y_2-\mu_1y_2-\mu_2y_1+\mu_1\mu_2]\\

&= \mathbb{E}[y_1y_2] - \mu_1\mathbb{E}[y_2] - \mu_2\mathbb{E}[y_1] + \mu_1\mu_2\\

&= \mathbb{E}[y_1^3]\\

&= 0

\end{aligned}</script><p>therefore, the covariance matrix is either a diagonal matrix or a zero matrix.</p>
<h2 id="贝叶斯定理证明硬币例题"><a href="#贝叶斯定理证明硬币例题" class="headerlink" title="贝叶斯定理证明硬币例题"></a>贝叶斯定理证明硬币例题</h2><p>Bayes’ theorem</p>
<script type="math/tex; mode=display">
p(\mathbf{w}|\mathcal{D}) = \frac{p(\mathcal{D}|\mathbf{w})p(\mathbf{w})}{p(\mathcal{D})}</script><p>We denote $A$ as the convex side of the coin being heads, here $p(A)=0.1$, $B$ as the oppsite, $D$ as the fact, $p_0=0.8$ as the probability of heads after flipping, $p_1=0.2$ as the oppsite. We wanna know $p(B|D)$</p>
<script type="math/tex; mode=display">
\begin{aligned}

p(B|D) &= \frac{p(D|B)p(B)}{p(D)}\\

&= \frac{p(D|B)p(B)}{p(D|B)p(B) + p(D|A)p(A)}

\end{aligned}</script><p>we have</p>
<script type="math/tex; mode=display">
p(D|A) = p_0^8 \times p_1^2\\
p(D|B) = p_1^8 \times p_0^2</script><p>hence,</p>
<script type="math/tex; mode=display">
\begin{aligned}

p(B|D) &= \frac{p_1^8 \times p_0^2 \times 0.9}{p_1^8 \times p_0^2\times 0.9 + p_0^8 \times p_1^2\times 0.1}\\

&= \frac{0.00000147456}{0.00000147456 + 0.00067108864} = \frac{0.00000147456}{0.0006725632}\\

&\approx 0.002192\\

\end{aligned}</script><p>therefore, the posterior probability of coin heading up is $0.002192$.</p>
<p>Similarly, we get $p(A|D) = 0.997808$. Finally, </p>
<script type="math/tex; mode=display">
\begin{aligned}

p &= p_0p(A|D) + p_1p(B|D)\\

&= 0.8\times 0.997808 + 0.2\times 0.002192\\

&= 0.7982464 + 0.0004384\\

&= 0.7986848\\

&\approx 0.7987


\end{aligned}</script><p>the probability that the next flip will land heads up is $0.7987$</p>
<h2 id="推导正则化误差函数的结果"><a href="#推导正则化误差函数的结果" class="headerlink" title="推导正则化误差函数的结果"></a>推导正则化误差函数的结果</h2><p>Formula 2.114 is</p>
<script type="math/tex; mode=display">
-\ln p(\mathbf{w}|\mathcal{D}) = -\ln  p(\mathcal{D}|\mathbf{w}) - \ln p(\mathbf{w}) + \ln p(\mathcal{D})</script><p>Formula 2.115 is</p>
<script type="math/tex; mode=display">
p(\mathbf{w}|s) = \prod_{i=0}^M\mathcal{N}(w_i|0, s^2) = \prod_{i=0}^{M}\left(\frac{1}{2\pi s^2} \right)^{\frac{1}{2}}\exp \left\{-\frac{w_i^2}{2s^2} \right\}</script><p>Formula 2.117 is</p>
<script type="math/tex; mode=display">
E(\mathbf{w}) = \frac{1}{2\sigma^2}\sum_{n=1}^N \left\{y(x_n, \mathbf{w})-t_n \right\}^2 + \frac{1}{2s^2}\mathbf{w}^\mathrm{T}\mathbf{w}</script><p>Formula 2.66 is</p>
<script type="math/tex; mode=display">
\ln p(\mathsf{t}|\mathsf{x}, \mathbf{w}, \sigma^2) = -\frac{1}{2\sigma^2}\sum_{n=1}^N\left\{y(x_n, \mathbf{w})-t_n \right\}^2-\frac{N}{2}\ln \sigma^2 -\frac{N}{2}\ln(2\pi)</script><p>Here we have</p>
<script type="math/tex; mode=display">
\begin{aligned}

-\ln p(\mathbf{w}|s) &= \frac{N}{2}\ln(2\pi s^2)+\sum_{i=1}^N \frac{w_i^2}{2s^2}\\

&= \frac{N}{2}\ln(2\pi s^2)+\frac{1}{2s^2}\sum_{i=1}^N w_i^2\\


\end{aligned}</script><p>As instructed, we substituting formula 2.115 and 2.66 into 2.114</p>
<script type="math/tex; mode=display">
\begin{aligned}

-\ln p(\mathbf{w}|\mathcal{D}) &= -\ln  p(\mathcal{D}|\mathbf{w}) - \ln p(\mathbf{w}) + \ln p(\mathcal{D})\\

&= \frac{1}{2\sigma^2}\sum_{n=1}^N\left\{y(x_n, \mathbf{w})-t_n \right\}^2+\frac{N}{2}\ln \sigma^2 +\frac{N}{2}\ln(2\pi) + \frac{N}{2}\ln(2\pi s^2)+\frac{1}{2s^2}\sum_{i=1}^N w_i^2 + \ln p(\mathcal{D})

\end{aligned}</script><blockquote>
<p>在贝叶斯统计中，<script type="math/tex">\ln⁡ p(\mathcal{D})</script> 表示数据 $\mathcal{D}$ 的对数边缘概率（或对数边缘似然）。这个量通常被称为证据（evidence），并且在给定模型的情况下，它是一个关于数据的常数。具体来说，$\ln p(\mathcal{D})$ 是模型参数 $\mathbf{w}$ 的所有可能值的似然 $p(\mathcal{D}|\mathbf{w})$ 的积分（或求和）的对数：</p>
<script type="math/tex; mode=display">
\ln ⁡p(\mathcal{D})=\ln⁡\int p(\mathcal{D}∣\mathbf{w})p(\mathbf{w})\mathrm{d}\mathbf{w}</script><p>在实际应用中，计算这个积分通常是非常困难的，因为它涉及到对所有可能的参数值进行积分。因此，$\ln p(\mathcal{D})$ 通常被视为一个常数项，特别是在参数估计和模型比较中，因为它不依赖于特定的参数值 $\mathbf{w}$。</p>
<p>在模型比较（例如，贝叶斯因子计算）或模型选择中，$\ln p(\mathcal{D})$ 的值是重要的，因为它可以帮助我们确定哪个模型更好地解释了数据。然而，在给定模型的情况下， $\ln p(\mathcal{D})$通常被忽略，因为它在优化或推断过程中是一个不依赖于参数的常数项。</p>
<p>因此，$\ln p(\mathcal{D})$ 可以被简化为一个常数项，记为 $C$，在推导和优化过程中通常会被省略。</p>
</blockquote>
<p>here, we ignore the constant terms that do not depend on $w$ and get</p>
<script type="math/tex; mode=display">
\begin{aligned}

E(\mathbf{w}) &= \frac{1}{2\sigma^2}\sum_{n=1}^N\left\{y(x_n, \mathbf{w})-t_n \right\}^2+\frac{1}{2s^2}\sum_{i=1}^N w_i^2\\

&= \frac{1}{2\sigma^2}\sum_{n=1}^N\left\{y(x_n, \mathbf{w})-t_n \right\}^2+\frac{1}{2s^2}\mathbf{w}^{\mathrm{T}}\mathbf{w}

\end{aligned}</script><blockquote>
<script type="math/tex; mode=display">
\mathbf{w}^{\mathrm{T}}\mathbf{w} = \sum_{i=1}^N w_i^2</script></blockquote>
<h1 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h1><h2 id="证明伯努利分布的性质"><a href="#证明伯努利分布的性质" class="headerlink" title="证明伯努利分布的性质"></a>证明伯努利分布的性质</h2><script type="math/tex; mode=display">
\begin{aligned}

\sum_{x=0}^1 p(x|\mu) &= \sum_{x=0}^1 \mu ^x(1-\mu)^{1-x}\\

&= 1-\mu + \mu\\

&= 1

\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[x] &= 0\times(1-\mu) + 1\times \mu\\

&= \mu

\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}

\mathrm{var}[x] &= \mathbb{E}[(x-\mathbb{E}[x])^2]\\

&= \mu - 2\mu^2 + \mu^2\\

&= \mu(1-\mu)

\end{aligned}</script><p>as for the entropy, we have</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathrm{H}[x] &= -\sum_{x=0}^1 p(x|\mu)\ln p(x|\mu)\\

&= -(1-\mu)\ln (1-\mu) - \mu \ln \mu

\end{aligned}</script><h2 id="证明伯努利分布对称形式的性质"><a href="#证明伯努利分布对称形式的性质" class="headerlink" title="证明伯努利分布对称形式的性质"></a>证明伯努利分布对称形式的性质</h2><script type="math/tex; mode=display">
\begin{aligned}

p(x=-1|\mu) + p(x=1|\mu) &= \frac{1-\mu}{2} + \frac{1+\mu}{2}\\

&= 1

\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[x] &= -1\times p(x=-1|\mu) + 1\times p(x=1|\mu)\\

&= -\frac{1-\mu}{2} + \frac{1+\mu}{2}\\

&= \mu

\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}

\mathrm{var}[x] &= \mathbb{E}[(x-\mathbb{E}[x])^2]\\

&= \frac{1-\mu}{2} + \frac{1+\mu}{2} - 2\mu^2 + \mu^2\\

&= 1-\mu^2

\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}

\mathrm{H}[x] &= -p(x=-1|\mu)\ln p(x=-1|\mu) - p(x=1|\mu)\ln p(x=1|\mu)\\

&= -\frac{1-\mu}{2}\ln \frac{1-\mu}{2} - \frac{1+\mu}{2}\ln \frac{1+\mu}{2}\\

&= -(1-\mu)\ln(1-\mu) - (1+\mu)\ln (1+\mu) + \ln 2

\end{aligned}</script><h2 id="证明二项式定理和二项分布归一化"><a href="#证明二项式定理和二项分布归一化" class="headerlink" title="证明二项式定理和二项分布归一化"></a>证明二项式定理和二项分布归一化</h2><script type="math/tex; mode=display">
\begin{aligned}

\begin{pmatrix} N\\m\end{pmatrix} + \begin{pmatrix} N\\m-1\end{pmatrix} &= \frac{N!}{(N-m)!m!} + \frac{N!}{(N-m+1)!(m-1)!}\\

&= \frac{N!\cdot(N-m+1) + N!\cdot m}{(N-m+1)!m!}\\

&= \frac{(N+1)!}{(N-m+1)!m!}

&= \begin{pmatrix} N+1\\m\end{pmatrix}

\end{aligned}</script><p>When $N=0$, obviously we have</p>
<script type="math/tex; mode=display">
(1+x)^0 = 1</script><p>Assume that the formula is right, we have</p>
<script type="math/tex; mode=display">
(1+x)^N = \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix}x^m</script><p>for $N+1$, we have</p>
<blockquote>
<p>learnt from Deepseekv2</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{aligned}

(1+x)^{N+1} &= (1+x)\sum_{m=0}^{N+1} \begin{pmatrix}N\\m\end{pmatrix}x^m\\

&= \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix}x^m\\

&= \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix}x^m + \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix}x^{m+1}\\

&= \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix}x^m + \sum_{m=1}^{N+1} \begin{pmatrix}N\\m-1\end{pmatrix}x^m\\

&= \begin{pmatrix}N\\0\end{pmatrix} + x^{N+1} + \sum_{m=1}^N \left(\begin{pmatrix}N\\m\end{pmatrix} + \begin{pmatrix}N\\m-1\end{pmatrix} \right)  x_m\\

&= \begin{pmatrix}N+1\\0\end{pmatrix} + \sum_{m=1}^N \begin{pmatrix}N+1\\m\end{pmatrix}x^m + \begin{pmatrix}N+1\\N+1\end{pmatrix} x^{N+1}\\

&= \sum_{m=0}^{N+1}\begin{pmatrix}N+1\\m\end{pmatrix}x^m

\end{aligned}</script><blockquote>
<script type="math/tex; mode=display">
\begin{pmatrix}N\\m\end{pmatrix} + \begin{pmatrix}N\\m-1\end{pmatrix} = \begin{pmatrix}N+1\\m\end{pmatrix}</script></blockquote>
<p>Next we show that the binomial distribution is normalized,</p>
<script type="math/tex; mode=display">
\begin{aligned}

\sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix} 
\mu^m (1-\mu)^{N-m} &= (1-\mu)^N  \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix} \mu^m(1-\mu)^{-m}\\


&= (1-\mu)^N \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix} \left(\frac{\mu}{1-\mu}\right)^m                \\

&= (1-\mu)^N \left(1+\frac{\mu}{1-\mu}\right)^N    \\

&= (1-\mu)^N \left(\frac{1}{1-\mu}\right)^N    \\

&= 1

\end{aligned}</script><h2 id="证明二项分布的均值"><a href="#证明二项分布的均值" class="headerlink" title="证明二项分布的均值"></a>证明二项分布的均值</h2><p>Formula 3.11 is </p>
<script type="math/tex; mode=display">
\mathbb{E}[m] = \sum_{m=0}^N m\mathrm{Bin}(m|N, \mu) = N\mu</script><p>Here we differentiate both sides of the normalization condition (3.198) with respect to $\mu$,</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{pmatrix}N\\m\end{pmatrix} = \frac{N}{m}\begin{pmatrix}N-1\\m-1\end{pmatrix}</script><p>而且注意到 $m=0$ 的时候可以消掉一些值, 从而使 $\sum<em>{m=0}^N = \sum</em>{m=1}^N$</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\mathrm{d}}{\mathrm{d}\mu} \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix} \mu^m(1-\mu)^{N-m} &= \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix}\left( m\mu^{m-1}(1-\mu)^{N-m}-(N-m)\mu^m(1-\mu)^{N-m-1} \right)\\

&= \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix} \left(\mu^{m-1}(1-\mu)^{N-m-1}(m(1-\mu) - (N-m)\mu)   \right)\\

&= \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix} \mu^{m-1}(1-\mu)^{N-m-1}(m-N\mu)\\

m\sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix} \mu^{m-1}(1-\mu)^{N-m-1} &= N\mu\sum_{m=0}^N \mu^{m-1}(1-\mu)^{N-m-1}   \\

m\sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix} \mu^{m}(1-\mu)^{N-m} &= N\mu\sum_{m=0}^N \mu^{m}(1-\mu)^{N-m}\\

\mathbb{E}[m] &= N\mu


\end{aligned}</script><p>Next, we differentiate formula 3.198 twice with respect to $\mu$</p>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\mathrm{d}^2}{\mathrm{d}\mu^2}  \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix} \mu^m(1-\mu)^{N-m} &= \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix}  \mu^{m-2}(1-\mu)^{N-m-2}     \left(m(m-1)(1-\mu)^2 - 2m(N-m)\mu(1-\mu) + (N-m)(N-m-1)\mu^2    \right)\\

&= \sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix}\mu^{m-2}(1-\mu)^{N-m-2} \left(N(N-1)\mu^2 + 2m(1-N)\mu + m^2-m    \right) = 0



\end{aligned}</script><p>We represent $\mathbb{E}[\mathbf{x}^2]$ as</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbb{E}[\mathbf{x}^2] &= \sum_{m=0}^N m^2 \begin{pmatrix}N\\m\end{pmatrix}\mu^m (1-\mu)^{N-m}\\

&= -\sum_{m=0}^N \begin{pmatrix}N\\m\end{pmatrix} \mu^m(1-\mu)^{N-m}\left(N(N-1)\mu^2 + 2m(1-N)\mu -m    \right)\\

&= -N(N-1)\mu^2 - 2(1-N)\mu\mathbb{E}[\mathbf{x}] +   \mathbb{E}[\mathbf{x}]\\

&= -N(N-1)\mu^2 - 2N(1-N)\mu^2 + N\mu\\

&= N(N-1)\mu + N\mu


\end{aligned}</script><p>therefore, we have</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathrm{var}[\mathbf{x}] &= \mathbb{E}[\mathbf{x}^2] - \mathbb{E}[\mathbf{x}]^2\\

&= N(N-1)\mu^2 + N\mu - N^2\mu^2\\

&= N(1-\mu)\mu

\end{aligned}</script><h2 id="证明多元高斯分布的众数与众数相同"><a href="#证明多元高斯分布的众数与众数相同" class="headerlink" title="证明多元高斯分布的众数与众数相同"></a>证明多元高斯分布的众数与众数相同</h2><p>Formula 3.26 is</p>
<script type="math/tex; mode=display">
\mathcal{N}(\mathbf{x}|\mu, \Sigma) = \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^{\frac{1}{2}}}\exp \left\{-\frac{1}{2}(\mathbf{x}-\mu)^{\mathrm{T}}\Sigma^{-1}(\mathbf{x}-\mu)   \right\}</script><p>To find the mode, we need to maximize the probability density function. Due to the monotonicity of the exponential function, we can equivalently minimize the quadratic form</p>
<script type="math/tex; mode=display">
(\mathbf{x}-\mu)^{\mathrm{T}}\Sigma^{-1}(\mathbf{x}-\mu)</script><p>here we denote $\mathbf{x}-\mu$ as $\mathbf{y}$, therefore,</p>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\mathrm{d}}{\mathrm{d}\mathbf{y}} &= (\mathbf{y}^{\mathrm{T}}\Sigma^{-1}\mathbf{y}) = 2\Sigma^{-1}\mathbf{y} = 0

\end{aligned}</script><p>here we have $\mathbf{y} = 0$. Hence, $\mathbf{x} = \mu$, the mode of the multivariate Gaussian is $\mu$</p>
<h2 id="多元高斯分布线性变换后的变量也是高斯分布"><a href="#多元高斯分布线性变换后的变量也是高斯分布" class="headerlink" title="多元高斯分布线性变换后的变量也是高斯分布"></a>多元高斯分布线性变换后的变量也是高斯分布</h2><p>We have</p>
<script type="math/tex; mode=display">
\mathcal{N}(\mathbf{x}|\mu, \Sigma) = \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^{\frac{1}{2}}}\exp \left\{-\frac{1}{2}(\mathbf{x}-\mu)^{\mathrm{T}}\Sigma^{-1}(\mathbf{x}-\mu)   \right\}</script><p>as $\mathbf{A}$ and $\mathbf{x}$ are independent, for any expectation and variance, we have</p>
<script type="math/tex; mode=display">
\mathbb{E}[\mathbf{Ax}] = \mathbb{E}[\mathbf{A}]\times \mathbb{E}[\mathbf{x}] + \mathbf{b} = \mathbf{A} \times \mathbb{E}[\mathbf{x}] + \mathbf{b} = \mathbf{A\mu} + \mathbf{b}\\

\mathrm{var}[\mathbf{Ax}] = \mathbf{A}\mathrm{var}[\mathbf{x}]\mathbf{A}^{\mathrm{T}} = \mathbf{A}\Sigma\mathbf{A}^{\mathrm{T}}</script><h2 id="计算两多元高斯分布的-KL-散度"><a href="#计算两多元高斯分布的-KL-散度" class="headerlink" title="计算两多元高斯分布的 KL 散度"></a>计算两多元高斯分布的 KL 散度</h2><blockquote>
<p>unfinished</p>
</blockquote>
<p>Here we have $q(\mathbf{x}) = \mathcal{N}(\mathbf{x}|\mu_{q}, \Sigma_q)$, $p(\mathbf{x}) = \mathcal{N}(\mathbf{x}|\mu_q, \Sigma_q)$</p>
<script type="math/tex; mode=display">
\mathcal{N}(\mathbf{x}|\mu, \Sigma) = \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^{\frac{1}{2}}}\exp \left\{-\frac{1}{2}(\mathbf{x}-\mu)^{\mathrm{T}}\Sigma^{-1}(\mathbf{x}-\mu)   \right\}</script><p>therefore</p>
<blockquote>
<p>由于 $\mathbb{E}_q[x-\mu_q] = 0$, 所以</p>
<script type="math/tex; mode=display">
\mathbb{E}_q[(x-\mu_q)^{\mathrm{T}}\Sigma_p^{-1}(x-\mu_q)] = \Tr (\Sigma)</script></blockquote>
<script type="math/tex; mode=display">
\begin{aligned}

\mathrm{KL}(q(\mathbf{x})\|p(\mathbf{x})) &= \int q(\mathbf{x})\ln \frac{q(\mathbf{x})}{p(\mathbf{x})}\mathrm{d} \mathbf{x}\\

&= \int \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma_q|^{\frac{1}{2}}}\exp \left\{-\frac{1}{2}(\mathbf{x}-\mu_q)^{\mathrm{T}}\Sigma^{-1}(\mathbf{x}-\mu_q)   \right\}           \ln \frac{\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma_q|^{\frac{1}{2}}}\exp \left\{-\frac{1}{2}(\mathbf{x}-\mu_q)^{\mathrm{T}}\Sigma_q^{-1}(\mathbf{x}-\mu_q)   \right\} }    {\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma_p|^{\frac{1}{2}}}\exp \left\{-\frac{1}{2}(\mathbf{x}-\mu_p)^{\mathrm{T}}\Sigma_p^{-1}(\mathbf{x}-\mu_p)   \right\}}\mathrm{d}\mathbf{x}\\


&= \int q(\mathbf{x})\left[\ln \left(\frac{\Sigma_p}{\Sigma_q}\right)^{\frac{1}{2}}\right]\cdot     \left(-\frac{1}{2}(\mathbf{x}-\mu_q)^{\mathrm{T}}\Sigma_q^{-1}(\mathbf{x}-\mu_q)   +  \frac{1}{2}(\mathbf{x}-\mu_p)^{\mathrm{T}}\Sigma_p^{-1}(\mathbf{x}-\mu_p)    \right)\mathrm{d}\mathbf{x}\\

&= 



\end{aligned}</script><blockquote>
<h6 id="unfinished"><a href="#unfinished" class="headerlink" title="unfinished"></a>unfinished</h6></blockquote>
<h2 id="给定协方差-高斯分布有最大熵"><a href="#给定协方差-高斯分布有最大熵" class="headerlink" title="给定协方差, 高斯分布有最大熵"></a>给定协方差, 高斯分布有最大熵</h2><p>As asked, we have the Lagrange function</p>
<script type="math/tex; mode=display">
\mathcal{L}[p(\mathbf{x})] = -H(\mathbf{x}) + \lambda\left(\int p(\mathbf{x})\mathrm{d}\mathbf{x} -1 \right) + \mu^T\left(\int p(\mathbf{x}\mathrm{d}\mathbf{x}-\mu\right) + \Tr\left[\Lambda\left(\int p(\mathbf{x})(\mathbf{x} - \mu)(\mathbf{x}-\mu)^T\mathrm{d}\mathbf{x}-\Sigma \right) \right]</script><p>next we wanna perform the variational maximization</p>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\delta\mathcal{L}[p(x)]}{\delta p(x)} &= \int \delta p(\mathbf{x})[-\ln p(\mathbf{x})-1+\lambda+\alpha^T(\mathbf{x}-\mu)+(\mathbf{x}-\mu)^T\Lambda(\mathbf{x}-\mu)]\mathrm{d}\mathbf{x} = 0

\end{aligned}</script><p>hence, </p>
<script type="math/tex; mode=display">
\begin{aligned}

p(\mathbf{x}) &= \exp\left(-1+\lambda+\alpha^T(\mathbf{x}-\mu)+(\mathbf{x}-\mu)^T\Lambda(\mathbf{x}-\mu)\right)\\

&= \frac{1}{(2\pi)^{\frac{d}{2}}|\Sigma|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}(\mathbf{x}-\mu)^T\Sigma^{-1}(\mathbf{x}-\mu) \right)

\end{aligned}</script><h2 id="证明多元高斯分布的熵"><a href="#证明多元高斯分布的熵" class="headerlink" title="证明多元高斯分布的熵"></a>证明多元高斯分布的熵</h2><script type="math/tex; mode=display">
\begin{aligned}

\mathrm{H}[\mathbf{x}] &= -\int p(\mathbf{x})\ln p(\mathbf{x})\mathrm{d}\mathbf{x}\\

&=-\int p(\mathbf{x})\left(-\frac{D}{2}\ln 2\pi - \frac{1}{2}\ln\Sigma -\frac{1}{2}(\mathbf{x}-\mu)^T\Sigma^{-1}(\mathbf{x}-\mu) \right)\mathrm{d}\mathbf{x}\\

&= \left(\frac{D}{2}\ln 2\pi + \frac{1}{2}\ln\Sigma\right)\int p(\mathbf{x})\mathrm{d}\mathbf{x} + \frac{1}{2}\int p(\mathbf{x})(\mathbf{x}-\mu)^T\Sigma^{-1}(\mathbf{x}-\mu)\mathrm{d}\mathbf{x}\\

&= \frac{D}{2}\ln 2\pi + \frac{1}{2}\ln\Sigma + \frac{D}{2}\\

&= \frac{1}{2}\ln |\Sigma| + \frac{D}{2}(1+\ln(2\pi))

\end{aligned}</script><blockquote>
<p>多元高斯分布中, $\exp\left{(x-\mu)^T(x-\mu) \right} = \Sigma$, 迹运算$\Tr(\Sigma\Sigma^{-1})=\Tr(I)=D$</p>
</blockquote>
<h2 id="推导两高斯分布和的微分熵表达式"><a href="#推导两高斯分布和的微分熵表达式" class="headerlink" title="推导两高斯分布和的微分熵表达式"></a>推导两高斯分布和的微分熵表达式</h2><blockquote>
<p>卷积, 可以记为 $p(x)=p_1(x)*p_2(x)$</p>
<script type="math/tex; mode=display">
p(x) = \int_{-\infty}^{\infty} p_1(x-x_2)p_2(x_2)\mathrm{d}x_2</script></blockquote>
<p>Here we denote $p(\mathbf{x|\mathbf{x}_2})$ as $p(\mathbf{x}_1) = p(\mathbf{x}-\mathbf{x_2})$. Therefore, we have $p(\mathbf{x})=p(\mathbf{x}_1)<em>p(\mathbf{x}_2)$, here \</em> denote the convolution.</p>
<p>Precision is the reciprocal of variance, hence</p>
<script type="math/tex; mode=display">
\Sigma = \frac{1}{\tau_1} + \frac{1}{\tau_2}\\
\tau_x = \frac{\tau_1\tau_2}{\tau_1+\tau_2}</script><p>next we get</p>
<script type="math/tex; mode=display">
p(\mathbf{x}) = \sqrt{\frac{\tau}{2\pi}}\exp\left(-\frac{\tau_x}{2}(\mathbf{x}-\mu_x)^2 \right)</script><p>The differencetial entropy of the Gaussian is</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathrm{H}[\mathbf{x}] &= \frac{1}{2}\left\{1+\ln(2\pi\sigma^2) \right\}\\

&= \frac{1}{2}\left\{1+\ln\frac{2\pi}{\tau_x} \right\}

\end{aligned}</script><p>hence,</p>
<script type="math/tex; mode=display">
\mathrm{H}[\mathbf{x}] = \frac{1}{2}\left\{1+\ln\frac{2\pi(\tau_1+\tau_2)}{\tau_1\tau_2} \right\}</script><h2 id="证明反对称项不会出现在高斯分布公式的指数部分"><a href="#证明反对称项不会出现在高斯分布公式的指数部分" class="headerlink" title="证明反对称项不会出现在高斯分布公式的指数部分"></a>证明反对称项不会出现在高斯分布公式的指数部分</h2><p>Here we denote $\Lambda = \Sigma^{-1}$. Considering</p>
<script type="math/tex; mode=display">
p(\mathbf{x}) = \frac{1}{(2\pi)^{\frac{d}{2}}|\Sigma|^{\frac{1}{2}}}\exp \left(-\frac{1}{2}(\mathbf{x}-\mu)^T\Sigma^{-1}(\mathbf{x}-\mu) \right)</script><p>And we denote $S$ as symmetric matrix, $A$ as asymmetric matrix, therefore</p>
<blockquote>
<p>任意 $n$ 阶方阵都可以表示为一个对称矩阵和一个反对称矩阵的和. 比如矩阵 $A$, 构造 $A_1 = \frac{A+A^T}{2}$, $A_2 = \frac{A - A^T}{2}$, $A = A_1 +A_2$</p>
<p>对称矩阵symmetric matrix, 非对称矩阵asymmetric matrix</p>
</blockquote>
<script type="math/tex; mode=display">
\Lambda = S+A</script><p>next we have</p>
<script type="math/tex; mode=display">
(\mathbf{x}-\mu)^T\Lambda(\mathbf{x}-\mu) = (\mathbf{x}-\mu)^TS(\mathbf{x}-\mu) + (\mathbf{x}-\mu)^TA(\mathbf{x}-\mu)</script><p>As $A$ is asymmetric matrix, we have $A^T = -A$. Thus</p>
<script type="math/tex; mode=display">
(\mathbf{x}-\mu)^TA(\mathbf{x}-\mu) = 0</script><p>we get $(\mathbf{x}-\mu)^T\Lambda(\mathbf{x}-\mu) = (\mathbf{x}-\mu)^TS(\mathbf{x}-\mu)$. Hence, the precision matrix and the covariance matrix may be taken to be symmetric without loss of generality.</p>

  </article>

  <!-- prev and next -->
  <div class="flex justify-between mt-4 pt-4
    border-t border-[var(--c-sep)] text-sm
    gap-2 text-[var(--c-50)]
  ">
    <div>
      
        <a href="/2024/08/21/env-setup/VPN%E5%8D%8F%E8%AE%AE/"
          class="
            transition-all
            flex justify-center
            hover:-translate-x-1
            hover:text-[var(--c-80)]
          ">
          <iconify-icon width="20" icon="mingcute:left-fill" data-inline="false">
          </iconify-icon>
          VPN 协议梳理
        </a>
      
    </div>
    <div>
      
        <a href="/2024/08/01/Article/2024/2024-08-01-01-46/"
          class="
            flex 
            justify-center
            hover:translate-x-1 
            transition-transform
            hover:text-[var(--c-100)]
          "
        >
          八月一日夏季午夜梦回
          <iconify-icon width="20" icon="mingcute:right-fill" data-inline="false"></iconify-icon>
        </a>
      
    </div>
  </div>

  <!-- comment -->
  <div class="article-comments mt-12">
    
  <script src="https://giscus.app/client.js"
  data-repo="421zuoduan/blog-giscus-discussion"
  data-repo-id="R_kgDONKEKag"
  data-category="Announcements"
  data-category-id="DIC_kwDONKEKas4Cj9R8"
  data-mapping="pathname"
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="1"
  data-input-position="top"
  data-theme="preferred_color_scheme"
  data-lang="zh-CN"
  data-loading="lazy"
  crossorigin="anonymous"
  async>
</script>
<script>
  window.onload = function () {
    console.log("giscus loaded");
    const isDark = document.documentElement.classList.contains("dark");
    const giscusFrame = document.querySelector("iframe.giscus-frame");
    giscusFrame.contentWindow.postMessage(
      {
        giscus: {
          setConfig: {
            theme: isDark ? "dark" : "light",
          },
        },
      },
      "https://giscus.app"
    );
  };
</script>


  </div>
</section>
<!-- js inspect -->

<script src="/lib/clipboard.min.js"></script>


<script async src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
  $(document).ready(() => {
    const maraidConfig = {
      theme: "default",
      logLevel: 3,
      flowchart: { curve: "linear" },
      gantt: { axisFormat: "%m/%d/%Y" },
      sequence: { actorMargin: 50 },
    };
    mermaid.initialize(maraidConfig);
  });
</script>



<script src="/lib/fancybox/fancybox.umd.min.js"></script>

<script>
  $(document).ready(() => {
    $('.post-content').each(function(i){
      $(this).find('img').each(function(){
        if ($(this).parent().hasClass('fancybox') || $(this).parent().is('a')) return;
        var alt = this.alt;
        var title = this.title;
        if (alt) $(this).after('<span class="fancybox-alt">' + alt + '</span>');
        if (title) $(this).after('<span class="fancybox-title">' + title + '</span>');
        $(this).wrap('<a class="fancybox-img" href="' + this.src + '" data-fancybox=\"gallery\" data-caption="' + alt + '"></a>')
      });
      $(this).find('.fancybox').each(function(){
        $(this).attr('rel', 'article' + i);
      });
    });

    Fancybox.bind('[data-fancybox="gallery"]', {
        // options
    })
  })
</script>

<!-- tocbot begin -->

<script src="/lib/tocbot/tocbot.min.js"></script>

<script>
  $(document).ready(() => {
      tocbot.init({
        // Where to render the table of contents.
        tocSelector: '.post-toc',
        // Where to grab the headings to build the table of contents.
        contentSelector: '.post-content',
        // Which headings to grab inside of the contentSelector element.
        headingSelector: 'h1, h2, h3',
        // For headings inside relative or absolute positioned containers within content.
        hasInnerContainers: true,
    });
  })
</script>
<!-- tocbot end -->

  </main>
  <footer class="flex flex-col mt-18 mb-12 items-center
  text-[var(--c-50)] text-sm">
  <div class="flex flex-row items-center my-12">
    
    
        
        
            
            
        
        <a class="
            hover:text-[var(--c-theme)]
            hover:bg-[var(--c-20)]
            rounded-lg
            p-2
            my-1
            flex flex-row items-center
            group" title="Github" target="_blank" rel="noopener" href="https://www.github.com/421zuoduan">
            <iconify-icon width="28" icon="mingcute:github-fill"></iconify-icon>
        </a>
    
        
        
            
            
        
        <a class="
            hover:text-[var(--c-theme)]
            hover:bg-[var(--c-20)]
            rounded-lg
            p-2
            my-1
            flex flex-row items-center
            group" title="ZhiHu" target="_blank" rel="noopener" href="https://www.zhihu.com/people/ren-jian-lan-xue">
            <iconify-icon width="28" icon="ri:zhihu-line"></iconify-icon>
        </a>
    

  </div>
  <!-- busuanzi -->
  <div class="mb-6">
    
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!-- Busuanzi Analytics -->
<div class="flex flex-col items-center mb-2">
  <div class="flex flex-row items-center">
    <iconify-icon width="16" icon="ic:round-person" width="18"></iconify-icon>
    <span class="mr-1">访客 Visitors: </span>
    <span id="busuanzi_value_site_uv"></span>
  </div>
  <div class="flex flex-row items-center">
    <iconify-icon width="16" icon="carbon:view-filled" width="18"></iconify-icon>
    <span class="mx-1">浏览量 Page Views:</span>
    <span id="busuanzi_value_site_pv"></span>
  </div>
</div>
<!-- End Busuanzi Analytics -->


  </div>
  <!-- copyright -->
  <div class="flex flex-row items-center gap-2">
    <a class="hover:underline"
      target="_blank"
      href="https://creativecommons.org/licenses/by-nc-sa/4.0/"
    >
      CC BY-NC-SA 4.0
    </a>
    <span>© 2022-2024</span>
    <a class="hover:underline"
    href="https://github.com/chen-yingfa" 
    target="_blank" 
    rel="noopener noreferrer">陈英发</a>
  </div>
  <!-- powered by -->
  <div class="flex items-center gap-1">
    <span>Powered by</span>
    <a class="hover:underline" 
    href="https://hexo.io/" target="_blank" rel="noopener noreferrer">Hexo</a>
    <span>&</span>
    <a href="https://github.com/chen-yingfa/hexo-theme-fengye" 
    class="hover:underline"
    target="_blank"
      rel="noopener noreferrer"
      >
      枫叶 Fengye
    </a>
  </div>

</footer>

  <div class="
    back-to-top
    fixed right-6
    z-1024
    -bottom-20
    rounded-lg
    font-bold
    py-1 px-2
    text-[var(--c-80)]
    bg-[var(--c-20)]
    cursor-pointer
    text-center
    drop-shadow-md
  ">
    <span class="flex justify-center items-center text-sm">
      <span id="scrollpercent"><span>0</span> %</span>
      <iconify-icon width="18" icon="mingcute:arrow-to-up-fill" id="go-top"></iconify-icon>
    </span>
  </div>
  
<script src="/js/main.js"></script>


  <div class="fixed top-0 bottom-0 left-0 right-0 pointer-events-none print:hidden" id="maple"></div>
</body>

</html>
